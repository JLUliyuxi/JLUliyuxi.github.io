<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LyC&#39;s Blog</title>
  
  <subtitle>莫道君行早 更有早行人</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jluliyuxi.github.io/"/>
  <updated>2020-07-18T14:53:53.164Z</updated>
  <id>https://jluliyuxi.github.io/</id>
  
  <author>
    <name>LyC</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python数据分析与展示（第1周 数据分析之表示 单元三 实例一：图像的手绘效果）</title>
    <link href="https://jluliyuxi.github.io/2020/07/18/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%B1%95%E7%A4%BA%EF%BC%88%E7%AC%AC1%E5%91%A8%20%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8B%E8%A1%A8%E7%A4%BA%20%E5%8D%95%E5%85%83%E4%B8%89%20%E5%AE%9E%E4%BE%8B%E4%B8%80%EF%BC%9A%E5%9B%BE%E5%83%8F%E7%9A%84%E6%89%8B%E7%BB%98%E6%95%88%E6%9E%9C%EF%BC%89/"/>
    <id>https://jluliyuxi.github.io/2020/07/18/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%B1%95%E7%A4%BA%EF%BC%88%E7%AC%AC1%E5%91%A8%20%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8B%E8%A1%A8%E7%A4%BA%20%E5%8D%95%E5%85%83%E4%B8%89%20%E5%AE%9E%E4%BE%8B%E4%B8%80%EF%BC%9A%E5%9B%BE%E5%83%8F%E7%9A%84%E6%89%8B%E7%BB%98%E6%95%88%E6%9E%9C%EF%BC%89/</id>
    <published>2020-07-18T14:13:10.849Z</published>
    <updated>2020-07-18T14:53:53.164Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图像的数组表示"><a href="#图像的数组表示" class="headerlink" title="图像的数组表示"></a>图像的数组表示</h1><p><strong>图像一般采用RGB模式，取值范围均为0-255。</strong></p><p>Python的第三方库：PIL库    </p><p>在命令行下安装<code>pip install pillow</code>     </p><p>引用方法<code>from PIL import Image</code>     </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from PIL import Image        </span><br><span class="line">import numpy as np                                            </span><br><span class="line">im &#x3D; np.array(Image.open(&quot;C:&#x2F;Users&#x2F;91353&#x2F;Desktop&#x2F;Wallpaper&#x2F;nr1jj0.png&quot;))        </span><br><span class="line">print(im.shape, im.dtype)</span><br></pre></td></tr></table></figure><p><strong>输出：</strong><code>(1080, 1920, 4) uint8</code></p><p><strong>图像是一个三维数组，维度分别是高度、宽度和像素RGB值</strong></p><h1 id="图像的变换"><a href="#图像的变换" class="headerlink" title="图像的变换"></a>图像的变换</h1><p><strong>- 读入图像</strong>     </p><p><strong>- 获得像素的RGB值</strong>    </p><p><strong>- 修改后保存为新文件</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">from PIL import Image</span><br><span class="line">import numpy as np                                    </span><br><span class="line">a &#x3D; np.array(Image.open(&quot;C:&#x2F;Users&#x2F;91353&#x2F;Desktop&#x2F;Wallpaper&#x2F;nr1jj0.png&quot;))</span><br><span class="line">print(a.shape, a.dtype)</span><br><span class="line">b &#x3D; [255, 255, 255] -a</span><br><span class="line">im &#x3D; Image.formarray(b.astype(&#39;uint8&#39;))</span><br><span class="line">im.save(&quot;C:&#x2F;Users&#x2F;91353&#x2F;Desktop&#x2F;Wallpaper&#x2F;TestPhoto.png&quot;)</span><br></pre></td></tr></table></figure><h1 id="“图像的手绘效果”实例分析"><a href="#“图像的手绘效果”实例分析" class="headerlink" title="“图像的手绘效果”实例分析"></a>“图像的手绘效果”实例分析</h1><p><strong>特点：黑白灰色、边界线条较重、相同或相近色彩趋于白色</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from PIL import Image</span><br><span class="line">import numpy as np</span><br><span class="line"> </span><br><span class="line">a &#x3D; np.asarray(Image.open(&#39;C:&#x2F;Users&#x2F;91353&#x2F;Desktop&#x2F;Wallpaper&#x2F;nr1jj0.png&#39;).convert(&#39;L&#39;)).astype(&#39;float&#39;)</span><br><span class="line"> </span><br><span class="line">depth &#x3D; 10.                      # (0-100)</span><br><span class="line">grad &#x3D; np.gradient(a)             #取图像灰度的梯度值</span><br><span class="line">grad_x, grad_y &#x3D; grad               #分别取横纵图像梯度值</span><br><span class="line">grad_x &#x3D; grad_x*depth&#x2F;100.</span><br><span class="line">grad_y &#x3D; grad_y*depth&#x2F;100.</span><br><span class="line">A &#x3D; np.sqrt(grad_x**2 + grad_y**2 + 1.)</span><br><span class="line">uni_x &#x3D; grad_x&#x2F;A</span><br><span class="line">uni_y &#x3D; grad_y&#x2F;A</span><br><span class="line">uni_z &#x3D; 1.&#x2F;A</span><br><span class="line"> </span><br><span class="line">vec_el &#x3D; np.pi&#x2F;2.2                   # 光源的俯视角度，弧度值</span><br><span class="line">vec_az &#x3D; np.pi&#x2F;4.                    # 光源的方位角度，弧度值</span><br><span class="line">dx &#x3D; np.cos(vec_el)*np.cos(vec_az)   #光源对x 轴的影响</span><br><span class="line">dy &#x3D; np.cos(vec_el)*np.sin(vec_az)   #光源对y 轴的影响</span><br><span class="line">dz &#x3D; np.sin(vec_el)              #光源对z 轴的影响</span><br><span class="line"> </span><br><span class="line">b &#x3D; 255*(dx*uni_x + dy*uni_y + dz*uni_z)     #光源归一化</span><br><span class="line">b &#x3D; b.clip(0,255)</span><br><span class="line"> </span><br><span class="line">im &#x3D; Image.fromarray(b.astype(&#39;uint8&#39;))  #重构图像</span><br><span class="line">im.save(&#39;C:&#x2F;Users&#x2F;91353&#x2F;Desktop&#x2F;Wallpaper&#x2F;Test.png&#39;)</span><br></pre></td></tr></table></figure><h2 id="原图"><a href="#原图" class="headerlink" title="原图"></a>原图</h2><p><img src="https://i.loli.net/2020/07/18/s8IN6QqbW74KpM1.png" alt="Original_Photo"></p><h2 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h2><p><img src="https://i.loli.net/2020/07/18/QIpeGlDH3sObcWT.png" alt="Test_Photo"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;图像的数组表示&quot;&gt;&lt;a href=&quot;#图像的数组表示&quot; class=&quot;headerlink&quot; title=&quot;图像的数组表示&quot;&gt;&lt;/a&gt;图像的数组表示&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;图像一般采用RGB模式，取值范围均为0-255。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;P
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Python数据分析与展示（第1周 数据分析之表示 单元二 NumPy数据存储与函数）</title>
    <link href="https://jluliyuxi.github.io/2020/07/12/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%B1%95%E7%A4%BA%EF%BC%88%E7%AC%AC1%E5%91%A8%20%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8B%E8%A1%A8%E7%A4%BA%20%E5%8D%95%E5%85%83%E4%BA%8C%20NumPy%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%B8%8E%E5%87%BD%E6%95%B0%EF%BC%89/"/>
    <id>https://jluliyuxi.github.io/2020/07/12/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%B1%95%E7%A4%BA%EF%BC%88%E7%AC%AC1%E5%91%A8%20%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8B%E8%A1%A8%E7%A4%BA%20%E5%8D%95%E5%85%83%E4%BA%8C%20NumPy%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%B8%8E%E5%87%BD%E6%95%B0%EF%BC%89/</id>
    <published>2020-07-12T02:33:45.242Z</published>
    <updated>2020-07-12T03:43:04.811Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据的CSV文件存取"><a href="#数据的CSV文件存取" class="headerlink" title="数据的CSV文件存取"></a>数据的CSV文件存取</h1><p><strong>CSV(Comma-Separated Value,逗号分隔值)</strong>        </p><h2 id="将数据写入CSV文件"><a href="#将数据写入CSV文件" class="headerlink" title="将数据写入CSV文件"></a>将数据写入CSV文件</h2><p><code>np.savetxt(frame,array,fmt=&#39;%.18e&#39;,delimiter=None)</code>        </p><blockquote><p><strong>frame</strong>:文件、字符串或产生器，可以是.gz或.bz2的压缩文件<br><strong>array</strong>:存入文件的数组<br><strong>fmt</strong>:写入文件的格式，例如：%d %.2f %.18e<br><strong>delimiter</strong>:分割字符串，默认是任何空格     </p></blockquote><h2 id="将CSV文件数据读入numpy数组类型"><a href="#将CSV文件数据读入numpy数组类型" class="headerlink" title="将CSV文件数据读入numpy数组类型"></a>将CSV文件数据读入numpy数组类型</h2><p><code>np.loadtxt(frame,dtype=np.float,delimiter=None,unpack=False</code>       </p><blockquote><p><strong>frame</strong>:文件、字符串或产生器，可以是.gz或.bz2的压缩文件<br><strong>dtype</strong>:数据类型，可选，默认是浮点数类型<br><strong>delimiter</strong>:分割字符串，默认是任何空格<br><strong>unpack</strong>:如果True，读入属性将分别写入不同变量        </p></blockquote><h1 id="多维数据的存取"><a href="#多维数据的存取" class="headerlink" title="多维数据的存取"></a>多维数据的存取</h1><h2 id="方法一："><a href="#方法一：" class="headerlink" title="方法一："></a>方法一：</h2><h3 id="a-tofile-frame-sep-39-39-format-39-s-39"><a href="#a-tofile-frame-sep-39-39-format-39-s-39" class="headerlink" title="a.tofile(frame,sep=&#39;&#39;,format=&#39;%s&#39;)"></a><code>a.tofile(frame,sep=&#39;&#39;,format=&#39;%s&#39;)</code></h3><blockquote><p><strong>frame</strong>:文件、字符串<br><strong>sep</strong>:数据分割字符串，如果是空串，写入文件为二进制<br><strong>format</strong>:写入数据的格式     </p></blockquote><h3 id="np-fromfile-frame-dtype-float-count-1-sep-39-39"><a href="#np-fromfile-frame-dtype-float-count-1-sep-39-39" class="headerlink" title="np.fromfile(frame,dtype=float,count=-1,sep=&#39;&#39;)"></a><code>np.fromfile(frame,dtype=float,count=-1,sep=&#39;&#39;)</code></h3><blockquote><p><strong>frame</strong>:文件、字符串<br><strong>dtype</strong>:读取的数据类型<br><strong>count</strong>:读入元素个数，-1表示读入整个文件<br><strong>sep</strong>:数据分割字符串，如果是空串，写入文件为二进制     </p></blockquote><p><strong><em>需要注意：</em></strong><br><strong><em>该方法需要读取时知道存入文件时数组的维度和元素类型</em></strong><br><strong><em>a.tofile()和np.fromfile()需要配合使用</em></strong><br><strong><em>可以通过元数据文件来存储额外信息</em></strong>      </p><h2 id="方法二：NumPy的便携文件存取"><a href="#方法二：NumPy的便携文件存取" class="headerlink" title="方法二：NumPy的便携文件存取"></a>方法二：NumPy的便携文件存取</h2><h3 id="np-save-fname-array-或-np-savez-fname-array"><a href="#np-save-fname-array-或-np-savez-fname-array" class="headerlink" title="np.save(fname,array) 或 np.savez(fname,array)"></a><code>np.save(fname,array)</code> 或 <code>np.savez(fname,array)</code></h3><blockquote><p><strong>frame</strong>:文件名，以.npy为扩展名，压缩扩展名为.npz<br><strong>array</strong>:数组变量     </p></blockquote><h3 id="np-load-fname"><a href="#np-load-fname" class="headerlink" title="np.load(fname)"></a><code>np.load(fname)</code></h3><blockquote><p><strong>frame</strong>:文件名，以.npy为扩展名，压缩扩展名为.npz         </p></blockquote><h1 id="NumPy的随机数函数"><a href="#NumPy的随机数函数" class="headerlink" title="NumPy的随机数函数"></a>NumPy的随机数函数</h1><p>NumPy的random子库      </p><p><strong>np.random的随机数函数(1)</strong></p><table><thead><tr><th align="center">函数</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">rand(d0,d1,..,dn)</td><td align="center">根据d0-dn创建随机数数组，浮点数，[0,1)，均匀分布</td></tr><tr><td align="center">randn(d0,d1,..,dn)</td><td align="center">根据d0-dn创建随机数数组，标准正态分布</td></tr><tr><td align="center">randint(low[,high,shape])</td><td align="center">根据shape创建随机整数或整数数组，范围是[low, high)</td></tr><tr><td align="center">seed(s)</td><td align="center">随机数种子，s是给定的种子值</td></tr></tbody></table><p><strong>np.random的随机数函数(2)</strong></p><table><thead><tr><th align="center">函数</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">shuffle(a)</td><td align="center">根据数组a的第1轴进行随排列，改变数组x</td></tr><tr><td align="center">permutation(a)</td><td align="center">根据数组a的第1轴产生一个新的乱序数组，不改变数组x</td></tr><tr><td align="center">choice(a[,size,replace,p])</td><td align="center">从一维数组a中以概率p抽取元素，形成size形状新数组,replace表示是否可以重用元素，默认为False</td></tr></tbody></table><p><strong>np.random的随机数函数(3)</strong></p><table><thead><tr><th align="center">函数</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">uniform(low,high,size)</td><td align="center">产生具有均匀分布的数组，low起始值，high结束值，size形状</td></tr><tr><td align="center">normal(loc,scale,size)</td><td align="center">产生具有正态分布的数组，loc均值，scale标准差，size形状</td></tr><tr><td align="center">poisson(lam,size)</td><td align="center">产生具有泊松分布的数组，lam随机事件发生率，size形状</td></tr></tbody></table><h1 id="NumPy的统计函数"><a href="#NumPy的统计函数" class="headerlink" title="NumPy的统计函数"></a>NumPy的统计函数</h1><p><strong>np.random的统计函数(1)</strong></p><table><thead><tr><th align="center">函数</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">sum(a,axis=None)</td><td align="center">根据给定轴axis计算数组a相关元素之和，axis整数或元组</td></tr><tr><td align="center">mean(a,axis=None)</td><td align="center">根据给定轴axis计算数组a相关元素的期望，axis整数或元组</td></tr><tr><td align="center">average(a,axis=None,weights=None)</td><td align="center">根据给定轴axis计算数组a相关元素的加权平均值</td></tr><tr><td align="center">std(a,axis=None)</td><td align="center">根据给定轴axis计算数组a相关元素的标准差</td></tr><tr><td align="center">var(a,axis=None)</td><td align="center">根据给定轴axis计算数组a相关元素的方差</td></tr></tbody></table><p><strong>np.random的统计函数(2)</strong></p><table><thead><tr><th align="center">函数</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">min(a) max(a)</td><td align="center">计算数组a中元素的最小值、最大值</td></tr><tr><td align="center">argmin(a) argmax(a)</td><td align="center">计算数组a中元素最小值、最大值的降一维后下标</td></tr><tr><td align="center">unravel_index(index,shape)</td><td align="center">根据shape将一维下标index转换成多维下标</td></tr><tr><td align="center">ptp(a)</td><td align="center">计算数组a中元素最大值与最小值的差</td></tr><tr><td align="center">median(a)</td><td align="center">计算数组a中元素的中位数(中值)</td></tr></tbody></table><h1 id="NumPy的梯度函数"><a href="#NumPy的梯度函数" class="headerlink" title="NumPy的梯度函数"></a>NumPy的梯度函数</h1><p>NumPy的梯度函数只有一个  </p><table><thead><tr><th align="center">函数</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">np.gradient()</td><td align="center">计算数组f中元素的梯度，当f为多维时，返回每个维度梯度</td></tr></tbody></table><p><strong>梯度：连续值之间的变化率，即斜率</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;数据的CSV文件存取&quot;&gt;&lt;a href=&quot;#数据的CSV文件存取&quot; class=&quot;headerlink&quot; title=&quot;数据的CSV文件存取&quot;&gt;&lt;/a&gt;数据的CSV文件存取&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;CSV(Comma-Separated Value,逗号分隔
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Python数据分析与展示（第1周 数据分析之表示 单元一 NumPy库入门）</title>
    <link href="https://jluliyuxi.github.io/2020/07/11/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%B1%95%E7%A4%BA%EF%BC%88%E7%AC%AC1%E5%91%A8%20%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8B%E8%A1%A8%E7%A4%BA%20%E5%8D%95%E5%85%83%E4%B8%80%20NumPy%E5%BA%93%E5%85%A5%E9%97%A8%EF%BC%89/"/>
    <id>https://jluliyuxi.github.io/2020/07/11/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%B1%95%E7%A4%BA%EF%BC%88%E7%AC%AC1%E5%91%A8%20%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8B%E8%A1%A8%E7%A4%BA%20%E5%8D%95%E5%85%83%E4%B8%80%20NumPy%E5%BA%93%E5%85%A5%E9%97%A8%EF%BC%89/</id>
    <published>2020-07-11T03:25:10.770Z</published>
    <updated>2020-07-11T11:28:45.496Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据的维度"><a href="#数据的维度" class="headerlink" title="数据的维度"></a>数据的维度</h1><p><strong>维度</strong>：一组数据的组织形式        </p><p>一维数据：由对等关系的有序或无序数据构成，采用线性方式组织。      </p><p>列表数据类型可以不同，数组数据类型必须相同。      </p><p>二维数据：由多个一维数据构成，是一维数据的组合形式。      </p><p>数据维度的Python表示：      </p><p>一维数据：列表+集合<br>二维数据：列表<br>三维数据：列表     </p><h1 id="NumPy的数组对象：ndarray"><a href="#NumPy的数组对象：ndarray" class="headerlink" title="NumPy的数组对象：ndarray"></a>NumPy的数组对象：ndarray</h1><p>NumPy是一个开源的Python科学计算库<br>※一个强大的N维数组对象 ndarray<br>※广播功能函数<br>※整合C/C++/Fortran代码的工具<br>※线性代数、傅里叶变换、随机数生成等功能        </p><h2 id="NumPy的引用"><a href="#NumPy的引用" class="headerlink" title="NumPy的引用"></a>NumPy的引用</h2><p><code>import numpy as np</code>引用NumPy库<br><strong>尽量使用约定的别名np</strong></p><h2 id="N维度数组对象：ndarray"><a href="#N维度数组对象：ndarray" class="headerlink" title="N维度数组对象：ndarray"></a>N维度数组对象：ndarray</h2><p><strong>例如：计算A平方+B三次方，其中，A和B是一维数组</strong>   </p><p><strong>常规方法：</strong>     </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def pySum():</span><br><span class="line">    a &#x3D; [0,1,2,3,4]</span><br><span class="line">    b &#x3D; [9,8,7,6,5]</span><br><span class="line">    c &#x3D; []</span><br><span class="line"></span><br><span class="line">    for i in range(len(a)):</span><br><span class="line">        c.append(a[i]**2 + b[i]**3)</span><br><span class="line"></span><br><span class="line">    return c</span><br><span class="line"></span><br><span class="line">print(pySum())</span><br></pre></td></tr></table></figure><p><strong>ndarray方法:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">def npSum():</span><br><span class="line">    a &#x3D; np.array([0,1,2,3,4])</span><br><span class="line">    b &#x3D; np.array([9,8,7,6,5])</span><br><span class="line"></span><br><span class="line">    c &#x3D; a**2 + b**3 #维度相同时可直接进行计算</span><br><span class="line"></span><br><span class="line">    return c</span><br><span class="line"></span><br><span class="line">print(npSum())</span><br></pre></td></tr></table></figure><p><strong>※数组对象可以去掉元素间运算所需的循环，使一维向量更像个单个数据。</strong>  </p><p><strong>※设置专门的数组对象，经过优化，可以提升这类应用的运算速度。</strong>      </p><h2 id="ndarray简述"><a href="#ndarray简述" class="headerlink" title="ndarray简述"></a>ndarray简述</h2><p>ndarray是一个多维数组对象，由两部分组成：    </p><p>※实际的数据      </p><p>※描述这些数据的元数据（数据维度、数据类型等）     </p><p><code>np.array()</code>生成一个ndarray数组<br><em>ndarray在程序中的别名是：array</em>     </p><p>np.array()输出成[]形式，元素由空格分割。      </p><p><strong>轴(axis)：保存数据的维度</strong>     </p><p><strong>秩(rank)：轴的数量</strong>    </p><h2 id="ndarray对象的属性"><a href="#ndarray对象的属性" class="headerlink" title="ndarray对象的属性"></a>ndarray对象的属性</h2><table><thead><tr><th align="center">属性</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">.ndim</td><td align="center">秩，即轴的数量或维度的数量</td></tr><tr><td align="center">.shape</td><td align="center">ndarray对象的尺度，对于矩阵，n行m列</td></tr><tr><td align="center">.size</td><td align="center">ndarray对象元素的个数，相当于.shape中n* m的值</td></tr><tr><td align="center">.dtype</td><td align="center">ndarray对象的元素类型</td></tr><tr><td align="center">.itemsize</td><td align="center">ndarray对象中每个元素的大小，以字节为单位</td></tr></tbody></table><h2 id="ndarray的元素类型"><a href="#ndarray的元素类型" class="headerlink" title="ndarray的元素类型"></a>ndarray的元素类型</h2><p><img src="https://i.loli.net/2020/07/11/VBJsknFgWUYbC5S.png" alt="ndarray的元素类型①"> </p><p><img src="https://i.loli.net/2020/07/11/qk7uOPKNmsJF5Ll.png" alt="ndarray的元素类型②"></p><p><img src="https://i.loli.net/2020/07/11/HYBQnRzukqpE8Od.png" alt="ndarray的元素类型③"></p><h1 id="ndarray数组的创建和变换"><a href="#ndarray数组的创建和变换" class="headerlink" title="ndarray数组的创建和变换"></a>ndarray数组的创建和变换</h1><h2 id="ndarray的创建方法："><a href="#ndarray的创建方法：" class="headerlink" title="ndarray的创建方法："></a>ndarray的创建方法：</h2><p>① 从Python中的列表、元组等类型创建ndarray数组。     </p><p><code>x = np.array(list/tuple)</code></p><p><code>x = np.array(list/tuple,dtype=np.float32) #指定元素数据类型</code><br>当np.array()不指定dtype时，NumPy将根据数据情况关联一个dtype类型。       </p><p>② 使用NumPy中函数创建ndarray数组，如：arange,ones,zeros等。       </p><table><thead><tr><th align="center">函数</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">np.arange(n)</td><td align="center">类似range(函数，返回ndarray类型， 元素从0到n-1</td></tr><tr><td align="center">np.ones(shape)</td><td align="center">根据shape生成一 个全 1数组，shape是元组类型</td></tr><tr><td align="center">np.zeros(shape)</td><td align="center">根据shape生成一 个 全0数组，shape是元组类型</td></tr><tr><td align="center">np.full(shape,va1)</td><td align="center">根据shape生成一个数组，每个元素值都是val</td></tr><tr><td align="center">np.eye(n)</td><td align="center">创建一个正方的n*n单位矩阵，对角线为1，其余为0</td></tr><tr><td align="center">np.ones_like(a)</td><td align="center">根据数组a的形状生成一个全1数组</td></tr><tr><td align="center">np.zeros_like(a)</td><td align="center">根据数组a的形状生成一个全0数组</td></tr><tr><td align="center">np.full_like(a,val)</td><td align="center">根据数组a的形状生成一个数组， 每个元素值都是val</td></tr><tr><td align="center">np.linspace()</td><td align="center">根据起止数据等间距地填充数据，形成数组</td></tr><tr><td align="center">np.concatenate()</td><td align="center">将两个或多个数组合并成一个新的数组</td></tr></tbody></table><p>③ 从字节流（raw bytes)中创建ndarray数组。</p><p>④ 从文件中读取特定格式，创建ndarray数组。</p><h2 id="ndarray数组的维度变换"><a href="#ndarray数组的维度变换" class="headerlink" title="ndarray数组的维度变换"></a>ndarray数组的维度变换</h2><table><thead><tr><th align="center">方法</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">.reshape(shape)</td><td align="center">不改变数组元素，返回一个shape形状的数组，原数组不变</td></tr><tr><td align="center">.resize(shape)</td><td align="center">与.reshape()功能一致， 但修改原数组</td></tr><tr><td align="center">.swapaxes(ax1,ax2)</td><td align="center">将数组n个维度中两个维度进行调换</td></tr><tr><td align="center">.flatten()</td><td align="center">对数组进行降维，返回折叠后的一维数组，原数组不变</td></tr></tbody></table><h1 id="ndarray数组的操作"><a href="#ndarray数组的操作" class="headerlink" title="ndarray数组的操作"></a>ndarray数组的操作</h1><p>即对数组的<strong>索引</strong>和<strong>切片</strong></p><h2 id="一维数组的索引和切片：与Python类似"><a href="#一维数组的索引和切片：与Python类似" class="headerlink" title="一维数组的索引和切片：与Python类似"></a>一维数组的索引和切片：与Python类似</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; np.array([9,8,7,6,5])</span><br><span class="line">a[2] # out:7</span><br><span class="line">a[1:4:2] # out:array([8,6])</span><br></pre></td></tr></table></figure><h2 id="多维数组的索引："><a href="#多维数组的索引：" class="headerlink" title="多维数组的索引："></a>多维数组的索引：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; np.arange(24).reshape((2,3,4))</span><br><span class="line">a[1,2,3] # out:23</span><br><span class="line">a[0,1,2] # out:6</span><br><span class="line">a[-1,-2,-3] # out:17</span><br><span class="line">#每个维度一个索引值，用逗号分隔</span><br></pre></td></tr></table></figure><h2 id="多维数组的切片："><a href="#多维数组的切片：" class="headerlink" title="多维数组的切片："></a>多维数组的切片：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; np.arange(24).reshape((2,3,4))</span><br><span class="line">a[:,1,-3] # out:array([5,17])</span><br><span class="line">a[:,1:3,:]</span><br><span class="line">a[:,:,::2]</span><br></pre></td></tr></table></figure><h1 id="ndarray数组的运算"><a href="#ndarray数组的运算" class="headerlink" title="ndarray数组的运算"></a>ndarray数组的运算</h1><h2 id="数组与标量之间的运算"><a href="#数组与标量之间的运算" class="headerlink" title="数组与标量之间的运算"></a>数组与标量之间的运算</h2><p>数组与标量之间的运算作用于数组的每一个元素       </p><p><img src="https://i.loli.net/2020/07/11/DPIQtpvEOHslqJA.png" alt=""></p><h2 id="NumPy一元函数"><a href="#NumPy一元函数" class="headerlink" title="NumPy一元函数"></a>NumPy一元函数</h2><table><thead><tr><th align="center">函数</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">np.abs(x) np.fabs(x)</td><td align="center">计算数组各元素的绝对值</td></tr><tr><td align="center">np.sqrt(x)</td><td align="center">计算数组各元素的平方根</td></tr><tr><td align="center">np.square(x)</td><td align="center">计算数组各元素的平方</td></tr><tr><td align="center">np.log(x) np.log10(x) np.log2(x)</td><td align="center">计算数组各元素的自然对数、10底对数和2底对数</td></tr><tr><td align="center">np.ceil(x) np.floor(x)</td><td align="center">计算数组各 元素的eiling值或noor值</td></tr><tr><td align="center">np.rint(x)</td><td align="center">计算数组各元素的四舍五入值</td></tr><tr><td align="center">np.modf(x)</td><td align="center">将数组各元素的小数和整数部分以两个独立数组形式返回</td></tr><tr><td align="center">np.cos(x) np.cosh(x)</td><td align="center"></td></tr><tr><td align="center">np.sin(x) np.sinh(x)</td><td align="center">计算数组各元素的普通型和双曲型三角函数</td></tr><tr><td align="center">np.tan(x) np.tanh(x)</td><td align="center"></td></tr><tr><td align="center">np.exp(x)</td><td align="center">计算数组各元素的指数值</td></tr><tr><td align="center">np.sign(x)</td><td align="center">计算数组各元素的符号值，1(+),0, -1(-)</td></tr></tbody></table><h2 id="NumPy二元函数"><a href="#NumPy二元函数" class="headerlink" title="NumPy二元函数"></a>NumPy二元函数</h2><table><thead><tr><th align="center">函数</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center"><code>+ - * / **</code></td><td align="center">两个数组各元素进行对应运算</td></tr><tr><td align="center">np.maximum(x,y) np.fmax()</td><td align="center"></td></tr><tr><td align="center"></td><td align="center">元素级的最大值/最小值计算</td></tr><tr><td align="center">np.minimum(x,y) np.fmin()</td><td align="center"></td></tr><tr><td align="center">np.mod(x,y)</td><td align="center">元素级的模运算</td></tr><tr><td align="center">np.copysign(x,y)</td><td align="center">将数组y中各元素值的符号赋值给数组x对应元素</td></tr><tr><td align="center"><code>&gt; &lt; &gt;= &lt;= == !=</code></td><td align="center">算术比较，产生布尔型数组</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;数据的维度&quot;&gt;&lt;a href=&quot;#数据的维度&quot; class=&quot;headerlink&quot; title=&quot;数据的维度&quot;&gt;&lt;/a&gt;数据的维度&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;维度&lt;/strong&gt;：一组数据的组织形式        &lt;/p&gt;
&lt;p&gt;一维数据：由对等关系的有序
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Python数据分析与展示（第0周 数据分析之前奏）</title>
    <link href="https://jluliyuxi.github.io/2020/07/10/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%B1%95%E7%A4%BA%EF%BC%88%E7%AC%AC0%E5%91%A8%20%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8B%E5%89%8D%E5%A5%8F%EF%BC%89/"/>
    <id>https://jluliyuxi.github.io/2020/07/10/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%B1%95%E7%A4%BA%EF%BC%88%E7%AC%AC0%E5%91%A8%20%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8B%E5%89%8D%E5%A5%8F%EF%BC%89/</id>
    <published>2020-07-10T13:47:51.138Z</published>
    <updated>2020-07-10T14:33:46.529Z</updated>
    
    <content type="html"><![CDATA[<h1 id="“数据分析”课程内容导学"><a href="#“数据分析”课程内容导学" class="headerlink" title="“数据分析”课程内容导学"></a>“数据分析”课程内容导学</h1><p>NumPy库，Matplotlib库，Pandas库      </p><p><strong>摘要</strong>：有损地提取数据特征的过程<br>※基本统计(含排序)<br>※分布/累计统计<br>※数据特征，相关性、周期性等<br>※数据挖掘(形成知识)     </p><p><strong><em>建议使用Anaconda IDE</em></strong>      </p><p><strong><em>理解和掌握 conda Spyder IPython 的使用</em></strong>        </p><p><strong>内容组织</strong>：<br>绘制坐标系、饼图、直方图、极坐标图、散点图<br>实例1：图像的手绘效果<br>实例2：引力波的绘制<br>实例3：房价趋势的关联因素分析<br>实例4：股票数据的趋势分析曲线     </p><h1 id="Anaconda-IDE的基本使用"><a href="#Anaconda-IDE的基本使用" class="headerlink" title="Anaconda IDE的基本使用"></a>Anaconda IDE的基本使用</h1><h2 id="包管理和环境管理工具-conda"><a href="#包管理和环境管理工具-conda" class="headerlink" title="包管理和环境管理工具-conda"></a>包管理和环境管理工具-conda</h2><p>包管理与pip类似，管理Python第三方库      </p><p>环境管理能够允许用户使用不同版本的Python，并能灵活切换。</p><p>启动cmd,执行<code>conda --version</code>，可获取conda版本号</p><h2 id="Anaconda-一个集合，包括conda、某版本Python、一批第三方库等。"><a href="#Anaconda-一个集合，包括conda、某版本Python、一批第三方库等。" class="headerlink" title="Anaconda:一个集合，包括conda、某版本Python、一批第三方库等。"></a>Anaconda:一个集合，包括conda、某版本Python、一批第三方库等。</h2><h2 id="编程工具：Spyder"><a href="#编程工具：Spyder" class="headerlink" title="编程工具：Spyder"></a>编程工具：Spyder</h2><p>包括编辑区、文件导航和IPython      </p><h2 id="交互式编程环境：IPython"><a href="#交互式编程环境：IPython" class="headerlink" title="交互式编程环境：IPython"></a>交互式编程环境：IPython</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;“数据分析”课程内容导学&quot;&gt;&lt;a href=&quot;#“数据分析”课程内容导学&quot; class=&quot;headerlink&quot; title=&quot;“数据分析”课程内容导学&quot;&gt;&lt;/a&gt;“数据分析”课程内容导学&lt;/h1&gt;&lt;p&gt;NumPy库，Matplotlib库，Pandas库    
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Python网络爬虫与信息提取（第4周 网络爬虫框架 第十一单元 股票数据Scrapy爬虫）</title>
    <link href="https://jluliyuxi.github.io/2020/07/03/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC4%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%20%E7%AC%AC%E5%8D%81%E4%B8%80%E5%8D%95%E5%85%83%20%E8%82%A1%E7%A5%A8%E6%95%B0%E6%8D%AEScrapy%E7%88%AC%E8%99%AB%EF%BC%89/"/>
    <id>https://jluliyuxi.github.io/2020/07/03/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC4%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%20%E7%AC%AC%E5%8D%81%E4%B8%80%E5%8D%95%E5%85%83%20%E8%82%A1%E7%A5%A8%E6%95%B0%E6%8D%AEScrapy%E7%88%AC%E8%99%AB%EF%BC%89/</id>
    <published>2020-07-03T03:11:31.724Z</published>
    <updated>2020-07-03T03:22:29.853Z</updated>
    
    <content type="html"><![CDATA[<h1 id="股票数据Scrapy爬虫实例介绍"><a href="#股票数据Scrapy爬虫实例介绍" class="headerlink" title="股票数据Scrapy爬虫实例介绍"></a>股票数据Scrapy爬虫实例介绍</h1><hr><p>获取股票列表:     </p><p>东方财富网: <a href="http://quote.eastmoney.com/stocklist.html" target="_blank" rel="noopener">http://quote.eastmoney.com/stocklist.html</a>        </p><p>获取个股信息:     </p><p>百度股票: <a href="https://gupiao.baidu.com/stock/" target="_blank" rel="noopener">https://gupiao.baidu.com/stock/</a>       </p><p>单个股票: <a href="https://gupiao.baidu.com/stock/sz002439.html" target="_blank" rel="noopener">https://gupiao.baidu.com/stock/sz002439.html</a>      </p><h1 id="股票数据Scrapy爬虫实例编写"><a href="#股票数据Scrapy爬虫实例编写" class="headerlink" title="股票数据Scrapy爬虫实例编写"></a>股票数据Scrapy爬虫实例编写</h1><h2 id="Stocks-py"><a href="#Stocks-py" class="headerlink" title="Stocks.py"></a>Stocks.py</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line">import re</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">class StocksSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &quot;stocks&quot;</span><br><span class="line">    start_urls &#x3D; [&#39;https:&#x2F;&#x2F;quote.eastmoney.com&#x2F;stocklist.html&#39;]</span><br><span class="line"> </span><br><span class="line">    def parse(self, response):</span><br><span class="line">        for href in response.css(&#39;a::attr(href)&#39;).extract():</span><br><span class="line">            try:</span><br><span class="line">                stock &#x3D; re.findall(r&quot;[s][hz]\d&#123;6&#125;&quot;, href)[0]</span><br><span class="line">                url &#x3D; &#39;https:&#x2F;&#x2F;gupiao.baidu.com&#x2F;stock&#x2F;&#39; + stock + &#39;.html&#39;</span><br><span class="line">                yield scrapy.Request(url, callback&#x3D;self.parse_stock)</span><br><span class="line">            except:</span><br><span class="line">                continue</span><br><span class="line"> </span><br><span class="line">    def parse_stock(self, response):</span><br><span class="line">        infoDict &#x3D; &#123;&#125;</span><br><span class="line">        stockInfo &#x3D; response.css(&#39;.stock-bets&#39;)</span><br><span class="line">        name &#x3D; stockInfo.css(&#39;.bets-name&#39;).extract()[0]</span><br><span class="line">        keyList &#x3D; stockInfo.css(&#39;dt&#39;).extract()</span><br><span class="line">        valueList &#x3D; stockInfo.css(&#39;dd&#39;).extract()</span><br><span class="line">        for i in range(len(keyList)):</span><br><span class="line">            key &#x3D; re.findall(r&#39;&gt;.*&lt;&#x2F;dt&gt;&#39;, keyList[i])[0][1:-5]</span><br><span class="line">            try:</span><br><span class="line">                val &#x3D; re.findall(r&#39;\d+\.?.*&lt;&#x2F;dd&gt;&#39;, valueList[i])[0][0:-5]</span><br><span class="line">            except:</span><br><span class="line">                val &#x3D; &#39;--&#39;</span><br><span class="line">            infoDict[key]&#x3D;val</span><br><span class="line"> </span><br><span class="line">        infoDict.update(</span><br><span class="line">            &#123;&#39;股票名称&#39;: re.findall(&#39;\s.*\(&#39;,name)[0].split()[0] + \</span><br><span class="line">             re.findall(&#39;\&gt;.*\&lt;&#39;, name)[0][1:-1]&#125;)</span><br><span class="line">        yield infoDict</span><br></pre></td></tr></table></figure><h2 id="Pipelines-py"><a href="#Pipelines-py" class="headerlink" title="Pipelines.py"></a>Pipelines.py</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"> </span><br><span class="line"># Define your item pipelines here</span><br><span class="line">#</span><br><span class="line"># Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting</span><br><span class="line"># See: https:&#x2F;&#x2F;doc.scrapy.org&#x2F;en&#x2F;latest&#x2F;topics&#x2F;item-pipeline.html</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">class BaidustocksPipeline(object):</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        return item</span><br><span class="line"> </span><br><span class="line">class BaidustocksInfoPipeline(object):</span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        self.f &#x3D; open(&#39;BaiduStockInfo.txt&#39;, &#39;w&#39;)</span><br><span class="line"> </span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        self.f.close()</span><br><span class="line"> </span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        try:</span><br><span class="line">            line &#x3D; str(dict(item)) + &#39;\n&#39;</span><br><span class="line">            self.f.write(line)</span><br><span class="line">        except:</span><br><span class="line">            pass</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;股票数据Scrapy爬虫实例介绍&quot;&gt;&lt;a href=&quot;#股票数据Scrapy爬虫实例介绍&quot; class=&quot;headerlink&quot; title=&quot;股票数据Scrapy爬虫实例介绍&quot;&gt;&lt;/a&gt;股票数据Scrapy爬虫实例介绍&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;获取股票列表:  
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Python网络爬虫与信息提取（第4周 网络爬虫框架 第十一单元 Scrapy爬虫基本使用）</title>
    <link href="https://jluliyuxi.github.io/2020/06/21/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC4%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%20%E7%AC%AC%E5%8D%81%E4%B8%80%E5%8D%95%E5%85%83%20Scrapy%E7%88%AC%E8%99%AB%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%EF%BC%89/"/>
    <id>https://jluliyuxi.github.io/2020/06/21/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC4%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%20%E7%AC%AC%E5%8D%81%E4%B8%80%E5%8D%95%E5%85%83%20Scrapy%E7%88%AC%E8%99%AB%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%EF%BC%89/</id>
    <published>2020-06-21T09:28:17.598Z</published>
    <updated>2020-06-22T09:27:26.702Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Scrapy爬虫的第一个实例"><a href="#Scrapy爬虫的第一个实例" class="headerlink" title="Scrapy爬虫的第一个实例"></a>Scrapy爬虫的第一个实例</h1><p>演示HTML页面地址：     </p><p><a href="http://python123.io/ws/demo.html" target="_blank" rel="noopener">http://python123.io/ws/demo.html</a>    </p><p>文件名称：demo.html</p><p><strong>爬虫步骤：</strong>   </p><ol><li>建立一个Scrapy爬虫工程       </li></ol><p>在工程路径下打开cmd, 输入<code>scrapy stratproject python123demo</code>,定义一个名为python123demo的文件夹  </p><p><img src="https://i.loli.net/2020/06/21/bAQupzMg26taRKX.png" alt=""></p><p>随后打开路径，会发现已经建立好了一个文件夹   </p><p><img src="https://i.loli.net/2020/06/21/wH123tiZGexsd9b.png" alt=""></p><p>生成的工程路径： </p><p><img src="https://i.loli.net/2020/06/21/4kyHIgZ78iR2vfd.png" alt=""></p><p><img src="https://i.loli.net/2020/06/21/uOcdYtlCspKI1Qh.png" alt=""></p><ol start="2"><li>在工程中产生一个Scrapy爬虫     </li></ol><p>进入python123demo,cmd输入<code>scrapy genspider demo python123.io</code>产生名为demo的spider</p><p><img src="https://i.loli.net/2020/06/21/JDwHBoA8zhUExbv.png" alt="生成一条名称为demo的spider"></p><p>生成爬虫的代码内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">#demo.py文件</span><br><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class DemoSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &#39;demo&#39;   #名称</span><br><span class="line">    allowed_domains &#x3D; [&#39;python123.io&#39;]  #只能爬虫这个域名以下的链接</span><br><span class="line">    start_urls &#x3D; [&#39;http:&#x2F;&#x2F;python123.io&#x2F;&#39;]   #爬取页面的初始页面</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        pass</span><br></pre></td></tr></table></figure><p><strong>parse()用于处理响应，解析内容形成字典，发现新的URL爬取请求。</strong></p><ol start="3"><li>配置产生的spider爬虫    </li></ol><p>对上述spider进行配置：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class DemoSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &#39;demo&#39;</span><br><span class="line">    allowed_domains &#x3D; [&#39;python123.io&#39;]</span><br><span class="line">    start_urls &#x3D; [&#39;http:&#x2F;&#x2F;python123.io&#x2F;ws&#x2F;demo.html&#39;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        fname &#x3D; response.url.split(&#39;&#x2F;&#39;)[-1] #存储到本地的response文件名称</span><br><span class="line">        with open(fname, &#39;wb&#39;) as f :</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(&quot;Saved file %s.&quot; % name)</span><br><span class="line">        pass</span><br></pre></td></tr></table></figure><ol start="4"><li>运行爬虫，获取网页    </li></ol><p>cmd内输入<code>scrapy crawl demo</code>，捕获页面存储在html文件中</p><p><img src="https://i.loli.net/2020/06/21/zI2sELjWvt3frpF.png" alt=""></p><h2 id="yield关键字的使用"><a href="#yield关键字的使用" class="headerlink" title="yield关键字的使用"></a>yield关键字的使用</h2><p>生成器是一个不断产生值的函数。      </p><p>包含yield语句的函数是一个生成器。     </p><p>生成器每次产生一个值(yield语句)，函数被冻结，被唤醒后再产生一个值。</p><p><strong><em>生成器写法：</em></strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def gen(n) :</span><br><span class="line">    for i in range(n) :</span><br><span class="line">        yield i ** 2</span><br><span class="line"></span><br><span class="line">for i in gen(5):</span><br><span class="line">    print(i, &#39; &#39;, end&#x3D;&#39;&#39;)</span><br></pre></td></tr></table></figure><p><strong><em>普通写法：</em></strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def square(n) :</span><br><span class="line">    ls &#x3D; [i**2 for i in range(n)]</span><br><span class="line">    return ls</span><br><span class="line"></span><br><span class="line">for i in square(5):</span><br><span class="line">    print(i, &#39; &#39;, end&#x3D;&#39;&#39;)</span><br></pre></td></tr></table></figure><p>生成器：<br>更节省存储空间<br>响应更迅速<br>使用更灵活   </p><h1 id="Scrapy爬虫的基本使用"><a href="#Scrapy爬虫的基本使用" class="headerlink" title="Scrapy爬虫的基本使用"></a>Scrapy爬虫的基本使用</h1><p><strong>Scrapy爬虫的使用步骤</strong></p><p>步骤一：创建一个工程和Spider模板     </p><p>步骤二：编写Spider    </p><p>步骤三：编写Item Pipeline     </p><p>步骤四：优化配置策略      </p><p><strong>Scrapy爬虫的数据类型</strong></p><p>Request类：表示一个HTTP请求，由Spider生成，由Downloader执行。    </p><table><thead><tr><th align="center">属性或方法</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">.url</td><td align="center">Request对应的请求URL地址</td></tr><tr><td align="center">.method</td><td align="center">对应的请求方法，’GET’ ‘POST’等</td></tr><tr><td align="center">.headers</td><td align="center">字典类型风格的请求头</td></tr><tr><td align="center">.body</td><td align="center">请求内容主体，字符串类型</td></tr><tr><td align="center">.meta</td><td align="center">用户添加的扩展信息，在Scrapy内部模块间传递信息使用</td></tr><tr><td align="center">.copy()</td><td align="center">复制该请求</td></tr></tbody></table><p>Response类：表示一个HTTP响应，由Downloader生成，由Spider处理</p><table><thead><tr><th align="center">属性或方法</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">.url</td><td align="center">Response对应的URL地址</td></tr><tr><td align="center">.status</td><td align="center">HTTP状态码，默认是200</td></tr><tr><td align="center">.headers</td><td align="center">Response对应的头部信息</td></tr><tr><td align="center">.body</td><td align="center">Response对应的内容信息，字符串类型</td></tr><tr><td align="center">.flags</td><td align="center">一组标记</td></tr><tr><td align="center">.request</td><td align="center">产生Response类型对应的Request对象</td></tr><tr><td align="center">.copy()</td><td align="center">复制该响应</td></tr></tbody></table><p>Item类：表示从一个HTML页面中提取的信息内容，由Spider生成，由Item Pipeline处理<br>类似于字典，可用字典类型使用      </p><p><strong>CSS Selector的基本使用：</strong></p><p><img src="https://i.loli.net/2020/06/21/fjLIdzmBqwYDb65.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Scrapy爬虫的第一个实例&quot;&gt;&lt;a href=&quot;#Scrapy爬虫的第一个实例&quot; class=&quot;headerlink&quot; title=&quot;Scrapy爬虫的第一个实例&quot;&gt;&lt;/a&gt;Scrapy爬虫的第一个实例&lt;/h1&gt;&lt;p&gt;演示HTML页面地址：     &lt;/p&gt;
&lt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>从键盘输入年月日，计算是这一年中的第几天</title>
    <link href="https://jluliyuxi.github.io/2020/06/21/%E4%BB%8E%E9%94%AE%E7%9B%98%E8%BE%93%E5%85%A5%E5%B9%B4%E6%9C%88%E6%97%A5%EF%BC%8C%E8%AE%A1%E7%AE%97%E6%98%AF%E8%BF%99%E4%B8%80%E5%B9%B4%E4%B8%AD%E7%9A%84%E7%AC%AC%E5%87%A0%E5%A4%A9/"/>
    <id>https://jluliyuxi.github.io/2020/06/21/%E4%BB%8E%E9%94%AE%E7%9B%98%E8%BE%93%E5%85%A5%E5%B9%B4%E6%9C%88%E6%97%A5%EF%BC%8C%E8%AE%A1%E7%AE%97%E6%98%AF%E8%BF%99%E4%B8%80%E5%B9%B4%E4%B8%AD%E7%9A%84%E7%AC%AC%E5%87%A0%E5%A4%A9/</id>
    <published>2020-06-21T07:55:41.822Z</published>
    <updated>2020-06-22T09:29:33.978Z</updated>
    
    <content type="html"><![CDATA[<p>这个问题是我同学给我的期末Python模拟题里面的一道，最开始我是试着用判断是否闰年，分30天的月份和31天的月份。      </p><p>结果代码越写越长，显然是不太合适的。       </p><p>下面的是我用最初的方法写的代码：     </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">months_days30 &#x3D; [4,6,9,11]</span><br><span class="line">months_days31 &#x3D; [1,3,5,7,8,10,12]</span><br><span class="line">month_Feb &#x3D; 2 #区分不同的月份对应的天数</span><br><span class="line"></span><br><span class="line">def isleapyear(year) : #判断是否为闰年</span><br><span class="line">    if year % 4 &#x3D;&#x3D; 0 :</span><br><span class="line">        return True</span><br><span class="line">    else:</span><br><span class="line">        return False</span><br><span class="line"></span><br><span class="line">def main(): #计算天数</span><br><span class="line">    year &#x3D; eval(input(&quot;请输入年份：&quot;))</span><br><span class="line">    month &#x3D; eval(input(&quot;请输入月份：&quot;))</span><br><span class="line">    day &#x3D; eval(input(&quot;请输入年份：&quot;))</span><br><span class="line">    num_day31 &#x3D; 0</span><br><span class="line">    num_day30 &#x3D; 0</span><br><span class="line"></span><br><span class="line">    if month &#x3D;&#x3D; 1 :</span><br><span class="line">        print(str(day))</span><br><span class="line">    if month &#x3D;&#x3D; 2 :</span><br><span class="line">        print(str(day+31))</span><br><span class="line">    for d30 in months_days30 :</span><br><span class="line">        if d30 &lt; month :</span><br><span class="line">            num_day30 +&#x3D; 1</span><br><span class="line">        else:</span><br><span class="line">            break</span><br><span class="line">    for d31 in months_days31 :</span><br><span class="line">        if d31 &lt; month :</span><br><span class="line">            num_day31 +&#x3D; 1</span><br><span class="line">        else:</span><br><span class="line">            break</span><br><span class="line">    if isleapyear(year) :</span><br><span class="line">        print(num_day30 * 30 + num_day31 * 31 + 29 + day)</span><br><span class="line">    else :</span><br><span class="line">        print(num_day30 * 30 + num_day31 * 31 + 28 + day)</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure><p>后来又翻了一下嵩天的《Python语言程序设计基础》，复习了一下time库和datetime库，发现几行代码就可以解决。    </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">date_input &#x3D; input(&quot;请输入日期(年-月-日）：&quot;)</span><br><span class="line">t &#x3D; time.strptime(date_input, &#39;%Y-%m-%d&#39;)</span><br><span class="line">print(time.strftime(&#39;%j&#39;,t))</span><br></pre></td></tr></table></figure><p>具体方法：   </p><table><thead><tr><th align="center">方法</th><th align="center">含义</th></tr></thead><tbody><tr><td align="center">%a</td><td align="center">英文星期简写</td></tr><tr><td align="center">%A</td><td align="center">英文星期的完全</td></tr><tr><td align="center">%b</td><td align="center">英文月份的简写</td></tr><tr><td align="center">%B</td><td align="center">英文月份的完全</td></tr><tr><td align="center">%c</td><td align="center">显示本地日期时间</td></tr><tr><td align="center">%d</td><td align="center">日期，取1-31</td></tr><tr><td align="center">%H</td><td align="center">小时， 0-23</td></tr><tr><td align="center">%I</td><td align="center">小时， 0-12</td></tr><tr><td align="center">%m</td><td align="center">月， 01 -12</td></tr><tr><td align="center">%M</td><td align="center">分钟，1-59</td></tr><tr><td align="center">%j</td><td align="center">年中当天的天数</td></tr><tr><td align="center">%w</td><td align="center">显示今天是星期几</td></tr><tr><td align="center">%W</td><td align="center">第几周</td></tr><tr><td align="center">%x</td><td align="center">当天日期</td></tr><tr><td align="center">%X</td><td align="center">本地的当天时间</td></tr><tr><td align="center">%y</td><td align="center">年份 00-99间</td></tr><tr><td align="center">%Y</td><td align="center">年份的完整拼写</td></tr></tbody></table><p>所以在这个实例中，直接用<code>%j</code>即可显示日期对应的当年天数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这个问题是我同学给我的期末Python模拟题里面的一道，最开始我是试着用判断是否闰年，分30天的月份和31天的月份。      &lt;/p&gt;
&lt;p&gt;结果代码越写越长，显然是不太合适的。       &lt;/p&gt;
&lt;p&gt;下面的是我用最初的方法写的代码：     &lt;/p&gt;
&lt;figur
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Python网络爬虫与信息提取（第4周 网络爬虫框架 第十单元 Scrapy爬虫框架）</title>
    <link href="https://jluliyuxi.github.io/2020/06/20/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC4%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%20%E7%AC%AC%E5%8D%81%E5%8D%95%E5%85%83%20Scrapy%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%EF%BC%89/"/>
    <id>https://jluliyuxi.github.io/2020/06/20/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC4%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%20%E7%AC%AC%E5%8D%81%E5%8D%95%E5%85%83%20Scrapy%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%EF%BC%89/</id>
    <published>2020-06-20T06:28:47.364Z</published>
    <updated>2020-06-22T09:28:13.095Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Scrapy爬虫框架介绍"><a href="#Scrapy爬虫框架介绍" class="headerlink" title="Scrapy爬虫框架介绍"></a>Scrapy爬虫框架介绍</h1><h2 id="Scrapy的安装"><a href="#Scrapy的安装" class="headerlink" title="Scrapy的安装"></a>Scrapy的安装</h2><p>命令行输入<code>pip install Scrapy</code>       </p><p>安装后小测：执行<code>scrapy -h</code>     </p><p>安装的过程中可能会出现失败的情况，报错显示<code>error:MicrosoftVisual C++ 14.0 is required</code>   </p><p><strong>解决办法：</strong><br>根据Python版本和系统位数选择对应的Twisted模块(<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/" target="_blank" rel="noopener">点击此处下载</a>)，例如我的Python是3.8，系统位数为64位，所以选择</p><p><img src="https://i.loli.net/2020/06/20/rL13TsQMgJwFIjA.png" alt=""></p><p>将文件下载到Python/Scripts文件夹，cd到此文件夹内输入<code>pip install Twisted-20.3.0-cp38-cp38-win_amd64.whl</code>      </p><p>安装成功后再输入<code>pip install scrapy</code>即可成功    </p><p>scrapy不单单是一个库，而是一个爬虫框架      </p><p><strong>爬虫框架：</strong>是实现爬虫功能的一个软件结构和功能组件集合      </p><h2 id="Scrapy爬虫框架结构"><a href="#Scrapy爬虫框架结构" class="headerlink" title="Scrapy爬虫框架结构"></a>Scrapy爬虫框架结构</h2><p><strong>5+2结构和3条数据流：</strong></p><p><img src="https://i.loli.net/2020/06/20/UGyuikEcmS3aNzr.png" alt="5+2结构和3条数据流"></p><p>结构中，用户需要编写配置的是模块是<strong>SPIDERS(入口)</strong>和<strong>ITEM PIPELINES(出口)</strong></p><h1 id="Scrapy爬虫框架解析"><a href="#Scrapy爬虫框架解析" class="headerlink" title="Scrapy爬虫框架解析"></a>Scrapy爬虫框架解析</h1><p><strong>Engine：</strong>控制所有模块之间的数据流/根据条件出发事件，不需要用户修改       </p><p><strong>Downloader：</strong>跟去请求下载网页，不需要用户修改     </p><p><strong>Scheduler:</strong>对所有爬取请求进行调度管理，不需要用户修改     </p><p><strong>Downloader Middleware:</strong>实施上述三组模块站之间进行用户可配置的控制。修改、丢弃、新增请求或相应。     </p><p><strong>Spider:</strong>解析Downloader返回的响应(Response)/产生爬取项(scraped item)/产生额外的爬取请求(Request)       </p><p><strong>Item Pipelines:</strong>以流水线方式处理Spider产生的爬取项/由一组操作顺序组成，类似流水线，每个操作是一个Item Pipeline类型/可能操作包括：清理、检验和查重爬取项中的HTML数据，将数据存储到数据库     </p><p><strong>Spider Middleware:</strong>对请求和爬取项的再处理。修改、丢弃、新增请求或爬取项。       </p><h1 id="Requests库和Scrapy框架的比较"><a href="#Requests库和Scrapy框架的比较" class="headerlink" title="Requests库和Scrapy框架的比较"></a>Requests库和Scrapy框架的比较</h1><p><strong>相同点：</strong>       </p><p>两者都可以进行页面请求和爬取，Python爬虫的两个重要技术路线        </p><p>两者可用性都好，文档丰富，入门简单       </p><p>两者都没有处理js、提交表单、应对验证码等功能(可扩展)        </p><p><strong>不同点:</strong>    </p><table><thead><tr><th align="center">requests</th><th align="center">Serapy</th></tr></thead><tbody><tr><td align="center">页面级爬虫</td><td align="center">网站级爬虫</td></tr><tr><td align="center">功能库</td><td align="center">框架</td></tr><tr><td align="center">并发性考虑不足，性能较差</td><td align="center">并发性好，性能较高</td></tr><tr><td align="center">重点在于页面下载</td><td align="center">重点在于爬虫结构</td></tr><tr><td align="center">定制灵活</td><td align="center">一般定制灵活，深度定制困难</td></tr><tr><td align="center">上手十分简单</td><td align="center">入门稍难</td></tr></tbody></table><p><strong><em>对于非常小的需求，用Requests库。</em></strong>      </p><p><strong><em>对于不太小的需求，用Scrapy框架</em></strong></p><h1 id="Scrapy爬虫的常用指令"><a href="#Scrapy爬虫的常用指令" class="headerlink" title="Scrapy爬虫的常用指令"></a>Scrapy爬虫的常用指令</h1><p>Scrapy是为持续运行设计的专业爬虫框架，提供操作的Scrapy命令行。  </p><p>cmd里输入<code>scrapy -h</code>即可打开Scrapy命令行      </p><p>Scrapy命令行格式：    </p><blockquote><p>scrapy<command>[options][args]     </p></blockquote><p><strong>Scrapy常用命令</strong></p><table><thead><tr><th align="center">命令</th><th align="center">说明</th><th align="center">格式</th></tr></thead><tbody><tr><td align="center"><strong>startproject</strong></td><td align="center">创建一个新工程</td><td align="center">scrapy startproject<name>[dir]</td></tr><tr><td align="center"><strong>genspider</strong></td><td align="center">创建一个爬虫</td><td align="center">scrapy genspider [options]<name><domain></td></tr><tr><td align="center">settings</td><td align="center">获得爬虫配置信息</td><td align="center">scrapy settings [options]</td></tr><tr><td align="center"><strong>crawl</strong></td><td align="center">运行一个爬虫</td><td align="center">scrapy crawl <spider></td></tr><tr><td align="center">list</td><td align="center">列出工程中所有爬虫</td><td align="center">scrapy list</td></tr><tr><td align="center">shell</td><td align="center">启动URL调试命令行</td><td align="center">scrapy shell [url]</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Scrapy爬虫框架介绍&quot;&gt;&lt;a href=&quot;#Scrapy爬虫框架介绍&quot; class=&quot;headerlink&quot; title=&quot;Scrapy爬虫框架介绍&quot;&gt;&lt;/a&gt;Scrapy爬虫框架介绍&lt;/h1&gt;&lt;h2 id=&quot;Scrapy的安装&quot;&gt;&lt;a href=&quot;#Scra
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Python网络爬虫与信息提取（第3周 网络爬虫实战 第八九单元 实例）</title>
    <link href="https://jluliyuxi.github.io/2020/06/18/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC3%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%20%E7%AC%AC%E5%85%AB%E4%B9%9D%E5%8D%95%E5%85%83%20%E5%AE%9E%E4%BE%8B%EF%BC%89/"/>
    <id>https://jluliyuxi.github.io/2020/06/18/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC3%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%20%E7%AC%AC%E5%85%AB%E4%B9%9D%E5%8D%95%E5%85%83%20%E5%AE%9E%E4%BE%8B%EF%BC%89/</id>
    <published>2020-06-18T01:04:29.824Z</published>
    <updated>2020-06-22T09:25:34.421Z</updated>
    
    <content type="html"><![CDATA[<h1 id="淘宝商品比价定向爬虫实例"><a href="#淘宝商品比价定向爬虫实例" class="headerlink" title="淘宝商品比价定向爬虫实例"></a>淘宝商品比价定向爬虫实例</h1><hr><p><strong>流程图：</strong></p><p><img src="https://i.loli.net/2020/06/18/2Amz9ujKZkIwlNd.png" alt="流程图"></p><p><strong>代码编写：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def getHTMLText(url):</span><br><span class="line">    try:</span><br><span class="line">        r &#x3D; requests.get(url, timeout&#x3D;30)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding &#x3D; r.apparent_encoding</span><br><span class="line">        return r.text</span><br><span class="line">    except:</span><br><span class="line">        return &quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def parsePage(ilt, html):</span><br><span class="line">    try:</span><br><span class="line">        plt &#x3D; re.findall(r&#39;\&quot;view_price\&quot;\:\&quot;[\d\.]*\&quot;&#39;, html)</span><br><span class="line">        tlt &#x3D; re.findall(r&#39;\&quot;raw_title\&quot;\:\&quot;.*?\&quot;&#39;, html)</span><br><span class="line">        for i in range(len(plt)):</span><br><span class="line">            price &#x3D; eval(plt[i].split(&#39;:&#39;)[1])</span><br><span class="line">            title &#x3D; eval(tlt[i].split(&#39;:&#39;)[1])</span><br><span class="line">            ilt.append([price, title])</span><br><span class="line">    except:</span><br><span class="line">        print(&quot;&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def printGoodsList(ilt):</span><br><span class="line">    tplt &#x3D; &quot;&#123;:4&#125;\t&#123;:8&#125;\t&#123;:16&#125;&quot;</span><br><span class="line">    print(tplt.format(&quot;序号&quot;, &quot;价格&quot;, &quot;商品名称&quot;))</span><br><span class="line">    count &#x3D; 0</span><br><span class="line">    for g in ilt:</span><br><span class="line">        count &#x3D; count + 1</span><br><span class="line">        print(tplt.format(count, g[0], g[1]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    goods &#x3D; &#39;书包&#39;</span><br><span class="line">    depth &#x3D; 3</span><br><span class="line">    start_url &#x3D; &#39;https:&#x2F;&#x2F;s.taobao.com&#x2F;search?q&#x3D;&#39; + goods</span><br><span class="line">    infoList &#x3D; []</span><br><span class="line">    for i in range(depth):</span><br><span class="line">        try:</span><br><span class="line">            url &#x3D; start_url + &#39;&amp;s&#x3D;&#39; + str(44 * i)</span><br><span class="line">            html &#x3D; getHTMLText(url)</span><br><span class="line">            parsePage(infoList, html)</span><br><span class="line">        except:</span><br><span class="line">            continue</span><br><span class="line">    printGoodsList(infoList)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure><h1 id="股票数据定向爬虫实例"><a href="#股票数据定向爬虫实例" class="headerlink" title="股票数据定向爬虫实例"></a>股票数据定向爬虫实例</h1><hr><p>候选数据网站选择：<a href="http://finance.sina.com.cn/stock/" target="_blank" rel="noopener">新浪股票</a>、<a href="https://gupiao.baidu.com/stock/" target="_blank" rel="noopener">百度股票</a>、<a href="http://quote.eastmoney.com/center/gridlist.html#hs_a_board" target="_blank" rel="noopener">东方财富网</a><br><strong>数据网站选择原则：</strong><br>股票信息静态存在于HTML页面中，非js代码生成，没有Robots协议限制。</p><p><strong>流程图：</strong></p><p><img src="https://i.loli.net/2020/06/19/DvXQAj4ikfZswap.png" alt="流程图"></p><p><strong>代码编写：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">#CrawBaiduStocksB.py</span><br><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import traceback</span><br><span class="line">import re</span><br><span class="line"> </span><br><span class="line">def getHTMLText(url, code&#x3D;&quot;utf-8&quot;):</span><br><span class="line">    try:</span><br><span class="line">        r &#x3D; requests.get(url)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding &#x3D; code</span><br><span class="line">        return r.text</span><br><span class="line">    except:</span><br><span class="line">        return &quot;&quot;</span><br><span class="line"> </span><br><span class="line">def getStockList(lst, stockURL):</span><br><span class="line">    html &#x3D; getHTMLText(stockURL, &quot;GB2312&quot;)</span><br><span class="line">    soup &#x3D; BeautifulSoup(html, &#39;html.parser&#39;) </span><br><span class="line">    a &#x3D; soup.find_all(&#39;a&#39;)</span><br><span class="line">    for i in a:</span><br><span class="line">        try:</span><br><span class="line">            href &#x3D; i.attrs[&#39;href&#39;]</span><br><span class="line">            lst.append(re.findall(r&quot;[s][hz]\d&#123;6&#125;&quot;, href)[0])</span><br><span class="line">        except:</span><br><span class="line">            continue</span><br><span class="line"> </span><br><span class="line">def getStockInfo(lst, stockURL, fpath):</span><br><span class="line">    count &#x3D; 0</span><br><span class="line">    for stock in lst:</span><br><span class="line">        url &#x3D; stockURL + stock + &quot;.html&quot;</span><br><span class="line">        html &#x3D; getHTMLText(url)</span><br><span class="line">        try:</span><br><span class="line">            if html&#x3D;&#x3D;&quot;&quot;:</span><br><span class="line">                continue</span><br><span class="line">            infoDict &#x3D; &#123;&#125;</span><br><span class="line">            soup &#x3D; BeautifulSoup(html, &#39;html.parser&#39;)</span><br><span class="line">            stockInfo &#x3D; soup.find(&#39;div&#39;,attrs&#x3D;&#123;&#39;class&#39;:&#39;stock-bets&#39;&#125;)</span><br><span class="line"> </span><br><span class="line">            name &#x3D; stockInfo.find_all(attrs&#x3D;&#123;&#39;class&#39;:&#39;bets-name&#39;&#125;)[0]</span><br><span class="line">            infoDict.update(&#123;&#39;股票名称&#39;: name.text.split()[0]&#125;)</span><br><span class="line">             </span><br><span class="line">            keyList &#x3D; stockInfo.find_all(&#39;dt&#39;)</span><br><span class="line">            valueList &#x3D; stockInfo.find_all(&#39;dd&#39;)</span><br><span class="line">            for i in range(len(keyList)):</span><br><span class="line">                key &#x3D; keyList[i].text</span><br><span class="line">                val &#x3D; valueList[i].text</span><br><span class="line">                infoDict[key] &#x3D; val</span><br><span class="line">             </span><br><span class="line">            with open(fpath, &#39;a&#39;, encoding&#x3D;&#39;utf-8&#39;) as f:</span><br><span class="line">                f.write( str(infoDict) + &#39;\n&#39; )</span><br><span class="line">                count &#x3D; count + 1</span><br><span class="line">                print(&quot;\r当前进度: &#123;:.2f&#125;%&quot;.format(count*100&#x2F;len(lst)),end&#x3D;&quot;&quot;)</span><br><span class="line">        except:</span><br><span class="line">            count &#x3D; count + 1</span><br><span class="line">            print(&quot;\r当前进度: &#123;:.2f&#125;%&quot;.format(count*100&#x2F;len(lst)),end&#x3D;&quot;&quot;)</span><br><span class="line">            continue</span><br><span class="line"> </span><br><span class="line">def main():</span><br><span class="line">    stock_list_url &#x3D; &#39;http:&#x2F;&#x2F;quote.eastmoney.com&#x2F;center&#x2F;gridlist.html#hs_a_board&#39;</span><br><span class="line">    stock_info_url &#x3D; &#39;https:&#x2F;&#x2F;gupiao.baidu.com&#x2F;stock&#x2F;&#39;</span><br><span class="line">    output_file &#x3D; &#39;D:&#x2F;BaiduStockInfo.txt&#39;</span><br><span class="line">    slist&#x3D;[]</span><br><span class="line">    getStockList(slist, stock_list_url)</span><br><span class="line">    getStockInfo(slist, stock_info_url, output_file)</span><br><span class="line"> </span><br><span class="line">main()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;淘宝商品比价定向爬虫实例&quot;&gt;&lt;a href=&quot;#淘宝商品比价定向爬虫实例&quot; class=&quot;headerlink&quot; title=&quot;淘宝商品比价定向爬虫实例&quot;&gt;&lt;/a&gt;淘宝商品比价定向爬虫实例&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;流程图：&lt;/strong&gt;&lt;/p&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Python网络爬虫与信息提取（第3周 网络爬虫实战 单元七 Re库入门）</title>
    <link href="https://jluliyuxi.github.io/2020/06/17/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC3%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%20%E5%8D%95%E5%85%83%E4%B8%83%20Re%E5%BA%93%E5%85%A5%E9%97%A8%EF%BC%89/"/>
    <id>https://jluliyuxi.github.io/2020/06/17/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC3%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%20%E5%8D%95%E5%85%83%E4%B8%83%20Re%E5%BA%93%E5%85%A5%E9%97%A8%EF%BC%89/</id>
    <published>2020-06-17T09:52:20.047Z</published>
    <updated>2020-06-22T09:24:49.704Z</updated>
    
    <content type="html"><![CDATA[<h1 id="正则表达式的概念"><a href="#正则表达式的概念" class="headerlink" title="正则表达式的概念"></a>正则表达式的概念</h1><p>Regular expression–Re<br><strong>·通用的字符串表达框架<br>·简洁表达一组字符串的表达式<br>·针对字符串表达“简洁”和“特征”思想的工具<br>·判断某字符串的特征归属</strong>        </p><h1 id="正则表达式的语法"><a href="#正则表达式的语法" class="headerlink" title="正则表达式的语法"></a>正则表达式的语法</h1><p>eg:<strong>P(Y|YT|YTH|YTHO)?N</strong><br>正则表达式由字符和操作符构成       </p><p><strong>正则表达式常用的操作符</strong></p><table><thead><tr><th align="center">操作符</th><th align="center">说明</th><th align="center">实例</th></tr></thead><tbody><tr><td align="center">.</td><td align="center">表示任何单个字符</td><td align="center"></td></tr><tr><td align="center">[ ]</td><td align="center">字符集，对单个字符给出取值范围</td><td align="center">[abe]表示a、 b、c, [a-z]表示a到z单个字符</td></tr><tr><td align="center">[^ ]</td><td align="center">非字符集，对单个字符给出排除范围</td><td align="center">[^abc]表示非a或b或c的单个字符</td></tr><tr><td align="center">*</td><td align="center">前一个字符0次或无限次扩展</td><td align="center">abc*表示ab、abc、abce、abecc等</td></tr><tr><td align="center">+</td><td align="center">前一个字符1次或无限次扩展</td><td align="center">abc+表示abc、abcc、 abccc等</td></tr><tr><td align="center">?</td><td align="center">前一个字符0次或1次扩展</td><td align="center">abc?表示ab、abc</td></tr><tr><td align="center">`</td><td align="center">`</td><td align="center">左右表达式任意一个</td></tr><tr><td align="center"><code>{m}</code></td><td align="center">扩展前一个字符m次</td><td align="center">ab{2}c表示abbc</td></tr><tr><td align="center"><code>{m,n}</code></td><td align="center">扩展前一个字符m至n次(含n)</td><td align="center">ab{1,2}c表示abc、abbc</td></tr><tr><td align="center">^</td><td align="center">匹配字符串开头</td><td align="center">^abc表示abc且在一个字符串的开头</td></tr><tr><td align="center">$</td><td align="center">匹配字符串结尾</td><td align="center">abc$表示abc且在一个字符串的结尾</td></tr><tr><td align="center">()</td><td align="center">分组标记，内部只能使用`</td><td align="center">`操作符</td></tr><tr><td align="center">\d</td><td align="center">数字，等价于[0-9]</td><td align="center"></td></tr><tr><td align="center">\w</td><td align="center">单词字符，等价于[A-Za-z0-9_]</td><td align="center"></td></tr></tbody></table><p><strong>正则表达式语法实例</strong></p><table><thead><tr><th align="center">正则表达式</th><th align="center">对应字符串</th></tr></thead><tbody><tr><td align="center">`P(Y</td><td align="center">YT</td></tr><tr><td align="center">PYTHON+</td><td align="center">‘PYTHON’ ‘PYTHONN’ ‘PYTHONNN’…</td></tr><tr><td align="center">PY[TH]ON</td><td align="center">‘PYTON’ ‘PYHON’</td></tr><tr><td align="center">PY[^TH]?ON</td><td align="center">‘PYON’ ‘PTaon’ ‘PYbON’ ‘PYcON’…</td></tr><tr><td align="center">PY{:3}N</td><td align="center">‘PN’ ‘PYN’ ‘PYYN’ ‘PYYYN’</td></tr></tbody></table><p><strong>经典正则表达式实例</strong><br>^[A-Za-z]+$&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;由26个字母组成的字符串<br>^[A-Za-z0-9]+$&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;由26个字母和数字组成的字符串<br>^-?\d+$&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&emsp;&ensp;整数形式的字符串<br><code>^[0-9]*[1-9][0-9]*$</code>&ensp;&ensp;&ensp;&ensp;&ensp;正整数形式的字符串<br>[1-9]\d{5}&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;中国境内邮政编码，6位<br>[\u4e00-\u9fa5]&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;匹配中文字符<br>\d{3}-\d{8}|\d{4}-\d{7}&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;国内电话号码，010-68913536</p><h1 id="Re库的基本使用"><a href="#Re库的基本使用" class="headerlink" title="Re库的基本使用"></a>Re库的基本使用</h1><p>正则表达式的表达类型<br><strong>·raw string类型(原生字符串)<br>·sting类型，较繁琐</strong>      </p><h2 id="Re库主要功能函数"><a href="#Re库主要功能函数" class="headerlink" title="Re库主要功能函数"></a>Re库主要功能函数</h2><table><thead><tr><th align="center">函数</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">re.search()</td><td align="center">在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象</td></tr><tr><td align="center">re.match()</td><td align="center">从一个字符串的开始位置起匹配正则表达式，返回match对象</td></tr><tr><td align="center">re.findall()</td><td align="center">搜索字符串，以列表类型返回全部能匹配的子串</td></tr><tr><td align="center">re.split()</td><td align="center">将一个字符串按照正则表达式匹配结果进行分割，返回列表类型</td></tr><tr><td align="center">re.finditer()</td><td align="center">搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象</td></tr><tr><td align="center">re.sub()</td><td align="center">在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串</td></tr></tbody></table><p><strong>re.search(parttern,string,flags=0)</strong><br>※在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象。<br>pattern:正则表达式的字符串或原生字符串表示<br>strings:待匹配字符串<br>flags:正则表达式使用时的控制标记<br>flags常用控制标记：    </p><table><thead><tr><th align="center">常用标记</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">re.I(re.IGNORECASE)</td><td align="center">忽略正则表达式的大小写，[A-Z]能够匹配小写字符</td></tr><tr><td align="center">re.M(re.MULTLINE)</td><td align="center">正则表达式中的^操作符能够将给定字符串的每行当作匹配开始</td></tr><tr><td align="center">re.S(re.DOTALL)</td><td align="center">正则表达式中的.操作符能够匹配所有字符，默认匹配除换行外的所有字符</td></tr></tbody></table><p><strong>re.match(pattern,string,flags=0)</strong><br>※从一个字符串的开始位置起匹配正则表达式，返回match对象。<br>pattern:正则表达式的字符串或原生字符串表示<br>string:待匹配字符串<br>flags:正则表达式使用时的控制标记     </p><p><strong>re.findall(pattern,string,flags=0)</strong><br>※搜索字符串，以列表类型返回全部能匹配的子串。<br>pattern:正则表达式的字符串或原生字符串表示<br>string;待匹配字符串<br>flags:正则表达式使用时的控制标记     </p><p><strong>re.split(pattern, string, maxsplit=0, flags=0)</strong><br>※将一个字符串按照正则表达式匹配结果进行分割，返回列表类型。<br>pattern:正则表达式的字符串或原生字符串表示<br>string;待匹配字符串<br>maxsplit:最大分割数，剩余部分作为最后一个元素输出<br>flags:正则表达式使用时的控制标记     </p><p><strong>re.finditer(pattern,string,flags=0)</strong><br>※搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象。<br>pattern:正则表达式的字符串或原生字符串表示<br>string:待匹配字符串<br>flags:正则表达式使用时的控制标记     </p><p><strong>re.sub(pattern, repl, string, count=0, flags=0)</strong><br>※在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串。<br>pattern:正则表达式的字符串或原生字符串表示<br>repl:替换匹配字符串的字符串<br>string:待匹配字符串<br>count:匹配的最大替换次数<br>flags:正则表达式使用时的控制标记     </p><p><strong>regex = re.compile(pattern, flags=0)</strong><br>※将正则表达式的字符串形式编译成正则表达式对象<br>pattern:正则表达式的字符串或原生字符串表示<br>flags:正则表达式使用时的控制标记     </p><h2 id="Re库的Match对象"><a href="#Re库的Match对象" class="headerlink" title="Re库的Match对象"></a>Re库的Match对象</h2><p><strong>Match对象的属性</strong>  </p><table><thead><tr><th align="center">属性</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">.string</td><td align="center">待匹配的文本</td></tr><tr><td align="center">.re</td><td align="center">匹配时使用的pattern对象(正则表达式)</td></tr><tr><td align="center">.pos</td><td align="center">正则表达式搜索文本的开始位置</td></tr><tr><td align="center">.endpos</td><td align="center">正则表达式搜索文本的结束位置</td></tr></tbody></table><p><strong>Match对象的方法</strong></p><table><thead><tr><th align="center">方法</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">.group(0)</td><td align="center">获得匹配后的字符串</td></tr><tr><td align="center">.start()</td><td align="center">匹配字符串在原始字符串的开始位置</td></tr><tr><td align="center">.end()</td><td align="center">匹配字符串在原始字符串的结束位置</td></tr><tr><td align="center">.span()</td><td align="center">返回(.start(),.end())</td></tr></tbody></table><h2 id="Re库的贪婪匹配和最小匹配"><a href="#Re库的贪婪匹配和最小匹配" class="headerlink" title="Re库的贪婪匹配和最小匹配"></a>Re库的贪婪匹配和最小匹配</h2><p><strong>Re库默认采用贪婪匹配，即输出匹配最长的字符串</strong>     </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">match &#x3D; re.search(r&#39;PY.*N&#39;, &#39;PYANBNCNDN&#39;)</span><br><span class="line">print(match.group(0))</span><br></pre></td></tr></table></figure><p>输出：<code>PYANBNCNDN</code></p><p><strong>若要输出最小匹配字符串：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">match &#x3D; re.search(r&#39;PY.*?N&#39;, &#39;PYANBNCNDN&#39;)</span><br><span class="line">print(match.group(0))</span><br></pre></td></tr></table></figure><p>输出：<code>PYAN</code></p><p><strong>最小匹配操作符</strong></p><table><thead><tr><th align="center">操作符</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">*?</td><td align="center">前一个字符0次或无限次扩展，最小匹配</td></tr><tr><td align="center">+?</td><td align="center">前一个字符1次或无限次扩展，最小匹配</td></tr><tr><td align="center">??</td><td align="center">前一个字符0次或1次扩展，最小匹配</td></tr><tr><td align="center"><code>{m,n}?</code></td><td align="center">扩展前一个字符m至n次(含n)，最小匹配</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;正则表达式的概念&quot;&gt;&lt;a href=&quot;#正则表达式的概念&quot; class=&quot;headerlink&quot; title=&quot;正则表达式的概念&quot;&gt;&lt;/a&gt;正则表达式的概念&lt;/h1&gt;&lt;p&gt;Regular expression–Re&lt;br&gt;&lt;strong&gt;·通用的字符串表达框架&lt;b
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Python网络爬虫与信息提取（第2周 网络爬虫提取 单元六 中国大学排名爬虫）</title>
    <link href="https://jluliyuxi.github.io/2020/06/16/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC2%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%8F%90%E5%8F%96%20%E5%8D%95%E5%85%83%E5%85%AD%20%E4%B8%AD%E5%9B%BD%E5%A4%A7%E5%AD%A6%E6%8E%92%E5%90%8D%E7%88%AC%E8%99%AB%EF%BC%89/"/>
    <id>https://jluliyuxi.github.io/2020/06/16/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC2%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%8F%90%E5%8F%96%20%E5%8D%95%E5%85%83%E5%85%AD%20%E4%B8%AD%E5%9B%BD%E5%A4%A7%E5%AD%A6%E6%8E%92%E5%90%8D%E7%88%AC%E8%99%AB%EF%BC%89/</id>
    <published>2020-06-16T11:07:04.745Z</published>
    <updated>2020-06-22T09:22:14.242Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2020/06/22/ZTaXs6i9SwBAzQt.png" alt="程序流程图"></p><h1 id="定向爬虫实例"><a href="#定向爬虫实例" class="headerlink" title="定向爬虫实例"></a>定向爬虫实例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import bs4</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line">def getHTMLText(url):</span><br><span class="line">    try:</span><br><span class="line">        r &#x3D; requests.get(url, timeout &#x3D; 30)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding &#x3D; r.apparent_encoding</span><br><span class="line">        return  r.text</span><br><span class="line">    except:</span><br><span class="line">        return &quot;&quot;</span><br><span class="line"></span><br><span class="line">def fillUnivList(ulist, html):</span><br><span class="line">    soup &#x3D; BeautifulSoup(html, &quot;html.parser&quot;)</span><br><span class="line">    for tr in soup.find(&#39;tbody&#39;).children:</span><br><span class="line">        if isinstance(tr, bs4.element.Tag):</span><br><span class="line">            tds &#x3D; tr(&#39;td&#39;)</span><br><span class="line">            ulist.append([tds[0].string, tds[1].string, tds[4].string])</span><br><span class="line"></span><br><span class="line">def printUnivList(ulist, num):</span><br><span class="line">    print(&quot;&#123;:^10&#125;\t&#123;:^6&#125;\t&#123;:^10&#125;&quot;.format(&quot;排名&quot;, &quot;学校名称&quot;, &quot;总分&quot;))</span><br><span class="line">    for i in range(num):</span><br><span class="line">        u &#x3D; ulist[i]</span><br><span class="line">        print(&quot;&#123;:^10&#125;\t&#123;:^6&#125;\t&#123;:^10&#125;&quot;.format(u[0], u[1], u[2]))</span><br><span class="line">    print(&quot;Suc&quot; + str(num))</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    unifo &#x3D; []</span><br><span class="line">    url &#x3D; &quot;http:&#x2F;&#x2F;www.zuihaodaxue.com&#x2F;zuihaodaxuepaiming2020.html&quot;</span><br><span class="line">    html &#x3D; getHTMLText(url)</span><br><span class="line">    fillUnivList(unifo, html)</span><br><span class="line">    printUnivList(unifo, 20)</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure><p><strong>输出结果</strong></p><p><img src="https://i.loli.net/2020/06/16/bB91nZezTtF7Usd.png" alt="输出结果"></p><h1 id="定向爬虫实例优化"><a href="#定向爬虫实例优化" class="headerlink" title="定向爬虫实例优化"></a>定向爬虫实例优化</h1><p><strong>中文对齐问题的解决</strong>：采用中文字符的空格填充<code>chr(12288)</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">#旨在优化中文字符空格的问题</span><br><span class="line">import requests</span><br><span class="line">import bs4</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line">def getHTMLText(url):</span><br><span class="line">    try:</span><br><span class="line">        r &#x3D; requests.get(url, timeout &#x3D; 30)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding &#x3D; r.apparent_encoding</span><br><span class="line">        return  r.text</span><br><span class="line">    except:</span><br><span class="line">        return &quot;&quot;</span><br><span class="line"></span><br><span class="line">def fillUnivList(ulist, html):</span><br><span class="line">    soup &#x3D; BeautifulSoup(html, &quot;html.parser&quot;)</span><br><span class="line">    for tr in soup.find(&#39;tbody&#39;).children:</span><br><span class="line">        if isinstance(tr, bs4.element.Tag):</span><br><span class="line">            tds &#x3D; tr(&#39;td&#39;)</span><br><span class="line">            ulist.append([tds[0].string, tds[1].string, tds[4].string])</span><br><span class="line"></span><br><span class="line">def printUnivList(ulist, num):</span><br><span class="line">    tplt &#x3D; &quot;&#123;0:^10&#125;\t&#123;1:&#123;3&#125;^10&#125;\t&#123;2:^10&#125;&quot;</span><br><span class="line">    print(tplt.format(&quot;排名&quot;, &quot;学校名称&quot;, &quot;总分&quot;, chr(12288)))</span><br><span class="line">    for i in range(num):</span><br><span class="line">        u &#x3D; ulist[i]</span><br><span class="line">        print(tplt.format(u[0], u[1], u[2], chr(12288)))</span><br><span class="line">    print(&quot;Suc&quot; + str(num))</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    unifo &#x3D; []</span><br><span class="line">    url &#x3D; &quot;http:&#x2F;&#x2F;www.zuihaodaxue.com&#x2F;zuihaodaxuepaiming2020.html&quot;</span><br><span class="line">    html &#x3D; getHTMLText(url)</span><br><span class="line">    fillUnivList(unifo, html)</span><br><span class="line">    printUnivList(unifo, 20)</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure><p><strong>输出结果</strong></p><p><img src="https://i.loli.net/2020/06/16/l6EbVGoksfxhXza.png" alt="输出结果"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/06/22/ZTaXs6i9SwBAzQt.png&quot; alt=&quot;程序流程图&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;定向爬虫实例&quot;&gt;&lt;a href=&quot;#定向爬虫实例&quot; class=&quot;headerlink&quot; title=
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Python网络爬虫与信息提取（第2周 网络爬虫提取 单元五 信息组织与提取方法）</title>
    <link href="https://jluliyuxi.github.io/2020/06/16/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC2%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%8F%90%E5%8F%96%20%E5%8D%95%E5%85%83%E4%BA%94%20%E4%BF%A1%E6%81%AF%E7%BB%84%E7%BB%87%E4%B8%8E%E6%8F%90%E5%8F%96%E6%96%B9%E6%B3%95%EF%BC%89/"/>
    <id>https://jluliyuxi.github.io/2020/06/16/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC2%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%8F%90%E5%8F%96%20%E5%8D%95%E5%85%83%E4%BA%94%20%E4%BF%A1%E6%81%AF%E7%BB%84%E7%BB%87%E4%B8%8E%E6%8F%90%E5%8F%96%E6%96%B9%E6%B3%95%EF%BC%89/</id>
    <published>2020-06-16T07:26:01.902Z</published>
    <updated>2020-06-22T09:06:14.551Z</updated>
    
    <content type="html"><![CDATA[<h1 id="信息标记的三种形式"><a href="#信息标记的三种形式" class="headerlink" title="信息标记的三种形式"></a>信息标记的三种形式</h1><h2 id="XML（eXtensible-Markup-Language）"><a href="#XML（eXtensible-Markup-Language）" class="headerlink" title="XML（eXtensible Markup Language）"></a>XML（eXtensible Markup Language）</h2><p>标签中含有内容时：<code>&lt;name&gt; ... &lt;/name&gt;</code>  </p><p>标签中不含内容时：<code>&lt;name/&gt;</code>     </p><p>注释：<code>&lt;!-- --&gt;</code></p><h2 id="JSON-JavaScript-Object-Notation"><a href="#JSON-JavaScript-Object-Notation" class="headerlink" title="JSON(JavaScript Object Notation)"></a>JSON(JavaScript Object Notation)</h2><p>“key” : “value”<br>“key” : [“value1”,”value2”]<br>“key” : {“subkey”:”subvalue”}<br>“key” : {<br>&ensp;&ensp;”name1” : “value1”<br>&ensp;&ensp;”name2” : “value2”<br>}   </p><h2 id="YAML"><a href="#YAML" class="headerlink" title="YAML"></a>YAML</h2><p><strong>无类型键值对</strong> key:name     </p><p><strong>通过缩进表达所属关系</strong>：<br>name :<br>&ensp;&ensp;newname :<br>&ensp;&ensp;oldname :     </p><p><strong>用-号表达并列关系</strong>：<br>name :<br>-name1<br>-name2      </p><p><strong>用|表示整块数据 #表示注释</strong>      </p><h1 id="三种信息标记形式的比较"><a href="#三种信息标记形式的比较" class="headerlink" title="三种信息标记形式的比较"></a>三种信息标记形式的比较</h1><p>XML Internet.上的信息交互与传递。<br>JSON 移动应用云端和节点的信息通信，无注释。<br>YAML 各类系统的配置文件，有注释易读。       </p><h1 id="信息提取的一般方法"><a href="#信息提取的一般方法" class="headerlink" title="信息提取的一般方法"></a>信息提取的一般方法</h1><h2 id="方法一-完整解析信息的标记形式，再提取关键信息。"><a href="#方法一-完整解析信息的标记形式，再提取关键信息。" class="headerlink" title="方法一:完整解析信息的标记形式，再提取关键信息。"></a>方法一:完整解析信息的标记形式，再提取关键信息。</h2><p>XML JSON YAML<br><strong>需要标记解析器</strong><br>例如: bs4库的标签树遍历<br>优点:信息解析准确<br>缺点:提取过程繁琐，速度慢。      </p><h2 id="方法二-无视标记形式，直接搜索关键信息。"><a href="#方法二-无视标记形式，直接搜索关键信息。" class="headerlink" title="方法二:无视标记形式，直接搜索关键信息。"></a>方法二:无视标记形式，直接搜索关键信息。</h2><p><strong>搜索</strong><br>对信息的文本查找函数即可。<br>优点:提取过程简洁，速度较快。<br>缺点:提取结果准确性与信息内容相关。      </p><h1 id="基于bs4库的HTML内容查找方法"><a href="#基于bs4库的HTML内容查找方法" class="headerlink" title="基于bs4库的HTML内容查找方法"></a>基于bs4库的HTML内容查找方法</h1><p><strong>&lt; &gt;.find_ all(name, attrs, recursive, string, **kwargs)</strong><br>返回一个列表类型，存储查找的结果。<br><strong>name</strong>:对标签名称的检索字符串。<br><strong>attrs</strong>:对标签属性值的检索字符串，可标注属性检索。<br><strong>recursive</strong>:是否对子孙全部检索，默认True。<br><strong>string: &lt; &gt;… &lt; / &gt;</strong>:中字符串区域的检索字符串。      </p><p><strong>扩展方法</strong></p><table><thead><tr><th align="center">方法</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">&lt;&gt;.find()</td><td align="center">搜索且只返回一个结果，字符串类型，同.find_all()参数</td></tr><tr><td align="center">&lt;&gt;.find_parents()</td><td align="center">在先辈节点中搜索，返回列表类型，同.find_all()参数</td></tr><tr><td align="center">&lt;&gt;.find_parent()</td><td align="center">在先辈节点中返回一个结果，字符串类型，同.find()参数</td></tr><tr><td align="center">&lt;&gt;.find_next_siblings()</td><td align="center">在后续平行节点中搜索，返回列表类型，同.find_all()参数</td></tr><tr><td align="center">&lt;&gt;.find_ next sibling()</td><td align="center">在后续平行节点中返回一个结果，字符串类型,同.find()参数</td></tr><tr><td align="center">&lt;&gt;.find_previous_siblings()</td><td align="center">在前序平行节点中搜索，返回列表类型，同.find_all()参数</td></tr><tr><td align="center">&lt;&gt;.find_previous_sibling()</td><td align="center">在前序平行节点中返回一个结果，字符串类型，同.find()参数</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;信息标记的三种形式&quot;&gt;&lt;a href=&quot;#信息标记的三种形式&quot; class=&quot;headerlink&quot; title=&quot;信息标记的三种形式&quot;&gt;&lt;/a&gt;信息标记的三种形式&lt;/h1&gt;&lt;h2 id=&quot;XML（eXtensible-Markup-Language）&quot;&gt;&lt;a h
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Python网络爬虫与信息提取（第2周 网络爬虫提取 单元四 BeautifulSoup库入门）</title>
    <link href="https://jluliyuxi.github.io/2020/06/11/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC2%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%8F%90%E5%8F%96%20%E5%8D%95%E5%85%83%E5%9B%9B%20BeautifulSoup%E5%BA%93%E5%85%A5%E9%97%A8%EF%BC%89/"/>
    <id>https://jluliyuxi.github.io/2020/06/11/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC2%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%8F%90%E5%8F%96%20%E5%8D%95%E5%85%83%E5%9B%9B%20BeautifulSoup%E5%BA%93%E5%85%A5%E9%97%A8%EF%BC%89/</id>
    <published>2020-06-11T07:48:30.595Z</published>
    <updated>2020-06-22T09:23:00.508Z</updated>
    
    <content type="html"><![CDATA[<h1 id="BeautifulSoup库的安装"><a href="#BeautifulSoup库的安装" class="headerlink" title="BeautifulSoup库的安装"></a>BeautifulSoup库的安装</h1><p>命令行内输入<code>pip install beautifulsoup4</code>      </p><h1 id="BeautifulSoup库的测试"><a href="#BeautifulSoup库的测试" class="headerlink" title="BeautifulSoup库的测试"></a>BeautifulSoup库的测试</h1><p>在任一浏览器中输入<code>http://python123.io/ws/demo.html</code><br>查看网站的源代码    </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">r &#x3D; requests.get(&#39;http:&#x2F;&#x2F;python123.io&#x2F;ws&#x2F;demo.html&#39;)</span><br><span class="line">demo &#x3D; r.text #获取网站的源代码</span><br><span class="line">soup &#x3D; BeautifulSoup(demo, &#39;html.parser&#39;) #对demo进行html的解析</span><br><span class="line">print(soup.prettify())</span><br></pre></td></tr></table></figure><p><strong>主要用法</strong>    </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">soup &#x3D; BeautifulSoup(&#39;&lt;p&gt;data&lt;&#x2F;p&gt;&#39;,&#39;html.parser&#39;)</span><br></pre></td></tr></table></figure><h1 id="BeautifulSoup库的基本元素"><a href="#BeautifulSoup库的基本元素" class="headerlink" title="BeautifulSoup库的基本元素"></a>BeautifulSoup库的基本元素</h1><p><img src="https://i.loli.net/2020/06/11/cuA1DqJTfeilpkj.png" alt=""></p><p>属性是由键值对构成的<br><strong>BeautifulSoup库解析器</strong></p><table><thead><tr><th align="center">解析器</th><th align="center">使用方法</th><th align="center">条件</th></tr></thead><tbody><tr><td align="center">bs4的HTML解析器</td><td align="center">BeautifulSoup(mk,’html.parser)</td><td align="center">安装bs4库</td></tr><tr><td align="center">lxml的HTML解析器</td><td align="center">BeautifulSoup(mk,’lxml’)</td><td align="center">pip install lxml</td></tr><tr><td align="center">lxml的XML解析器</td><td align="center">BeautifulSoup(mk,’ xml’)</td><td align="center">pip install lxml</td></tr><tr><td align="center">html5lib的解析器</td><td align="center">BeautifulSoup( mk,’ html5lib’)</td><td align="center">pip install html5lib</td></tr></tbody></table><p><strong>BeautifulSoup类的基本元素</strong></p><table><thead><tr><th align="center">基本元素</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">Tag</td><td align="center">标签，最基本的信息组织单元，分别用&lt;&gt;和&lt;/&gt;标明开头和结尾</td></tr><tr><td align="center">Name</td><td align="center">标签的名字，&lt; p &gt;…&lt; /p &gt;的名字是’p’， 格式: &lt; tag &gt;.name</td></tr><tr><td align="center">Attributes</td><td align="center">标签的属性，字典形式组织，格式: &lt; tag &gt; .attrs</td></tr><tr><td align="center">NavigableString</td><td align="center">标签内非属性字符串，&lt;&gt;…&lt;/&gt;中字符串， 格式: &lt; tag     &gt;.string</td></tr><tr><td align="center">Comment</td><td align="center">标签内字符串的注释部分，一种特殊的Comment类型</td></tr></tbody></table><p><strong>获取Tag的方法</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">r &#x3D; requests.get(&#39;http:&#x2F;&#x2F;python123.io&#x2F;ws&#x2F;demo.html&#39;)</span><br><span class="line">demo &#x3D; r.text #获取网站的源代码</span><br><span class="line">soup &#x3D; BeautifulSoup(demo, &#39;html.parser&#39;)</span><br><span class="line">print(soup.title)</span><br><span class="line">tag &#x3D; soup.a</span><br><span class="line">print(tag)</span><br></pre></td></tr></table></figure><p><strong>获取Name的方法</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">r &#x3D; requests.get(&#39;http:&#x2F;&#x2F;python123.io&#x2F;ws&#x2F;demo.html&#39;)</span><br><span class="line">demo &#x3D; r.text #获取网站的源代码</span><br><span class="line">soup &#x3D; BeautifulSoup(demo, &#39;html.parser&#39;)</span><br><span class="line">tag &#x3D; soup.a</span><br><span class="line">print(soup.a.name)</span><br><span class="line">print(soup.a.parent.name)</span><br><span class="line">print(soup.a.parent.parent.name)</span><br></pre></td></tr></table></figure><p><strong>获取Attributes的方法</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">r &#x3D; requests.get(&#39;http:&#x2F;&#x2F;python123.io&#x2F;ws&#x2F;demo.html&#39;)</span><br><span class="line">demo &#x3D; r.text #获取网站的源代码</span><br><span class="line">soup &#x3D; BeautifulSoup(demo, &#39;html.parser&#39;)</span><br><span class="line">tag &#x3D; soup.a</span><br><span class="line">print(tag.attrs)</span><br><span class="line">print(tag.attrs[&#39;class&#39;]) #获取class对应的值</span><br><span class="line">print(tag.attrs[&#39;href&#39;]) #获取标签链接</span><br><span class="line">print(type(tag.attrs)) #获取标签属性的类型</span><br></pre></td></tr></table></figure><p><strong>获取NavigableString的方法</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">r &#x3D; requests.get(&#39;http:&#x2F;&#x2F;python123.io&#x2F;ws&#x2F;demo.html&#39;)</span><br><span class="line">demo &#x3D; r.text #获取网站的源代码</span><br><span class="line">soup &#x3D; BeautifulSoup(demo, &#39;html.parser&#39;)</span><br><span class="line">print(soup.a.string)</span><br><span class="line">print((soup.p.string))</span><br><span class="line">print(type(soup.p.string))</span><br></pre></td></tr></table></figure><p><strong>获取Comment的方法</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">r &#x3D; requests.get(&#39;http:&#x2F;&#x2F;python123.io&#x2F;ws&#x2F;demo.html&#39;)</span><br><span class="line">demo &#x3D; r.text #获取网站的源代码</span><br><span class="line">soup &#x3D; BeautifulSoup(demo, &#39;html.parser&#39;)</span><br><span class="line">print(soup.p)</span><br><span class="line">newsoup &#x3D; BeautifulSoup(&#39;&lt;p class&#x3D;&quot;title&quot;&gt;&lt;b&gt;The demo python introduces several python courses.&lt;&#x2F;b&gt;&lt;&#x2F;p&gt;&#39;, &#39;html.parser&#39;)</span><br><span class="line">print(newsoup.b.string)</span><br><span class="line">print(type(newsoup.b.string))</span><br></pre></td></tr></table></figure><h1 id="基于bs4库的HTML内容遍历方法"><a href="#基于bs4库的HTML内容遍历方法" class="headerlink" title="基于bs4库的HTML内容遍历方法"></a>基于bs4库的HTML内容遍历方法</h1><p><img src="https://i.loli.net/2020/06/11/VOEiaJBo3fGAF1t.png" alt="HTML基本格式及三种遍历方法"></p><p><strong>标签树的下行遍历</strong></p><table><thead><tr><th align="center">属性</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">.contents</td><td align="center">子节点的列表，将&lt; tag &gt;所有儿子节点存入列表</td></tr><tr><td align="center">.children</td><td align="center">子节点的迭代类型，与.contents类似，用于循环遍历儿子节点</td></tr><tr><td align="center">.descendants</td><td align="center">子孙节点的迭代类型，包含所有子孙节点，用于循环遍历</td></tr></tbody></table><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">r &#x3D; requests.get(&#39;http:&#x2F;&#x2F;python123.io&#x2F;ws&#x2F;demo.html&#39;)</span><br><span class="line">demo &#x3D; r.text #获取网站的源代码</span><br><span class="line">soup &#x3D; BeautifulSoup(demo, &#39;html.parser&#39;)</span><br><span class="line">print(soup.head)</span><br><span class="line">print(soup.head.contents)</span><br><span class="line">print(soup.body.contents)</span><br><span class="line">print(soup.body.contents[1])</span><br></pre></td></tr></table></figure><p>遍历儿子节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for child in soup.body.children:</span><br><span class="line">    pirnt(child)</span><br></pre></td></tr></table></figure><p><strong>标签树的上行遍历</strong></p><table><thead><tr><th align="center">属性</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">.parent</td><td align="center">节点的父亲标签</td></tr><tr><td align="center">.parents</td><td align="center">节点先辈标签的迭代类型，用于循环遍历先辈节点</td></tr></tbody></table><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">soup &#x3D; BeautifulSoup(demo, &#39;html.parser&#39;)</span><br><span class="line">for parent in soup.a.parents:</span><br><span class="line">    if parent is None:</span><br><span class="line">        print(parent)</span><br><span class="line">    else:</span><br><span class="line">        print(parent.name) #打印soup.a标签所有的先辈</span><br></pre></td></tr></table></figure><p><strong>标签树的平行遍历</strong></p><table><thead><tr><th align="center">属性</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">.next_sibling</td><td align="center">返回按照HTML文本顺序的下一个平行节点标签</td></tr><tr><td align="center">.previous_sibling</td><td align="center">返回按照HTML文本顺序的上一 个平行节点标签</td></tr><tr><td align="center">.next_siblings</td><td align="center">迭代类型，返回按照HTML文本顺序的后续所有平行节点标签</td></tr><tr><td align="center">.previous_siblings</td><td align="center">迭代类型，返回按照HTML文本顺序的前续所有平行节点标签</td></tr></tbody></table><p><img src="https://i.loli.net/2020/06/11/PKTVZSgwBEQtL6a.png" alt="平行遍历发生在同一个父节点下的各节点间"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">soup &#x3D; BeautifulSoup(demo, &#39;html.parser&#39;)</span><br><span class="line">print(soup.a.next_sibling) #NavigableString也构成了节点</span><br><span class="line">print(soup.a.next_sibling.next_sibling)</span><br><span class="line">print(soup.a.previous_sibling)</span><br><span class="line">print(soup.a.parent)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#对标签树进行循环遍历</span><br><span class="line">for sibling in soup.a.next_siblings:</span><br><span class="line">    print(sibling)  #遍历后续节点</span><br><span class="line">for sibling in soup.a.previous_siblings:</span><br><span class="line">    print(sibling)  #遍历前续节点</span><br></pre></td></tr></table></figure><p><strong>遍历类型总结</strong></p><p><img src="https://i.loli.net/2020/06/11/Jh4sljIbeLHQoMy.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;BeautifulSoup库的安装&quot;&gt;&lt;a href=&quot;#BeautifulSoup库的安装&quot; class=&quot;headerlink&quot; title=&quot;BeautifulSoup库的安装&quot;&gt;&lt;/a&gt;BeautifulSoup库的安装&lt;/h1&gt;&lt;p&gt;命令行内输入&lt;cod
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>到此为止</title>
    <link href="https://jluliyuxi.github.io/2020/06/11/%E5%88%B0%E6%AD%A4%E4%B8%BA%E6%AD%A2/"/>
    <id>https://jluliyuxi.github.io/2020/06/11/%E5%88%B0%E6%AD%A4%E4%B8%BA%E6%AD%A2/</id>
    <published>2020-06-11T01:25:29.273Z</published>
    <updated>2020-06-11T01:26:21.981Z</updated>
    
    <content type="html"><![CDATA[<h1 id="到此为止"><a href="#到此为止" class="headerlink" title="到此为止"></a>到此为止</h1><p>走到现在这一步其实在我的意料之中。<br>最开始接触你我以为你是一个开朗的女孩儿，但是越接触我越能感觉到你身上无止境的负能量和像无底洞一样的抱怨和埋怨。<br>每天都在说好烦啊烦死了。学习工作压力一大，烦死不爽和叹气就挂在你的嘴边。我就像你身边的垃圾桶，负面情绪的处理站。<br>我最难熬的是大一下的期末，11门考试排的很密，而且还要做物理实验。那一个月，你每天都在抱怨，抱怨完寝室抱怨实验，抱怨完实验抱怨考试。<br>我每天都在如履薄冰，一边承受着和你同样的压力，一边接受你每天高强度的抱怨和负面情绪，不敢疏导，生怕俩人又开始生气吵架。<br>可能是我在家太舒坦了，我现在真不知道我当时是怎么忍受过来而没有崩溃的。<br>哪怕就是心理医生，也很难承受这样的高强度折磨吧？<br>聊天搜素里输入一个烦字，一拉就是一串，密密麻麻。<br>记得就在几星期前，你又开始因为一些琐碎的小事各种怨气缠身，聊天记录全都是烦死了，我好烦，真烦。<br>我在电脑这边实在是忍受不了了，情绪突然变得暴躁，用拳头疯狂锤桌子，然后把手边的草稿纸撕得粉碎，无力地坐在椅子上，心跳很快。<br>然后我平静了一下，聊天框里打出：没事啦宝贝 不用烦。<br>换到一年前，我根本想不到我这种正能量充电器会变得这样狂躁。<br>我知道，这个情况不解决，疙瘩早晚在那里，迟早得要解决。<br>昨天你对我说，既然你接受不了我的负面情绪，就别再委屈自己了，对自己好一点？<br>嗯，好。        </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;到此为止&quot;&gt;&lt;a href=&quot;#到此为止&quot; class=&quot;headerlink&quot; title=&quot;到此为止&quot;&gt;&lt;/a&gt;到此为止&lt;/h1&gt;&lt;p&gt;走到现在这一步其实在我的意料之中。&lt;br&gt;最开始接触你我以为你是一个开朗的女孩儿，但是越接触我越能感觉到你身上无止境的负能量
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Python网络爬虫与信息提取（第1周 网络爬虫规则）</title>
    <link href="https://jluliyuxi.github.io/2020/06/07/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC1%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E8%A7%84%E5%88%99%EF%BC%89/"/>
    <id>https://jluliyuxi.github.io/2020/06/07/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC1%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E8%A7%84%E5%88%99%EF%BC%89/</id>
    <published>2020-06-07T14:55:17.076Z</published>
    <updated>2020-06-22T09:00:20.906Z</updated>
    
    <content type="html"><![CDATA[<h1 id="单元一-Requests库入门"><a href="#单元一-Requests库入门" class="headerlink" title="单元一 Requests库入门"></a>单元一 Requests库入门</h1><hr><h2 id="Requests库的安装"><a href="#Requests库的安装" class="headerlink" title="Requests库的安装"></a>Requests库的安装</h2><p>命令行内输入<code>pip install requests</code>进行安装</p><h2 id="Requests库的测试"><a href="#Requests库的测试" class="headerlink" title="Requests库的测试"></a>Requests库的测试</h2><p>IDLE内输入以下代码： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">r &#x3D; requests.get(&quot;http:&#x2F;&#x2F;www.baidu.com&quot;)</span><br><span class="line">r.status_code</span><br></pre></td></tr></table></figure><p>返回<code>200</code>   </p><p>说明Requests库安装成功</p><h2 id="Requests库语法"><a href="#Requests库语法" class="headerlink" title="Requests库语法"></a>Requests库语法</h2><p><strong>Requests库的七个主要方法</strong>  </p><table><thead><tr><th align="center">方法</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">requests.requests()</td><td align="center">构造一个请求，支撑以下各方法的基础方法</td></tr><tr><td align="center">requests.get()</td><td align="center">获取HTML网页的主要方法，对应HTTP的GET</td></tr><tr><td align="center">requests.head()</td><td align="center">获取HTML头信息的方法，对应HTTP的HEAD</td></tr><tr><td align="center">requests.post()</td><td align="center">向HTML网页提交POST请求的方法，对应HTTP的POST</td></tr><tr><td align="center">requests.put()</td><td align="center">向HTML网页提交PUT请求的方法，对应于HTTP的PUT</td></tr><tr><td align="center">requests.patch()</td><td align="center">向HTML网页提交局部修改请求，对应于HTTP的PATCH</td></tr><tr><td align="center">requests.delete()</td><td align="center">向HTML页面提交删除请求，对应于HTTP的DELETE</td></tr></tbody></table><h2 id="Response对象的属性"><a href="#Response对象的属性" class="headerlink" title="Response对象的属性"></a>Response对象的属性</h2><table><thead><tr><th align="center">属性</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">r.status_code</td><td align="center">HTTP请求的返回状态，200表示连接成功，404表示失败</td></tr><tr><td align="center">r.text</td><td align="center">HTTP响应内容的字符串形式，即，url对应的页面内容</td></tr><tr><td align="center">r.encoding</td><td align="center">从HTTP header中猜测的响应内容编码方式</td></tr><tr><td align="center">r.apparent_encoding</td><td align="center">从内容中分析出的响应内容编码方式(备选编码方式)</td></tr><tr><td align="center">r.content</td><td align="center">HTTP响应内容的二进制形式</td></tr></tbody></table><p><em>备注:</em>  </p><p>r.encoding:如果header中不存在charset,则认为编码为IS0-8859-1    </p><p>rapparent encoding：根据网页内容分析出的编码方式</p><h2 id="爬取网页的通用代码框架"><a href="#爬取网页的通用代码框架" class="headerlink" title="爬取网页的通用代码框架"></a>爬取网页的通用代码框架</h2><p><strong>Requests库的六种异常处理</strong></p><table><thead><tr><th align="center">异常</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">requests.ConnectionError</td><td align="center">网络连接错误异常，如DNS查询失败、拒绝连接等</td></tr><tr><td align="center">requests.HTTPError</td><td align="center">HTTP错误异常</td></tr><tr><td align="center">requests.URLRequired</td><td align="center">URL缺失异常</td></tr><tr><td align="center">requests.TooManyRedirects</td><td align="center">超过最大重定向次数，产生重定向异常</td></tr><tr><td align="center">requests.ConnectTimeout</td><td align="center">连接远程服务器超时异常</td></tr><tr><td align="center">requests.Timeout</td><td align="center">请求URL超时，产生超时异常</td></tr></tbody></table><p>对于Response：</p><table><thead><tr><th align="center">异常</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">r.raise_ for_ status()</td><td align="center">如果不是200，产生异常requests.HTTPError</td></tr></tbody></table><p><strong>爬虫时务必使用try-except语句排除异常</strong></p><h2 id="HTTP协议及Requests库主要方法"><a href="#HTTP协议及Requests库主要方法" class="headerlink" title="HTTP协议及Requests库主要方法"></a>HTTP协议及Requests库主要方法</h2><h4 id="HTTP基础知识"><a href="#HTTP基础知识" class="headerlink" title="HTTP基础知识"></a>HTTP基础知识</h4><p>HTTP, Hypertext Transfer Protocol, 超文本传输协议。     </p><p>HTTP是一个基于“请求与响应”模式的、无状态的应用层协议。      </p><p>HTTP协议采用URL作为定位网络资源的标识。</p><p><strong>URL格式</strong>   <a href="http://host[:port][path]">http://host[:port][path]</a><br><strong>host</strong> : 合法的Internet主机域名或IP地址<br><strong>port</strong> : 端口号，缺省端口为80<br><strong>path</strong> : 请求资源的路径<br><strong>HTTP协议对资源的操作</strong></p><table><thead><tr><th align="center">方法</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">GET</td><td align="center">请求获取URL位置的资源</td></tr><tr><td align="center">HEAD</td><td align="center">请求获取URL位置资源的响应消息报告，即获得该资源的头部信息</td></tr><tr><td align="center">POST</td><td align="center">请求向URL位置的资源后附加新的数据</td></tr><tr><td align="center">PUT</td><td align="center">请求向URL位置存储一个资源，覆盖原URL位置的资源</td></tr><tr><td align="center">PATCH</td><td align="center">请求局部更新URL位置的资源，即改变该处资源的部分内容</td></tr><tr><td align="center">DELETE</td><td align="center">请求删除URL位置存储的资源</td></tr></tbody></table><p>HTTP协议操作与Requests库的方法是一致的</p><h3 id="Requests库主要方法解析"><a href="#Requests库主要方法解析" class="headerlink" title="Requests库主要方法解析"></a>Requests库主要方法解析</h3><p>**kwargs:控制访问的参数，均为可选项，包括以下13种       </p><p><strong>params</strong> : 字典或字节序列，作为参数增加到url中<br><strong>data</strong> : 字典、字节序列或文件对象，作为Request的内容<br><strong>json</strong> : JSON格式的数据，作为Request的内容<br><strong>headers</strong> : 字典，HTTP定制头<br><strong>cookies</strong> ： 字典或CookieJar, Request中的cookie<br><strong>auth</strong> : 元组，支持HTTP认证功能<br><strong>files</strong> : 字典类型，传输文件<br><strong>timeout</strong> : 设定超时时间，秒为单位<br><strong>proxies</strong> : 字典类型，设定访问代理服务器，可以增加登录认证<br><strong>allow_redirects</strong> : True/False, 默认为True,重定向开关<br><strong>stream</strong> : True/False, 默认为True,获取内容立即下载开关<br><strong>verify</strong> : True/False, 默认为True,认证SSL证书开关<br><strong>cert</strong> : 本地SSL证书路径    </p><hr><h1 id="单元二-网络爬虫的“盗亦有道”"><a href="#单元二-网络爬虫的“盗亦有道”" class="headerlink" title="单元二 网络爬虫的“盗亦有道”"></a>单元二 网络爬虫的“盗亦有道”</h1><h2 id="网络爬虫的尺寸"><a href="#网络爬虫的尺寸" class="headerlink" title="网络爬虫的尺寸"></a>网络爬虫的尺寸</h2><table><thead><tr><th align="center">规模</th><th align="center">特点</th><th align="center">库</th></tr></thead><tbody><tr><td align="center">爬取网页 玩转网页</td><td align="center">小规模 数据量小 爬取速度不敏感</td><td align="center">Requests库</td></tr><tr><td align="center">爬取网站 爬取系列网站</td><td align="center">中规模 数据规模较大 爬取速度敏感</td><td align="center">Scrapy库</td></tr><tr><td align="center">爬取全网</td><td align="center">大规模 搜索引擎 爬取速度关键</td><td align="center">定制开发</td></tr></tbody></table><h2 id="网络爬虫的限制"><a href="#网络爬虫的限制" class="headerlink" title="网络爬虫的限制"></a>网络爬虫的限制</h2><p><strong>·来源审查:判断User-Agent进行限制</strong>     </p><p>检查来访HTTP协议头的User-Agent域，只响应浏览器或友好爬虫的访问。     </p><p><strong>·发布公告: Robots协议</strong>      </p><p>告知所有爬虫网站的爬取策略，要求爬虫遵守。</p><h2 id="Robots协议"><a href="#Robots协议" class="headerlink" title="Robots协议"></a>Robots协议</h2><p>Robots Exclusion Standard网络爬虫排除标准   </p><p>作用:网站告知网络爬虫哪些页面可以抓取，哪些不行。   </p><p>形式:在网站根目录下的robots.txt文件。  </p><p><strong><em>Robots协议基本语法:</em></strong>     </p><p>*代表所有，/代表根目录    </p><p>User-agent : *  </p><p>Disallow : /    </p><hr><h1 id="单元三-Requests库网络爬虫实例"><a href="#单元三-Requests库网络爬虫实例" class="headerlink" title="单元三 Requests库网络爬虫实例"></a>单元三 Requests库网络爬虫实例</h1><h2 id="实例1：京东商品页面的爬取"><a href="#实例1：京东商品页面的爬取" class="headerlink" title="实例1：京东商品页面的爬取"></a>实例1：京东商品页面的爬取</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">url &#x3D; &quot;https:&#x2F;&#x2F;item.jd.com&#x2F;100002756531.html&quot;</span><br><span class="line">try:</span><br><span class="line">    r &#x3D; requests.get(url)</span><br><span class="line">    r.raise_for_status()</span><br><span class="line">    r.encoding &#x3D; r.apparent_encoding</span><br><span class="line">    print(r.text[:1000])</span><br><span class="line">except:</span><br><span class="line">    print(&quot;爬取失败&quot;)</span><br></pre></td></tr></table></figure><h2 id="实例2：亚马逊商品页面的爬取"><a href="#实例2：亚马逊商品页面的爬取" class="headerlink" title="实例2：亚马逊商品页面的爬取"></a>实例2：亚马逊商品页面的爬取</h2><p><strong>需要注意的是，要隐藏Python请求的user-agent</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">url &#x3D; &quot;https:&#x2F;&#x2F;www.amazon.cn&#x2F;dp&#x2F;B076YGLN6G?smid&#x3D;A3TEGLC21NOO5Y&amp;ref_&#x3D;Oct_CBBBCard_dsk_asin3&amp;pf_rd_r&#x3D;2TZ4NZR97BGR1G2W8E38&amp;pf_rd_p&#x3D;5a0738df-7719-4914-81ee-278221dce082&amp;pf_rd_m&#x3D;A1AJ19PSB66TGU&amp;pf_rd_s&#x3D;desktop-3&quot;</span><br><span class="line">try:</span><br><span class="line">    kv &#x3D; &#123;&#39;user-agent&#39; : &#39;Mozilla&#x2F;5.0&#39;&#125;</span><br><span class="line">    r &#x3D; requests.get(url,headers&#x3D;kv)</span><br><span class="line">    r.raise_for_status()</span><br><span class="line">    r.encoding &#x3D; r.apparent_encoding</span><br><span class="line">    print(r.text[1000:2000])</span><br><span class="line">except:</span><br><span class="line">    print(&quot;爬取失败&quot;)</span><br></pre></td></tr></table></figure><h2 id="实例3：百度-360搜索关键词提交"><a href="#实例3：百度-360搜索关键词提交" class="headerlink" title="实例3：百度/360搜索关键词提交"></a>实例3：百度/360搜索关键词提交</h2><p><strong>搜索引擎关键词提交接口</strong><br>百度的关键词接口: <a href="http://www.baidu.com/s?wd=keyword" target="_blank" rel="noopener">http://www.baidu.com/s?wd=keyword</a>     </p><p>360的关键词接口: <a href="http://www.so.com/s?q=keyword" target="_blank" rel="noopener">http://www.so.com/s?q=keyword</a>        </p><p>只需要替换keyword即可完成搜索</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">keyword &#x3D; &quot; Python&quot;</span><br><span class="line">try:</span><br><span class="line">    kv &#x3D; &#123;&#39;wd&#39;:&#39;Python&#39;&#125;</span><br><span class="line">    r &#x3D; requests.get(&quot;http:&#x2F;&#x2F;www.baidu.com&#x2F;s&quot;,params&#x3D;kv)</span><br><span class="line">    print(r.request.url)</span><br><span class="line">    r.raise_for_status()</span><br><span class="line">    print(len(r.text))</span><br><span class="line">except :</span><br><span class="line">    print(&quot;爬取失败&quot;)</span><br></pre></td></tr></table></figure><p>**⚠如上代码执行后仍不能获取搜索结果，在Python Shell中len(r.tetx) = 0.      </p><p>在Pycharm中执行后发现，结果会返回一个验证链接，打开后要求旋转图片。**</p><p><img src="https://i.loli.net/2020/06/08/u3dPTHWxmcSfh1Y.png" alt=""></p><p><strong>2020/6/8 目前尚无能力解决，此处留一个记号，将来学会了再进行处理</strong></p><h2 id="实例4：网络图片的爬取和存储"><a href="#实例4：网络图片的爬取和存储" class="headerlink" title="实例4：网络图片的爬取和存储"></a>实例4：网络图片的爬取和存储</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import os</span><br><span class="line">url &#x3D; &quot;http:&#x2F;&#x2F;img0.dili360.com&#x2F;pic&#x2F;2020&#x2F;05&#x2F;14&#x2F;5ebd27942d9ce1d29634596_t.jpg@!rw4&quot;</span><br><span class="line">root &#x3D; &#39;D:&#39;</span><br><span class="line">path &#x3D; root + url.split(&#39;&#x2F;&#39;)[-1]</span><br><span class="line">try:</span><br><span class="line">    if not os.path.exists(root):</span><br><span class="line">        os.mkdir(root)</span><br><span class="line">    if not os.path.exists(path):</span><br><span class="line">        r &#x3D; requests.get(url)</span><br><span class="line">        with open(path,&#39;wb&#39;) as f:</span><br><span class="line">            f.write(r.content)</span><br><span class="line">            f.close()</span><br><span class="line">        print(&quot;文件保存成功&quot;)</span><br><span class="line">    else:</span><br><span class="line">        print(&quot;文件已存在&quot;)</span><br><span class="line">except:</span><br><span class="line">    print(&quot;爬取失败&quot;)</span><br></pre></td></tr></table></figure><h2 id="实例5：IP地址归属地的自动查询"><a href="#实例5：IP地址归属地的自动查询" class="headerlink" title="实例5：IP地址归属地的自动查询"></a>实例5：IP地址归属地的自动查询</h2><p>通过<strong><em>ip138</em></strong>网站进行自动查询</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">url &#x3D; &quot;http:&#x2F;&#x2F;m.ip138.com&#x2F;ip.asp?ip&#x3D;&quot;</span><br><span class="line">try:</span><br><span class="line">    r &#x3D; requests.get(url+&#39;202.198.16.83&#39;)</span><br><span class="line">    r.raise_for_status()</span><br><span class="line">    r.encoding &#x3D; r.apparent_encoding</span><br><span class="line">    print(r.text[-500:])</span><br><span class="line">except:</span><br><span class="line">    print(&quot;爬取失败&quot;)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;单元一-Requests库入门&quot;&gt;&lt;a href=&quot;#单元一-Requests库入门&quot; class=&quot;headerlink&quot; title=&quot;单元一 Requests库入门&quot;&gt;&lt;/a&gt;单元一 Requests库入门&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;Requests
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Python网络爬虫与信息提取（第0周 网络爬虫前奏）</title>
    <link href="https://jluliyuxi.github.io/2020/06/07/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC0%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%89%8D%E5%A5%8F%EF%BC%89/"/>
    <id>https://jluliyuxi.github.io/2020/06/07/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC0%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%89%8D%E5%A5%8F%EF%BC%89/</id>
    <published>2020-06-07T13:50:21.805Z</published>
    <updated>2020-06-23T09:10:58.076Z</updated>
    
    <content type="html"><![CDATA[<p>引言：最近在学习北理嵩天的这门课，便在个人博客里做一下笔记，方面以后学习使用。</p><hr><h1 id="1-课程内容导学"><a href="#1-课程内容导学" class="headerlink" title="1.课程内容导学"></a>1.课程内容导学</h1><h2 id="Requests库"><a href="#Requests库" class="headerlink" title="Requests库"></a>Requests库</h2><p>自动爬取HTML页面 自动网络提交请求</p><h2 id="robots-txt"><a href="#robots-txt" class="headerlink" title="robots.txt"></a>robots.txt</h2><p>网络爬虫排除协议</p><h2 id="Beautiful-Soup"><a href="#Beautiful-Soup" class="headerlink" title="Beautiful Soup"></a>Beautiful Soup</h2><p>解析HTML页面</p><h2 id="Projects"><a href="#Projects" class="headerlink" title="Projects"></a>Projects</h2><p>实战项目A/B</p><h2 id="Re"><a href="#Re" class="headerlink" title="Re"></a>Re</h2><p>正则表达式详解 提取页面关键信息</p><h2 id="Scrapy"><a href="#Scrapy" class="headerlink" title="Scrapy*"></a>Scrapy*</h2><p>网络爬虫原理介绍 专业爬虫框架介绍</p><hr><h1 id="2-IDE选择"><a href="#2-IDE选择" class="headerlink" title="2.IDE选择"></a>2.IDE选择</h1><p>目前我自己使用的文本工具类IDE是<strong><em>Sublime Text3</em></strong>,集成工具类IDE是<strong><em>PyCharm</em></strong>。   </p><p>等这个课学完，下一个学习数据处理和分析课，再学习使用<strong><em>Anaconda</em></strong>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;引言：最近在学习北理嵩天的这门课，便在个人博客里做一下笔记，方面以后学习使用。&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&quot;1-课程内容导学&quot;&gt;&lt;a href=&quot;#1-课程内容导学&quot; class=&quot;headerlink&quot; title=&quot;1.课程内容导学&quot;&gt;&lt;/a&gt;1.课程内容导学&lt;/
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Python+Selenium Auto-Clock</title>
    <link href="https://jluliyuxi.github.io/2020/05/31/Python+Selenium%20Auto-Clock/"/>
    <id>https://jluliyuxi.github.io/2020/05/31/Python+Selenium%20Auto-Clock/</id>
    <published>2020-05-31T05:03:43.454Z</published>
    <updated>2020-06-20T11:47:32.805Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python-Selenium实现吉林大学健康申报自动打卡"><a href="#Python-Selenium实现吉林大学健康申报自动打卡" class="headerlink" title="Python+Selenium实现吉林大学健康申报自动打卡"></a>Python+Selenium实现吉林大学健康申报自动打卡</h1><p>引言：因为自己总是忘记学校打卡，经常让班长提醒，便想着能不能写一个脚本实现自动打卡。在网上查了很多资料琢磨了三四天，最终选择了Python+Selenium完成此项。Selenium库是一种模拟浏览器进行自动化操作的库，可通过此库来完成自动打卡。</p><p><strong><em>初学者，代码冗长，如有更优解，烦请指点。</em></strong></p><h2 id="0-流程图"><a href="#0-流程图" class="headerlink" title="0.流程图"></a>0.流程图</h2><p><img src="https://i.loli.net/2020/06/04/tTVOWBozMrpwA5u.png" alt=""></p><hr><h2 id="1-配置工作"><a href="#1-配置工作" class="headerlink" title="1.配置工作"></a>1.配置工作</h2><h3 id="①准备好Python3环境"><a href="#①准备好Python3环境" class="headerlink" title="①准备好Python3环境"></a>①准备好Python3环境</h3><h3 id="②其次根据浏览器的种类及版本，选择相对应的浏览器驱动（driver）"><a href="#②其次根据浏览器的种类及版本，选择相对应的浏览器驱动（driver）" class="headerlink" title="②其次根据浏览器的种类及版本，选择相对应的浏览器驱动（driver）"></a>②其次根据浏览器的种类及版本，选择相对应的浏览器驱动（driver）</h3><p>例如我的浏览器是Chrome的如下版本：</p><p><img src="https://i.loli.net/2020/05/31/O1YpRhxCos3vm5V.png" alt=""></p><p>然后我们打开<a href="http://npm.taobao.org/mirrors/chromedriver" target="_blank" rel="noopener">Chrome驱动镜像网站</a>，下载对应版本的驱动（如果没找到可以试试略低于自己版本的）。  </p><p>安装驱动到工程路径上。(通常至于Python文件夹下的Scripts文件夹内)</p><h3 id="③配置浏览器驱动"><a href="#③配置浏览器驱动" class="headerlink" title="③配置浏览器驱动"></a>③配置浏览器驱动</h3><p>我们需要将浏览器驱动路径加入到环境变量中，完成配置   </p><h4 id="右键【此电脑】，打开属性-R-选择左边的高级系统设置"><a href="#右键【此电脑】，打开属性-R-选择左边的高级系统设置" class="headerlink" title="右键【此电脑】，打开属性(R),选择左边的高级系统设置"></a>右键【此电脑】，打开属性(R),选择左边的高级系统设置</h4><p><img src="https://i.loli.net/2020/06/08/IazdnvjSo6QU8mx.png" alt=""></p><h4 id="点击环境变量"><a href="#点击环境变量" class="headerlink" title="点击环境变量"></a>点击环境变量</h4><p><img src="https://i.loli.net/2020/06/08/Q1SuxdM9ntWU3wk.png" alt=""></p><h4 id="选择Path编辑，新建后输入浏览器驱动所在路径"><a href="#选择Path编辑，新建后输入浏览器驱动所在路径" class="headerlink" title="选择Path编辑，新建后输入浏览器驱动所在路径"></a>选择Path编辑，新建后输入浏览器驱动所在路径</h4><p><img src="https://i.loli.net/2020/06/08/fjZWBILa8p3qzMC.png" alt=""></p><h3 id="④安装Selenium库"><a href="#④安装Selenium库" class="headerlink" title="④安装Selenium库"></a>④安装Selenium库</h3><p>打开命令行，输入<code>pip install selenium</code>  </p><p>关于selenium库的具体用法，我们可以参考如下链接：    </p><p><a href="http://www.selenium.org.cn/1598.html" target="_blank" rel="noopener">Selenium中文网</a>     </p><p><a href="https://blog.csdn.net/weixin_36279318/article/details/79475388" target="_blank" rel="noopener">Python Selenium库的使用</a>       </p><p>由于我们之后还会用到js命令，所以需要再安装<code>pip install js2py</code></p><h3 id="⑤事先录入一遍信息"><a href="#⑤事先录入一遍信息" class="headerlink" title="⑤事先录入一遍信息"></a>⑤事先录入一遍信息</h3><p>为了方便写代码，我们需要打开申报表，先填写一边信息并提交。   </p><p>这样的话浏览器会自动存储已经填好的信息，我们只需要点点点确认就可以了。</p><hr><h2 id="2-脚本分析"><a href="#2-脚本分析" class="headerlink" title="2.脚本分析"></a>2.脚本分析</h2><h3 id="①引用库"><a href="#①引用库" class="headerlink" title="①引用库"></a>①引用库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入selenium里的webdriver包</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> js2py</span><br><span class="line"><span class="comment"># noinspection PyUnresolvedReferences</span></span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys <span class="comment">#输入账号密码时会用到</span></span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options<span class="comment">#转换标签页时会用到</span></span><br></pre></td></tr></table></figure><h3 id="②打开吉林大学办事大厅"><a href="#②打开吉林大学办事大厅" class="headerlink" title="②打开吉林大学办事大厅"></a>②打开吉林大学办事大厅</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">driver = webdriver.Chrome()</span><br><span class="line">driver.get(<span class="string">"https://ehall.jlu.edu.cn/taskcenter/workflow/appall"</span>)<span class="comment">#打开吉林大学办事大厅</span></span><br><span class="line">time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="③定位账号密码并点击登录，"><a href="#③定位账号密码并点击登录，" class="headerlink" title="③定位账号密码并点击登录，"></a>③定位账号密码并点击登录，</h3><p>我在这个界面使用的是by_id方法定位账号密码框。<br>将光标放置在账号栏上，右键后点击【检查】，找到该元素的id。<br><img src="https://i.loli.net/2020/05/31/kg8p3nfczUDVXRG.png" alt=""><br>相同方法找出密码框和登录框的id<br><img src="https://i.loli.net/2020/05/31/LM5UoVNG2YWFwPt.png" alt=""></p><p><img src="https://i.loli.net/2020/05/31/yvqwmGeJp7K9t5M.png" alt=""><br>定位元素并输入账号密码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#输入用户名和密码 点击登录</span></span><br><span class="line">driver.find_element_by_id(<span class="string">"username"</span>).send_keys(<span class="string">"你的账号"</span>)</span><br><span class="line">driver.find_element_by_id(<span class="string">"password"</span>).send_keys(<span class="string">"你的密码"</span>)</span><br><span class="line">driver.find_element_by_id(<span class="string">"login-submit"</span>).click()</span><br><span class="line">time.sleep(<span class="number">1</span>)<span class="comment">#停1秒再继续操作</span></span><br></pre></td></tr></table></figure><h3 id="④点击本科生健康状况申报"><a href="#④点击本科生健康状况申报" class="headerlink" title="④点击本科生健康状况申报"></a>④点击本科生健康状况申报</h3><p>进入到下一页面后，直接用名称进行定位： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">driver.find_element_by_link_text(<span class="string">"本科生健康状况申报"</span>).click()</span><br><span class="line">time.sleep(<span class="number">4</span>)<span class="comment">#有时候表格加载时间比较长，故sleep四秒</span></span><br></pre></td></tr></table></figure><h3 id="⑤切换标签页并下拉滚动条"><a href="#⑤切换标签页并下拉滚动条" class="headerlink" title="⑤切换标签页并下拉滚动条"></a>⑤切换标签页并下拉滚动条</h3><p>这里我卡了好几天，run的时候总是出现<code>ElementNotVisibleException: element not visible</code>    </p><p>我以为是我在定位元素的时候出错了，便试了很多种定位方法还是报错。    </p><p>最后发现，虽然弹出了新的标签页，但是他还是定位在旧的标签页上。于是用以下代码切换标签页：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n = driver.window_handles</span><br><span class="line">driver.switch_to.window(n[<span class="number">-1</span>])</span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>更加详细的解释可以参考<a href="https://blog.csdn.net/u012941152/article/details/88418812?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase" target="_blank" rel="noopener">Selenium新窗口打开链接，并定位到新窗口</a>    </p><p>当我切换了标签页后，发现还是无法定位，又经过了十几次尝试，发现当所求元素不在当前屏幕内时，需要调整滚动条至对应位置才能定位。  </p><p>由于webdriver无法调整滚动条，故用js语句来实现。   </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">js = <span class="string">"window.scrollTo(0,document.body.scrollHeight)"</span></span><br><span class="line">driver.execute_script(js)</span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>更加详细的解释可以参考<a href="http://www.mamicode.com/info-detail-2324043.html" target="_blank" rel="noopener">Selenium浏览器滚动条操作</a>，<a href="https://blog.csdn.net/doulihang/article/details/80835290?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase" target="_blank" rel="noopener">Selenium操作滚动条</a>       </p><h3 id="⑥定位点击复选框及确认填报"><a href="#⑥定位点击复选框及确认填报" class="headerlink" title="⑥定位点击复选框及确认填报"></a>⑥定位点击复选框及确认填报</h3><p>在这里我用的是by_xpath进行定位，对于一些动态id或者是嵌套很深的元素，可以用xpath或css的方法来定位。  </p><p>光标放在最下面的复选框上，右键检查，再右键右边选中的Elements，按所示路径copy xpath.<br><img src="https://i.loli.net/2020/05/31/ECyi4hDsYlK3pkz.png" alt="">     </p><p>同样的方法点击上方的确认填报。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#勾选复选框 点击确认 点击弹窗</span></span><br><span class="line">driver.find_element_by_xpath(<span class="string">'/html/body/div[4]/form/div/div[2]/div[3]/div/div[1]/div[1]/table/tbody/tr[2]/td/div/table/tbody/tr[55]/td/div/input'</span>).click()</span><br><span class="line">driver.find_element_by_xpath(<span class="string">"/html/body/div[4]/form/div/div[1]/div[2]/ul/li[1]/a"</span>).click()</span><br><span class="line">time.sleep(<span class="number">2</span>)<span class="comment">#等待提示框弹出</span></span><br></pre></td></tr></table></figure><h3 id="⑦点击弹窗完成，退出浏览器。"><a href="#⑦点击弹窗完成，退出浏览器。" class="headerlink" title="⑦点击弹窗完成，退出浏览器。"></a>⑦点击弹窗完成，退出浏览器。</h3><p>同理定位弹窗内的【好】，并点击，随后关闭浏览器。   </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">driver.find_element_by_xpath(<span class="string">"/html/body/div[7]/div/div[2]/button[1]"</span>).click()</span><br><span class="line">driver.quit()<span class="comment">#自动打卡完成，关闭浏览器</span></span><br></pre></td></tr></table></figure><h3 id="⑧启用无头模式-headless-mode-打开浏览器"><a href="#⑧启用无头模式-headless-mode-打开浏览器" class="headerlink" title="⑧启用无头模式(headless mode)打开浏览器"></a>⑧启用无头模式(headless mode)打开浏览器</h3><p>如果每次都要弹出个浏览器等他运行完成以后再关闭属实累赘，于是我们可以再开头设置无头模式，让浏览器在后台自行完成并关闭，不影响台面上的工作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#用无头模式打开浏览器</span></span><br><span class="line">ch_options = Options()</span><br><span class="line">ch_options.add_argument(<span class="string">"--headless"</span>)  <span class="comment"># =&gt; 为Chrome配置无头模式</span></span><br><span class="line">driver = webdriver.Chrome(options=ch_options)</span><br></pre></td></tr></table></figure><hr><h2 id="3-完整代码"><a href="#3-完整代码" class="headerlink" title="3.完整代码"></a>3.完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入selenium里的webdriver包</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> js2py</span><br><span class="line"><span class="comment"># noinspection PyUnresolvedReferences</span></span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options</span><br><span class="line"><span class="comment">#用无头模式打开浏览器</span></span><br><span class="line">ch_options = Options()</span><br><span class="line">ch_options.add_argument(<span class="string">"--headless"</span>)  <span class="comment"># =&gt; 为Chrome配置无头模式</span></span><br><span class="line">driver = webdriver.Chrome(options=ch_options)</span><br><span class="line"></span><br><span class="line">driver.get(<span class="string">"https://ehall.jlu.edu.cn/taskcenter/workflow/appall"</span>)<span class="comment">#打开吉林大学办事大厅</span></span><br><span class="line">time.sleep(<span class="number">1</span>)</span><br><span class="line"><span class="comment">#输入用户名和密码 点击登录</span></span><br><span class="line">driver.find_element_by_id(<span class="string">"username"</span>).send_keys(<span class="string">"你的账号"</span>)</span><br><span class="line">driver.find_element_by_id(<span class="string">"password"</span>).send_keys(<span class="string">"你的密码"</span>)</span><br><span class="line">driver.find_element_by_id(<span class="string">"login-submit"</span>).click()</span><br><span class="line">time.sleep(<span class="number">1</span>)<span class="comment">#停1秒再继续操作</span></span><br><span class="line">driver.find_element_by_link_text(<span class="string">"本科生健康状况申报"</span>).click()</span><br><span class="line">time.sleep(<span class="number">4</span>)<span class="comment">#表格加载四秒</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#这里卡了好几天 最后发现问题是要先切换标签页 再滑动滚动条至底部才能定位元素</span></span><br><span class="line">n = driver.window_handles</span><br><span class="line">driver.switch_to.window(n[<span class="number">-1</span>])</span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line">js = <span class="string">"window.scrollTo(0,document.body.scrollHeight)"</span></span><br><span class="line">driver.execute_script(js)</span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#勾选复选框 点击确认 点击弹窗</span></span><br><span class="line">driver.find_element_by_xpath(<span class="string">'/html/body/div[4]/form/div/div[2]/div[3]/div/div[1]/div[1]/table/tbody/tr[2]/td/div/table/tbody/tr[55]/td/div/input'</span>).click()</span><br><span class="line">driver.find_element_by_xpath(<span class="string">"/html/body/div[4]/form/div/div[1]/div[2]/ul/li[1]/a"</span>).click()</span><br><span class="line">time.sleep(<span class="number">2</span>)<span class="comment">#等待提示框弹出</span></span><br><span class="line">driver.find_element_by_xpath(<span class="string">"/html/body/div[7]/div/div[2]/button[1]"</span>).click()</span><br><span class="line">driver.quit()<span class="comment">#自动打卡完成，关闭浏览器</span></span><br></pre></td></tr></table></figure><hr><h2 id="4-设置开机自动启动"><a href="#4-设置开机自动启动" class="headerlink" title="4.设置开机自动启动"></a>4.设置开机自动启动</h2><p>将完成的脚本创捷快捷方式至桌面     </p><p>Win+R打开运行窗口输入shell:startup，打开启动文件夹，将快捷方式拖入文件夹内即可。   </p><hr><h2 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5.参考资料"></a>5.参考资料</h2><p><a href="http://www.selenium.org.cn/1598.html" target="_blank" rel="noopener">http://www.selenium.org.cn/1598.html</a><br><a href="https://blog.csdn.net/weixin_36279318/article/details/79475388" target="_blank" rel="noopener">https://blog.csdn.net/weixin_36279318/article/details/79475388</a><br><a href="https://blog.csdn.net/u012941152/article/details/88418812?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase" target="_blank" rel="noopener">https://blog.csdn.net/u012941152/article/details/88418812?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase</a><br><a href="http://www.mamicode.com/info-detail-2324043.html" target="_blank" rel="noopener">http://www.mamicode.com/info-detail-2324043.html</a><br><a href="https://blog.csdn.net/doulihang/article/details/80835290?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase" target="_blank" rel="noopener">https://blog.csdn.net/doulihang/article/details/80835290?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Python-Selenium实现吉林大学健康申报自动打卡&quot;&gt;&lt;a href=&quot;#Python-Selenium实现吉林大学健康申报自动打卡&quot; class=&quot;headerlink&quot; title=&quot;Python+Selenium实现吉林大学健康申报自动打卡&quot;&gt;&lt;/a
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://jluliyuxi.github.io/2020/05/12/hello-world/"/>
    <id>https://jluliyuxi.github.io/2020/05/12/hello-world/</id>
    <published>2020-05-12T03:40:03.878Z</published>
    <updated>2020-05-28T11:14:02.049Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
