<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="药学 自由人 爱好者"><meta name="keywords" content=""><meta name="author" content="LyC"><meta name="copyright" content="LyC"><title>莫道君行早 更有早行人 | LyC's Blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="LyC's Blog" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="author-info"><div class="author-info__avatar text-center"><img src="https://i.loli.net/2020/05/17/HbaOsFvjp8c9MRE.jpg"></div><div class="author-info__name text-center">LyC</div><div class="author-info__description text-center">药学 自由人 爱好者</div><div class="follow-button"><a href="https://jluliyuxi.github.io/">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">24</span></a></div></div></div><nav id="nav" style="background-image: url(https://i.loli.net/2020/05/12/jFsUgX9TBbnQu2o.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">LyC's Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="site-info"><div id="site-title">LyC's Blog</div><div id="site-sub-title">莫道君行早 更有早行人</div><div id="site-social-icons"><a class="social-icon" href="https://github.com/JLUliyuxi || github" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-github fa"></i></a><a class="social-icon" href="https://weibo.com/3687299900/profile?rightmod=1&amp;wvr=6&amp;mod=personinfo || weibo" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-weibo fa"></i></a><a class="social-icon" href="https://www.google.com || chrome" target="_blank" rel="noreferrer noopener nofollow"><i class="fa-chrome fa"></i></a></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2020/07/03/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC4%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%20%E7%AC%AC%E5%8D%81%E4%B8%80%E5%8D%95%E5%85%83%20%E8%82%A1%E7%A5%A8%E6%95%B0%E6%8D%AEScrapy%E7%88%AC%E8%99%AB%EF%BC%89/">Python网络爬虫与信息提取（第4周 网络爬虫框架 第十一单元 股票数据Scrapy爬虫）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-07-03</time><div class="content"><h1 id="股票数据Scrapy爬虫实例介绍"><a href="#股票数据Scrapy爬虫实例介绍" class="headerlink" title="股票数据Scrapy爬虫实例介绍"></a>股票数据Scrapy爬虫实例介绍</h1><hr>
<p>获取股票列表:     </p>
<p>东方财富网: <a href="http://quote.eastmoney.com/stocklist.html" target="_blank" rel="noopener">http://quote.eastmoney.com/stocklist.html</a>        </p>
<p>获取个股信息:     </p>
<p>百度股票: <a href="https://gupiao.baidu.com/stock/" target="_blank" rel="noopener">https://gupiao.baidu.com/stock/</a>       </p>
<p>单个股票: <a href="https://gupiao.baidu.com/stock/sz002439.html" target="_blank" rel="noopener">https://gupiao.baidu.com/stock/sz002439.html</a>      </p>
<h1 id="股票数据Scrapy爬虫实例编写"><a href="#股票数据Scrapy爬虫实例编写" class="headerlink" title="股票数据Scrapy爬虫实例编写"></a>股票数据Scrapy爬虫实例编写</h1><h2 id="Stocks-py"><a href="#Stocks-py" class="headerlink" title="Stocks.py"></a>Stocks.py</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StocksSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"stocks"</span></span><br><span class="line">    start_urls = [<span class="string">'https://quote.eastmoney.com/stocklist.html'</span>]</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> href <span class="keyword">in</span> response.css(<span class="string">'a::attr(href)'</span>).extract():</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                stock = re.findall(<span class="string">r"[s][hz]\d&#123;6&#125;"</span>, href)[<span class="number">0</span>]</span><br><span class="line">                url = <span class="string">'https://gupiao.baidu.com/stock/'</span> + stock + <span class="string">'.html'</span></span><br><span class="line">                <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse_stock)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_stock</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        infoDict = &#123;&#125;</span><br><span class="line">        stockInfo = response.css(<span class="string">'.stock-bets'</span>)</span><br><span class="line">        name = stockInfo.css(<span class="string">'.bets-name'</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        keyList = stockInfo.css(<span class="string">'dt'</span>).extract()</span><br><span class="line">        valueList = stockInfo.css(<span class="string">'dd'</span>).extract()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(keyList)):</span><br><span class="line">            key = re.findall(<span class="string">r'&gt;.*&lt;/dt&gt;'</span>, keyList[i])[<span class="number">0</span>][<span class="number">1</span>:<span class="number">-5</span>]</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                val = re.findall(<span class="string">r'\d+\.?.*&lt;/dd&gt;'</span>, valueList[i])[<span class="number">0</span>][<span class="number">0</span>:<span class="number">-5</span>]</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                val = <span class="string">'--'</span></span><br><span class="line">            infoDict[key]=val</span><br><span class="line"> </span><br><span class="line">        infoDict.update(</span><br><span class="line">            &#123;<span class="string">'股票名称'</span>: re.findall(<span class="string">'\s.*\('</span>,name)[<span class="number">0</span>].split()[<span class="number">0</span>] + \</span><br><span class="line">             re.findall(<span class="string">'\&gt;.*\&lt;'</span>, name)[<span class="number">0</span>][<span class="number">1</span>:<span class="number">-1</span>]&#125;)</span><br><span class="line">        <span class="keyword">yield</span> infoDict</span><br></pre></td></tr></table></figure>

<h2 id="Pipelines-py"><a href="#Pipelines-py" class="headerlink" title="Pipelines.py"></a>Pipelines.py</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaidustocksPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaidustocksInfoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.f = open(<span class="string">'BaiduStockInfo.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.f.close()</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            line = str(dict(item)) + <span class="string">'\n'</span></span><br><span class="line">            self.f.write(line)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>


</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/06/21/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC4%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%20%E7%AC%AC%E5%8D%81%E4%B8%80%E5%8D%95%E5%85%83%20Scrapy%E7%88%AC%E8%99%AB%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%EF%BC%89/">Python网络爬虫与信息提取（第4周 网络爬虫框架 第十一单元 Scrapy爬虫基本使用）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-06-21</time><div class="content"><h1 id="Scrapy爬虫的第一个实例"><a href="#Scrapy爬虫的第一个实例" class="headerlink" title="Scrapy爬虫的第一个实例"></a>Scrapy爬虫的第一个实例</h1><p>演示HTML页面地址：     </p>
<p><a href="http://python123.io/ws/demo.html" target="_blank" rel="noopener">http://python123.io/ws/demo.html</a>    </p>
<p>文件名称：demo.html</p>
<p><strong>爬虫步骤：</strong>   </p>
<ol>
<li>建立一个Scrapy爬虫工程       </li>
</ol>
<p>在工程路径下打开cmd, 输入<code>scrapy stratproject python123demo</code>,定义一个名为python123demo的文件夹  </p>
<p><img src="https://i.loli.net/2020/06/21/bAQupzMg26taRKX.png" alt=""></p>
<p>随后打开路径，会发现已经建立好了一个文件夹   </p>
<p><img src="https://i.loli.net/2020/06/21/wH123tiZGexsd9b.png" alt=""></p>
<p>生成的工程路径： </p>
<p><img src="https://i.loli.net/2020/06/21/4kyHIgZ78iR2vfd.png" alt=""></p>
<p><img src="https://i.loli.net/2020/06/21/uOcdYtlCspKI1Qh.png" alt=""></p>
<ol start="2">
<li>在工程中产生一个Scrapy爬虫     </li>
</ol>
<p>进入python123demo,cmd输入<code>scrapy genspider demo python123.io</code>产生名为demo的spider</p>
<p><img src="https://i.loli.net/2020/06/21/JDwHBoA8zhUExbv.png" alt="生成一条名称为demo的spider"></p>
<p>生成爬虫的代码内容：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment">#demo.py文件</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DemoSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'demo'</span>   <span class="comment">#名称</span></span><br><span class="line">    allowed_domains = [<span class="string">'python123.io'</span>]  <span class="comment">#只能爬虫这个域名以下的链接</span></span><br><span class="line">    start_urls = [<span class="string">'http://python123.io/'</span>]   <span class="comment">#爬取页面的初始页面</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p><strong>parse()用于处理响应，解析内容形成字典，发现新的URL爬取请求。</strong></p>
<ol start="3">
<li>配置产生的spider爬虫    </li>
</ol>
<p>对上述spider进行配置：  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DemoSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'demo'</span></span><br><span class="line">    allowed_domains = [<span class="string">'python123.io'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://python123.io/ws/demo.html'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        fname = response.url.split(<span class="string">'/'</span>)[<span class="number">-1</span>] <span class="comment">#存储到本地的response文件名称</span></span><br><span class="line">        <span class="keyword">with</span> open(fname, <span class="string">'wb'</span>) <span class="keyword">as</span> f :</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(<span class="string">"Saved file %s."</span> % name)</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<ol start="4">
<li>运行爬虫，获取网页    </li>
</ol>
<p>cmd内输入<code>scrapy crawl demo</code>，捕获页面存储在html文件中</p>
<p><img src="https://i.loli.net/2020/06/21/zI2sELjWvt3frpF.png" alt=""></p>
<h2 id="yield关键字的使用"><a href="#yield关键字的使用" class="headerlink" title="yield关键字的使用"></a>yield关键字的使用</h2><p>生成器是一个不断产生值的函数。      </p>
<p>包含yield语句的函数是一个生成器。     </p>
<p>生成器每次产生一个值(yield语句)，函数被冻结，被唤醒后再产生一个值。</p>
<p><strong><em>生成器写法：</em></strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen</span><span class="params">(n)</span> :</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n) :</span><br><span class="line">        <span class="keyword">yield</span> i ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> gen(<span class="number">5</span>):</span><br><span class="line">    print(i, <span class="string">' '</span>, end=<span class="string">''</span>)</span><br></pre></td></tr></table></figure>

<p><strong><em>普通写法：</em></strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square</span><span class="params">(n)</span> :</span></span><br><span class="line">    ls = [i**<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">    <span class="keyword">return</span> ls</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> square(<span class="number">5</span>):</span><br><span class="line">    print(i, <span class="string">' '</span>, end=<span class="string">''</span>)</span><br></pre></td></tr></table></figure>

<p>生成器：<br>更节省存储空间<br>响应更迅速<br>使用更灵活   </p>
<h1 id="Scrapy爬虫的基本使用"><a href="#Scrapy爬虫的基本使用" class="headerlink" title="Scrapy爬虫的基本使用"></a>Scrapy爬虫的基本使用</h1><p><strong>Scrapy爬虫的使用步骤</strong></p>
<p>步骤一：创建一个工程和Spider模板     </p>
<p>步骤二：编写Spider    </p>
<p>步骤三：编写Item Pipeline     </p>
<p>步骤四：优化配置策略      </p>
<p><strong>Scrapy爬虫的数据类型</strong></p>
<p>Request类：表示一个HTTP请求，由Spider生成，由Downloader执行。    </p>
<table>
<thead>
<tr>
<th align="center">属性或方法</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">.url</td>
<td align="center">Request对应的请求URL地址</td>
</tr>
<tr>
<td align="center">.method</td>
<td align="center">对应的请求方法，’GET’ ‘POST’等</td>
</tr>
<tr>
<td align="center">.headers</td>
<td align="center">字典类型风格的请求头</td>
</tr>
<tr>
<td align="center">.body</td>
<td align="center">请求内容主体，字符串类型</td>
</tr>
<tr>
<td align="center">.meta</td>
<td align="center">用户添加的扩展信息，在Scrapy内部模块间传递信息使用</td>
</tr>
<tr>
<td align="center">.copy()</td>
<td align="center">复制该请求</td>
</tr>
</tbody></table>
<p>Response类：表示一个HTTP响应，由Downloader生成，由Spider处理</p>
<table>
<thead>
<tr>
<th align="center">属性或方法</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">.url</td>
<td align="center">Response对应的URL地址</td>
</tr>
<tr>
<td align="center">.status</td>
<td align="center">HTTP状态码，默认是200</td>
</tr>
<tr>
<td align="center">.headers</td>
<td align="center">Response对应的头部信息</td>
</tr>
<tr>
<td align="center">.body</td>
<td align="center">Response对应的内容信息，字符串类型</td>
</tr>
<tr>
<td align="center">.flags</td>
<td align="center">一组标记</td>
</tr>
<tr>
<td align="center">.request</td>
<td align="center">产生Response类型对应的Request对象</td>
</tr>
<tr>
<td align="center">.copy()</td>
<td align="center">复制该响应</td>
</tr>
</tbody></table>
<p>Item类：表示从一个HTML页面中提取的信息内容，由Spider生成，由Item Pipeline处理<br>类似于字典，可用字典类型使用      </p>
<p><strong>CSS Selector的基本使用：</strong></p>
<p><img src="https://i.loli.net/2020/06/21/fjLIdzmBqwYDb65.png" alt=""></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/06/21/%E4%BB%8E%E9%94%AE%E7%9B%98%E8%BE%93%E5%85%A5%E5%B9%B4%E6%9C%88%E6%97%A5%EF%BC%8C%E8%AE%A1%E7%AE%97%E6%98%AF%E8%BF%99%E4%B8%80%E5%B9%B4%E4%B8%AD%E7%9A%84%E7%AC%AC%E5%87%A0%E5%A4%A9/">从键盘输入年月日，计算是这一年中的第几天</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-06-21</time><div class="content"><p>这个问题是我同学给我的期末Python模拟题里面的一道，最开始我是试着用判断是否闰年，分30天的月份和31天的月份。      </p>
<p>结果代码越写越长，显然是不太合适的。       </p>
<p>下面的是我用最初的方法写的代码：     </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">months_days30 = [<span class="number">4</span>,<span class="number">6</span>,<span class="number">9</span>,<span class="number">11</span>]</span><br><span class="line">months_days31 = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">10</span>,<span class="number">12</span>]</span><br><span class="line">month_Feb = <span class="number">2</span> <span class="comment">#区分不同的月份对应的天数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isleapyear</span><span class="params">(year)</span> :</span> <span class="comment">#判断是否为闰年</span></span><br><span class="line">    <span class="keyword">if</span> year % <span class="number">4</span> == <span class="number">0</span> :</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span> <span class="comment">#计算天数</span></span><br><span class="line">    year = eval(input(<span class="string">"请输入年份："</span>))</span><br><span class="line">    month = eval(input(<span class="string">"请输入月份："</span>))</span><br><span class="line">    day = eval(input(<span class="string">"请输入年份："</span>))</span><br><span class="line">    num_day31 = <span class="number">0</span></span><br><span class="line">    num_day30 = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> month == <span class="number">1</span> :</span><br><span class="line">        print(str(day))</span><br><span class="line">    <span class="keyword">if</span> month == <span class="number">2</span> :</span><br><span class="line">        print(str(day+<span class="number">31</span>))</span><br><span class="line">    <span class="keyword">for</span> d30 <span class="keyword">in</span> months_days30 :</span><br><span class="line">        <span class="keyword">if</span> d30 &lt; month :</span><br><span class="line">            num_day30 += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">for</span> d31 <span class="keyword">in</span> months_days31 :</span><br><span class="line">        <span class="keyword">if</span> d31 &lt; month :</span><br><span class="line">            num_day31 += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> isleapyear(year) :</span><br><span class="line">        print(num_day30 * <span class="number">30</span> + num_day31 * <span class="number">31</span> + <span class="number">29</span> + day)</span><br><span class="line">    <span class="keyword">else</span> :</span><br><span class="line">        print(num_day30 * <span class="number">30</span> + num_day31 * <span class="number">31</span> + <span class="number">28</span> + day)</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<p>后来又翻了一下嵩天的《Python语言程序设计基础》，复习了一下time库和datetime库，发现几行代码就可以解决。    </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">date_input = input(<span class="string">"请输入日期(年-月-日）："</span>)</span><br><span class="line">t = time.strptime(date_input, <span class="string">'%Y-%m-%d'</span>)</span><br><span class="line">print(time.strftime(<span class="string">'%j'</span>,t))</span><br></pre></td></tr></table></figure>

<p>具体方法：   </p>
<table>
<thead>
<tr>
<th align="center">方法</th>
<th align="center">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="center">%a</td>
<td align="center">英文星期简写</td>
</tr>
<tr>
<td align="center">%A</td>
<td align="center">英文星期的完全</td>
</tr>
<tr>
<td align="center">%b</td>
<td align="center">英文月份的简写</td>
</tr>
<tr>
<td align="center">%B</td>
<td align="center">英文月份的完全</td>
</tr>
<tr>
<td align="center">%c</td>
<td align="center">显示本地日期时间</td>
</tr>
<tr>
<td align="center">%d</td>
<td align="center">日期，取1-31</td>
</tr>
<tr>
<td align="center">%H</td>
<td align="center">小时， 0-23</td>
</tr>
<tr>
<td align="center">%I</td>
<td align="center">小时， 0-12</td>
</tr>
<tr>
<td align="center">%m</td>
<td align="center">月， 01 -12</td>
</tr>
<tr>
<td align="center">%M</td>
<td align="center">分钟，1-59</td>
</tr>
<tr>
<td align="center">%j</td>
<td align="center">年中当天的天数</td>
</tr>
<tr>
<td align="center">%w</td>
<td align="center">显示今天是星期几</td>
</tr>
<tr>
<td align="center">%W</td>
<td align="center">第几周</td>
</tr>
<tr>
<td align="center">%x</td>
<td align="center">当天日期</td>
</tr>
<tr>
<td align="center">%X</td>
<td align="center">本地的当天时间</td>
</tr>
<tr>
<td align="center">%y</td>
<td align="center">年份 00-99间</td>
</tr>
<tr>
<td align="center">%Y</td>
<td align="center">年份的完整拼写</td>
</tr>
</tbody></table>
<p>所以在这个实例中，直接用<code>%j</code>即可显示日期对应的当年天数。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/06/20/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC4%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%20%E7%AC%AC%E5%8D%81%E5%8D%95%E5%85%83%20Scrapy%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%EF%BC%89/">Python网络爬虫与信息提取（第4周 网络爬虫框架 第十单元 Scrapy爬虫框架）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-06-20</time><div class="content"><h1 id="Scrapy爬虫框架介绍"><a href="#Scrapy爬虫框架介绍" class="headerlink" title="Scrapy爬虫框架介绍"></a>Scrapy爬虫框架介绍</h1><h2 id="Scrapy的安装"><a href="#Scrapy的安装" class="headerlink" title="Scrapy的安装"></a>Scrapy的安装</h2><p>命令行输入<code>pip install Scrapy</code>       </p>
<p>安装后小测：执行<code>scrapy -h</code>     </p>
<p>安装的过程中可能会出现失败的情况，报错显示<code>error:MicrosoftVisual C++ 14.0 is required</code>   </p>
<p><strong>解决办法：</strong><br>根据Python版本和系统位数选择对应的Twisted模块(<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/" target="_blank" rel="noopener">点击此处下载</a>)，例如我的Python是3.8，系统位数为64位，所以选择</p>
<p><img src="https://i.loli.net/2020/06/20/rL13TsQMgJwFIjA.png" alt=""></p>
<p>将文件下载到Python/Scripts文件夹，cd到此文件夹内输入<code>pip install Twisted-20.3.0-cp38-cp38-win_amd64.whl</code>      </p>
<p>安装成功后再输入<code>pip install scrapy</code>即可成功    </p>
<p>scrapy不单单是一个库，而是一个爬虫框架      </p>
<p><strong>爬虫框架：</strong>是实现爬虫功能的一个软件结构和功能组件集合      </p>
<h2 id="Scrapy爬虫框架结构"><a href="#Scrapy爬虫框架结构" class="headerlink" title="Scrapy爬虫框架结构"></a>Scrapy爬虫框架结构</h2><p><strong>5+2结构和3条数据流：</strong></p>
<p><img src="https://i.loli.net/2020/06/20/UGyuikEcmS3aNzr.png" alt="5+2结构和3条数据流"></p>
<p>结构中，用户需要编写配置的是模块是<strong>SPIDERS(入口)</strong>和<strong>ITEM PIPELINES(出口)</strong></p>
<h1 id="Scrapy爬虫框架解析"><a href="#Scrapy爬虫框架解析" class="headerlink" title="Scrapy爬虫框架解析"></a>Scrapy爬虫框架解析</h1><p><strong>Engine：</strong>控制所有模块之间的数据流/根据条件出发事件，不需要用户修改       </p>
<p><strong>Downloader：</strong>跟去请求下载网页，不需要用户修改     </p>
<p><strong>Scheduler:</strong>对所有爬取请求进行调度管理，不需要用户修改     </p>
<p><strong>Downloader Middleware:</strong>实施上述三组模块站之间进行用户可配置的控制。修改、丢弃、新增请求或相应。     </p>
<p><strong>Spider:</strong>解析Downloader返回的响应(Response)/产生爬取项(scraped item)/产生额外的爬取请求(Request)       </p>
<p><strong>Item Pipelines:</strong>以流水线方式处理Spider产生的爬取项/由一组操作顺序组成，类似流水线，每个操作是一个Item Pipeline类型/可能操作包括：清理、检验和查重爬取项中的HTML数据，将数据存储到数据库     </p>
<p><strong>Spider Middleware:</strong>对请求和爬取项的再处理。修改、丢弃、新增请求或爬取项。       </p>
<h1 id="Requests库和Scrapy框架的比较"><a href="#Requests库和Scrapy框架的比较" class="headerlink" title="Requests库和Scrapy框架的比较"></a>Requests库和Scrapy框架的比较</h1><p><strong>相同点：</strong>       </p>
<p>两者都可以进行页面请求和爬取，Python爬虫的两个重要技术路线        </p>
<p>两者可用性都好，文档丰富，入门简单       </p>
<p>两者都没有处理js、提交表单、应对验证码等功能(可扩展)        </p>
<p><strong>不同点:</strong>    </p>
<table>
<thead>
<tr>
<th align="center">requests</th>
<th align="center">Serapy</th>
</tr>
</thead>
<tbody><tr>
<td align="center">页面级爬虫</td>
<td align="center">网站级爬虫</td>
</tr>
<tr>
<td align="center">功能库</td>
<td align="center">框架</td>
</tr>
<tr>
<td align="center">并发性考虑不足，性能较差</td>
<td align="center">并发性好，性能较高</td>
</tr>
<tr>
<td align="center">重点在于页面下载</td>
<td align="center">重点在于爬虫结构</td>
</tr>
<tr>
<td align="center">定制灵活</td>
<td align="center">一般定制灵活，深度定制困难</td>
</tr>
<tr>
<td align="center">上手十分简单</td>
<td align="center">入门稍难</td>
</tr>
</tbody></table>
<p><strong><em>对于非常小的需求，用Requests库。</em></strong>      </p>
<p><strong><em>对于不太小的需求，用Scrapy框架</em></strong></p>
<h1 id="Scrapy爬虫的常用指令"><a href="#Scrapy爬虫的常用指令" class="headerlink" title="Scrapy爬虫的常用指令"></a>Scrapy爬虫的常用指令</h1><p>Scrapy是为持续运行设计的专业爬虫框架，提供操作的Scrapy命令行。  </p>
<p>cmd里输入<code>scrapy -h</code>即可打开Scrapy命令行      </p>
<p>Scrapy命令行格式：    </p>
<blockquote>
<p>scrapy<command>[options][args]     </p>
</blockquote>
<p><strong>Scrapy常用命令</strong></p>
<table>
<thead>
<tr>
<th align="center">命令</th>
<th align="center">说明</th>
<th align="center">格式</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>startproject</strong></td>
<td align="center">创建一个新工程</td>
<td align="center">scrapy startproject<name>[dir]</td>
</tr>
<tr>
<td align="center"><strong>genspider</strong></td>
<td align="center">创建一个爬虫</td>
<td align="center">scrapy genspider [options]<name><domain></td>
</tr>
<tr>
<td align="center">settings</td>
<td align="center">获得爬虫配置信息</td>
<td align="center">scrapy settings [options]</td>
</tr>
<tr>
<td align="center"><strong>crawl</strong></td>
<td align="center">运行一个爬虫</td>
<td align="center">scrapy crawl <spider></td>
</tr>
<tr>
<td align="center">list</td>
<td align="center">列出工程中所有爬虫</td>
<td align="center">scrapy list</td>
</tr>
<tr>
<td align="center">shell</td>
<td align="center">启动URL调试命令行</td>
<td align="center">scrapy shell [url]</td>
</tr>
</tbody></table>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/06/18/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC3%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%20%E7%AC%AC%E5%85%AB%E4%B9%9D%E5%8D%95%E5%85%83%20%E5%AE%9E%E4%BE%8B%EF%BC%89/">Python网络爬虫与信息提取（第3周 网络爬虫实战 第八九单元 实例）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-06-18</time><div class="content"><h1 id="淘宝商品比价定向爬虫实例"><a href="#淘宝商品比价定向爬虫实例" class="headerlink" title="淘宝商品比价定向爬虫实例"></a>淘宝商品比价定向爬虫实例</h1><hr>
<p><strong>流程图：</strong></p>
<p><img src="https://i.loli.net/2020/06/18/2Amz9ujKZkIwlNd.png" alt="流程图"></p>
<p><strong>代码编写：</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHTMLText</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url, timeout=<span class="number">30</span>)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding = r.apparent_encoding</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parsePage</span><span class="params">(ilt, html)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        plt = re.findall(<span class="string">r'\"view_price\"\:\"[\d\.]*\"'</span>, html)</span><br><span class="line">        tlt = re.findall(<span class="string">r'\"raw_title\"\:\".*?\"'</span>, html)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(plt)):</span><br><span class="line">            price = eval(plt[i].split(<span class="string">':'</span>)[<span class="number">1</span>])</span><br><span class="line">            title = eval(tlt[i].split(<span class="string">':'</span>)[<span class="number">1</span>])</span><br><span class="line">            ilt.append([price, title])</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printGoodsList</span><span class="params">(ilt)</span>:</span></span><br><span class="line">    tplt = <span class="string">"&#123;:4&#125;\t&#123;:8&#125;\t&#123;:16&#125;"</span></span><br><span class="line">    print(tplt.format(<span class="string">"序号"</span>, <span class="string">"价格"</span>, <span class="string">"商品名称"</span>))</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> g <span class="keyword">in</span> ilt:</span><br><span class="line">        count = count + <span class="number">1</span></span><br><span class="line">        print(tplt.format(count, g[<span class="number">0</span>], g[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    goods = <span class="string">'书包'</span></span><br><span class="line">    depth = <span class="number">3</span></span><br><span class="line">    start_url = <span class="string">'https://s.taobao.com/search?q='</span> + goods</span><br><span class="line">    infoList = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(depth):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            url = start_url + <span class="string">'&amp;s='</span> + str(<span class="number">44</span> * i)</span><br><span class="line">            html = getHTMLText(url)</span><br><span class="line">            parsePage(infoList, html)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    printGoodsList(infoList)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<h1 id="股票数据定向爬虫实例"><a href="#股票数据定向爬虫实例" class="headerlink" title="股票数据定向爬虫实例"></a>股票数据定向爬虫实例</h1><hr>
<p>候选数据网站选择：<a href="http://finance.sina.com.cn/stock/" target="_blank" rel="noopener">新浪股票</a>、<a href="https://gupiao.baidu.com/stock/" target="_blank" rel="noopener">百度股票</a>、<a href="http://quote.eastmoney.com/center/gridlist.html#hs_a_board" target="_blank" rel="noopener">东方财富网</a><br><strong>数据网站选择原则：</strong><br>股票信息静态存在于HTML页面中，非js代码生成，没有Robots协议限制。</p>
<p><strong>流程图：</strong></p>
<p><img src="https://i.loli.net/2020/06/19/DvXQAj4ikfZswap.png" alt="流程图"></p>
<p><strong>代码编写：</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#CrawBaiduStocksB.py</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> traceback</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHTMLText</span><span class="params">(url, code=<span class="string">"utf-8"</span>)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding = code</span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getStockList</span><span class="params">(lst, stockURL)</span>:</span></span><br><span class="line">    html = getHTMLText(stockURL, <span class="string">"GB2312"</span>)</span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">'html.parser'</span>) </span><br><span class="line">    a = soup.find_all(<span class="string">'a'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            href = i.attrs[<span class="string">'href'</span>]</span><br><span class="line">            lst.append(re.findall(<span class="string">r"[s][hz]\d&#123;6&#125;"</span>, href)[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getStockInfo</span><span class="params">(lst, stockURL, fpath)</span>:</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> stock <span class="keyword">in</span> lst:</span><br><span class="line">        url = stockURL + stock + <span class="string">".html"</span></span><br><span class="line">        html = getHTMLText(url)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> html==<span class="string">""</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            infoDict = &#123;&#125;</span><br><span class="line">            soup = BeautifulSoup(html, <span class="string">'html.parser'</span>)</span><br><span class="line">            stockInfo = soup.find(<span class="string">'div'</span>,attrs=&#123;<span class="string">'class'</span>:<span class="string">'stock-bets'</span>&#125;)</span><br><span class="line"> </span><br><span class="line">            name = stockInfo.find_all(attrs=&#123;<span class="string">'class'</span>:<span class="string">'bets-name'</span>&#125;)[<span class="number">0</span>]</span><br><span class="line">            infoDict.update(&#123;<span class="string">'股票名称'</span>: name.text.split()[<span class="number">0</span>]&#125;)</span><br><span class="line">             </span><br><span class="line">            keyList = stockInfo.find_all(<span class="string">'dt'</span>)</span><br><span class="line">            valueList = stockInfo.find_all(<span class="string">'dd'</span>)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(keyList)):</span><br><span class="line">                key = keyList[i].text</span><br><span class="line">                val = valueList[i].text</span><br><span class="line">                infoDict[key] = val</span><br><span class="line">             </span><br><span class="line">            <span class="keyword">with</span> open(fpath, <span class="string">'a'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write( str(infoDict) + <span class="string">'\n'</span> )</span><br><span class="line">                count = count + <span class="number">1</span></span><br><span class="line">                print(<span class="string">"\r当前进度: &#123;:.2f&#125;%"</span>.format(count*<span class="number">100</span>/len(lst)),end=<span class="string">""</span>)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            count = count + <span class="number">1</span></span><br><span class="line">            print(<span class="string">"\r当前进度: &#123;:.2f&#125;%"</span>.format(count*<span class="number">100</span>/len(lst)),end=<span class="string">""</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    stock_list_url = <span class="string">'http://quote.eastmoney.com/center/gridlist.html#hs_a_board'</span></span><br><span class="line">    stock_info_url = <span class="string">'https://gupiao.baidu.com/stock/'</span></span><br><span class="line">    output_file = <span class="string">'D:/BaiduStockInfo.txt'</span></span><br><span class="line">    slist=[]</span><br><span class="line">    getStockList(slist, stock_list_url)</span><br><span class="line">    getStockInfo(slist, stock_info_url, output_file)</span><br><span class="line"> </span><br><span class="line">main()</span><br></pre></td></tr></table></figure></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/06/17/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC3%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%20%E5%8D%95%E5%85%83%E4%B8%83%20Re%E5%BA%93%E5%85%A5%E9%97%A8%EF%BC%89/">Python网络爬虫与信息提取（第3周 网络爬虫实战 单元七 Re库入门）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-06-17</time><div class="content"><h1 id="正则表达式的概念"><a href="#正则表达式的概念" class="headerlink" title="正则表达式的概念"></a>正则表达式的概念</h1><p>Regular expression–Re<br><strong>·通用的字符串表达框架<br>·简洁表达一组字符串的表达式<br>·针对字符串表达“简洁”和“特征”思想的工具<br>·判断某字符串的特征归属</strong>        </p>
<h1 id="正则表达式的语法"><a href="#正则表达式的语法" class="headerlink" title="正则表达式的语法"></a>正则表达式的语法</h1><p>eg:<strong>P(Y|YT|YTH|YTHO)?N</strong><br>正则表达式由字符和操作符构成       </p>
<p><strong>正则表达式常用的操作符</strong></p>
<table>
<thead>
<tr>
<th align="center">操作符</th>
<th align="center">说明</th>
<th align="center">实例</th>
</tr>
</thead>
<tbody><tr>
<td align="center">.</td>
<td align="center">表示任何单个字符</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">[ ]</td>
<td align="center">字符集，对单个字符给出取值范围</td>
<td align="center">[abe]表示a、 b、c, [a-z]表示a到z单个字符</td>
</tr>
<tr>
<td align="center">[^ ]</td>
<td align="center">非字符集，对单个字符给出排除范围</td>
<td align="center">[^abc]表示非a或b或c的单个字符</td>
</tr>
<tr>
<td align="center">*</td>
<td align="center">前一个字符0次或无限次扩展</td>
<td align="center">abc*表示ab、abc、abce、abecc等</td>
</tr>
<tr>
<td align="center">+</td>
<td align="center">前一个字符1次或无限次扩展</td>
<td align="center">abc+表示abc、abcc、 abccc等</td>
</tr>
<tr>
<td align="center">?</td>
<td align="center">前一个字符0次或1次扩展</td>
<td align="center">abc?表示ab、abc</td>
</tr>
<tr>
<td align="center">`</td>
<td align="center">`</td>
<td align="center">左右表达式任意一个</td>
</tr>
<tr>
<td align="center"><code>{m}</code></td>
<td align="center">扩展前一个字符m次</td>
<td align="center">ab{2}c表示abbc</td>
</tr>
<tr>
<td align="center"><code>{m,n}</code></td>
<td align="center">扩展前一个字符m至n次(含n)</td>
<td align="center">ab{1,2}c表示abc、abbc</td>
</tr>
<tr>
<td align="center">^</td>
<td align="center">匹配字符串开头</td>
<td align="center">^abc表示abc且在一个字符串的开头</td>
</tr>
<tr>
<td align="center">$</td>
<td align="center">匹配字符串结尾</td>
<td align="center">abc$表示abc且在一个字符串的结尾</td>
</tr>
<tr>
<td align="center">()</td>
<td align="center">分组标记，内部只能使用`</td>
<td align="center">`操作符</td>
</tr>
<tr>
<td align="center">\d</td>
<td align="center">数字，等价于[0-9]</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">\w</td>
<td align="center">单词字符，等价于[A-Za-z0-9_]</td>
<td align="center"></td>
</tr>
</tbody></table>
<p><strong>正则表达式语法实例</strong></p>
<table>
<thead>
<tr>
<th align="center">正则表达式</th>
<th align="center">对应字符串</th>
</tr>
</thead>
<tbody><tr>
<td align="center">`P(Y</td>
<td align="center">YT</td>
</tr>
<tr>
<td align="center">PYTHON+</td>
<td align="center">‘PYTHON’ ‘PYTHONN’ ‘PYTHONNN’…</td>
</tr>
<tr>
<td align="center">PY[TH]ON</td>
<td align="center">‘PYTON’ ‘PYHON’</td>
</tr>
<tr>
<td align="center">PY[^TH]?ON</td>
<td align="center">‘PYON’ ‘PTaon’ ‘PYbON’ ‘PYcON’…</td>
</tr>
<tr>
<td align="center">PY{:3}N</td>
<td align="center">‘PN’ ‘PYN’ ‘PYYN’ ‘PYYYN’</td>
</tr>
</tbody></table>
<p><strong>经典正则表达式实例</strong><br>^[A-Za-z]+$&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;由26个字母组成的字符串<br>^[A-Za-z0-9]+$&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;由26个字母和数字组成的字符串<br>^-?\d+$&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&emsp;&ensp;整数形式的字符串<br><code>^[0-9]*[1-9][0-9]*$</code>&ensp;&ensp;&ensp;&ensp;&ensp;正整数形式的字符串<br>[1-9]\d{5}&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;中国境内邮政编码，6位<br>[\u4e00-\u9fa5]&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;匹配中文字符<br>\d{3}-\d{8}|\d{4}-\d{7}&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;国内电话号码，010-68913536</p>
<h1 id="Re库的基本使用"><a href="#Re库的基本使用" class="headerlink" title="Re库的基本使用"></a>Re库的基本使用</h1><p>正则表达式的表达类型<br><strong>·raw string类型(原生字符串)<br>·sting类型，较繁琐</strong>      </p>
<h2 id="Re库主要功能函数"><a href="#Re库主要功能函数" class="headerlink" title="Re库主要功能函数"></a>Re库主要功能函数</h2><table>
<thead>
<tr>
<th align="center">函数</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">re.search()</td>
<td align="center">在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象</td>
</tr>
<tr>
<td align="center">re.match()</td>
<td align="center">从一个字符串的开始位置起匹配正则表达式，返回match对象</td>
</tr>
<tr>
<td align="center">re.findall()</td>
<td align="center">搜索字符串，以列表类型返回全部能匹配的子串</td>
</tr>
<tr>
<td align="center">re.split()</td>
<td align="center">将一个字符串按照正则表达式匹配结果进行分割，返回列表类型</td>
</tr>
<tr>
<td align="center">re.finditer()</td>
<td align="center">搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象</td>
</tr>
<tr>
<td align="center">re.sub()</td>
<td align="center">在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串</td>
</tr>
</tbody></table>
<p><strong>re.search(parttern,string,flags=0)</strong><br>※在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象。<br>pattern:正则表达式的字符串或原生字符串表示<br>strings:待匹配字符串<br>flags:正则表达式使用时的控制标记<br>flags常用控制标记：    </p>
<table>
<thead>
<tr>
<th align="center">常用标记</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">re.I(re.IGNORECASE)</td>
<td align="center">忽略正则表达式的大小写，[A-Z]能够匹配小写字符</td>
</tr>
<tr>
<td align="center">re.M(re.MULTLINE)</td>
<td align="center">正则表达式中的^操作符能够将给定字符串的每行当作匹配开始</td>
</tr>
<tr>
<td align="center">re.S(re.DOTALL)</td>
<td align="center">正则表达式中的.操作符能够匹配所有字符，默认匹配除换行外的所有字符</td>
</tr>
</tbody></table>
<p><strong>re.match(pattern,string,flags=0)</strong><br>※从一个字符串的开始位置起匹配正则表达式，返回match对象。<br>pattern:正则表达式的字符串或原生字符串表示<br>string:待匹配字符串<br>flags:正则表达式使用时的控制标记     </p>
<p><strong>re.findall(pattern,string,flags=0)</strong><br>※搜索字符串，以列表类型返回全部能匹配的子串。<br>pattern:正则表达式的字符串或原生字符串表示<br>string;待匹配字符串<br>flags:正则表达式使用时的控制标记     </p>
<p><strong>re.split(pattern, string, maxsplit=0, flags=0)</strong><br>※将一个字符串按照正则表达式匹配结果进行分割，返回列表类型。<br>pattern:正则表达式的字符串或原生字符串表示<br>string;待匹配字符串<br>maxsplit:最大分割数，剩余部分作为最后一个元素输出<br>flags:正则表达式使用时的控制标记     </p>
<p><strong>re.finditer(pattern,string,flags=0)</strong><br>※搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象。<br>pattern:正则表达式的字符串或原生字符串表示<br>string:待匹配字符串<br>flags:正则表达式使用时的控制标记     </p>
<p><strong>re.sub(pattern, repl, string, count=0, flags=0)</strong><br>※在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串。<br>pattern:正则表达式的字符串或原生字符串表示<br>repl:替换匹配字符串的字符串<br>string:待匹配字符串<br>count:匹配的最大替换次数<br>flags:正则表达式使用时的控制标记     </p>
<p><strong>regex = re.compile(pattern, flags=0)</strong><br>※将正则表达式的字符串形式编译成正则表达式对象<br>pattern:正则表达式的字符串或原生字符串表示<br>flags:正则表达式使用时的控制标记     </p>
<h2 id="Re库的Match对象"><a href="#Re库的Match对象" class="headerlink" title="Re库的Match对象"></a>Re库的Match对象</h2><p><strong>Match对象的属性</strong>  </p>
<table>
<thead>
<tr>
<th align="center">属性</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">.string</td>
<td align="center">待匹配的文本</td>
</tr>
<tr>
<td align="center">.re</td>
<td align="center">匹配时使用的pattern对象(正则表达式)</td>
</tr>
<tr>
<td align="center">.pos</td>
<td align="center">正则表达式搜索文本的开始位置</td>
</tr>
<tr>
<td align="center">.endpos</td>
<td align="center">正则表达式搜索文本的结束位置</td>
</tr>
</tbody></table>
<p><strong>Match对象的方法</strong></p>
<table>
<thead>
<tr>
<th align="center">方法</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">.group(0)</td>
<td align="center">获得匹配后的字符串</td>
</tr>
<tr>
<td align="center">.start()</td>
<td align="center">匹配字符串在原始字符串的开始位置</td>
</tr>
<tr>
<td align="center">.end()</td>
<td align="center">匹配字符串在原始字符串的结束位置</td>
</tr>
<tr>
<td align="center">.span()</td>
<td align="center">返回(.start(),.end())</td>
</tr>
</tbody></table>
<h2 id="Re库的贪婪匹配和最小匹配"><a href="#Re库的贪婪匹配和最小匹配" class="headerlink" title="Re库的贪婪匹配和最小匹配"></a>Re库的贪婪匹配和最小匹配</h2><p><strong>Re库默认采用贪婪匹配，即输出匹配最长的字符串</strong>     </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">match = re.search(<span class="string">r'PY.*N'</span>, <span class="string">'PYANBNCNDN'</span>)</span><br><span class="line">print(match.group(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<p>输出：<code>PYANBNCNDN</code></p>
<p><strong>若要输出最小匹配字符串：</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">match = re.search(<span class="string">r'PY.*?N'</span>, <span class="string">'PYANBNCNDN'</span>)</span><br><span class="line">print(match.group(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<p>输出：<code>PYAN</code></p>
<p><strong>最小匹配操作符</strong></p>
<table>
<thead>
<tr>
<th align="center">操作符</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">*?</td>
<td align="center">前一个字符0次或无限次扩展，最小匹配</td>
</tr>
<tr>
<td align="center">+?</td>
<td align="center">前一个字符1次或无限次扩展，最小匹配</td>
</tr>
<tr>
<td align="center">??</td>
<td align="center">前一个字符0次或1次扩展，最小匹配</td>
</tr>
<tr>
<td align="center"><code>{m,n}?</code></td>
<td align="center">扩展前一个字符m至n次(含n)，最小匹配</td>
</tr>
</tbody></table>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/06/16/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC2%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%8F%90%E5%8F%96%20%E5%8D%95%E5%85%83%E5%85%AD%20%E4%B8%AD%E5%9B%BD%E5%A4%A7%E5%AD%A6%E6%8E%92%E5%90%8D%E7%88%AC%E8%99%AB%EF%BC%89/">Python网络爬虫与信息提取（第2周 网络爬虫提取 单元六 中国大学排名爬虫）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-06-16</time><div class="content"><p><img src="https://i.loli.net/2020/06/22/ZTaXs6i9SwBAzQt.png" alt="程序流程图"></p>
<h1 id="定向爬虫实例"><a href="#定向爬虫实例" class="headerlink" title="定向爬虫实例"></a>定向爬虫实例</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> bs4</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHTMLText</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url, timeout = <span class="number">30</span>)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding = r.apparent_encoding</span><br><span class="line">        <span class="keyword">return</span>  r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fillUnivList</span><span class="params">(ulist, html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">"html.parser"</span>)</span><br><span class="line">    <span class="keyword">for</span> tr <span class="keyword">in</span> soup.find(<span class="string">'tbody'</span>).children:</span><br><span class="line">        <span class="keyword">if</span> isinstance(tr, bs4.element.Tag):</span><br><span class="line">            tds = tr(<span class="string">'td'</span>)</span><br><span class="line">            ulist.append([tds[<span class="number">0</span>].string, tds[<span class="number">1</span>].string, tds[<span class="number">4</span>].string])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printUnivList</span><span class="params">(ulist, num)</span>:</span></span><br><span class="line">    print(<span class="string">"&#123;:^10&#125;\t&#123;:^6&#125;\t&#123;:^10&#125;"</span>.format(<span class="string">"排名"</span>, <span class="string">"学校名称"</span>, <span class="string">"总分"</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num):</span><br><span class="line">        u = ulist[i]</span><br><span class="line">        print(<span class="string">"&#123;:^10&#125;\t&#123;:^6&#125;\t&#123;:^10&#125;"</span>.format(u[<span class="number">0</span>], u[<span class="number">1</span>], u[<span class="number">2</span>]))</span><br><span class="line">    print(<span class="string">"Suc"</span> + str(num))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    unifo = []</span><br><span class="line">    url = <span class="string">"http://www.zuihaodaxue.com/zuihaodaxuepaiming2020.html"</span></span><br><span class="line">    html = getHTMLText(url)</span><br><span class="line">    fillUnivList(unifo, html)</span><br><span class="line">    printUnivList(unifo, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<p><strong>输出结果</strong></p>
<p><img src="https://i.loli.net/2020/06/16/bB91nZezTtF7Usd.png" alt="输出结果"></p>
<h1 id="定向爬虫实例优化"><a href="#定向爬虫实例优化" class="headerlink" title="定向爬虫实例优化"></a>定向爬虫实例优化</h1><p><strong>中文对齐问题的解决</strong>：采用中文字符的空格填充<code>chr(12288)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#旨在优化中文字符空格的问题</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> bs4</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHTMLText</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url, timeout = <span class="number">30</span>)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        r.encoding = r.apparent_encoding</span><br><span class="line">        <span class="keyword">return</span>  r.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fillUnivList</span><span class="params">(ulist, html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">"html.parser"</span>)</span><br><span class="line">    <span class="keyword">for</span> tr <span class="keyword">in</span> soup.find(<span class="string">'tbody'</span>).children:</span><br><span class="line">        <span class="keyword">if</span> isinstance(tr, bs4.element.Tag):</span><br><span class="line">            tds = tr(<span class="string">'td'</span>)</span><br><span class="line">            ulist.append([tds[<span class="number">0</span>].string, tds[<span class="number">1</span>].string, tds[<span class="number">4</span>].string])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printUnivList</span><span class="params">(ulist, num)</span>:</span></span><br><span class="line">    tplt = <span class="string">"&#123;0:^10&#125;\t&#123;1:&#123;3&#125;^10&#125;\t&#123;2:^10&#125;"</span></span><br><span class="line">    print(tplt.format(<span class="string">"排名"</span>, <span class="string">"学校名称"</span>, <span class="string">"总分"</span>, chr(<span class="number">12288</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num):</span><br><span class="line">        u = ulist[i]</span><br><span class="line">        print(tplt.format(u[<span class="number">0</span>], u[<span class="number">1</span>], u[<span class="number">2</span>], chr(<span class="number">12288</span>)))</span><br><span class="line">    print(<span class="string">"Suc"</span> + str(num))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    unifo = []</span><br><span class="line">    url = <span class="string">"http://www.zuihaodaxue.com/zuihaodaxuepaiming2020.html"</span></span><br><span class="line">    html = getHTMLText(url)</span><br><span class="line">    fillUnivList(unifo, html)</span><br><span class="line">    printUnivList(unifo, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<p><strong>输出结果</strong></p>
<p><img src="https://i.loli.net/2020/06/16/l6EbVGoksfxhXza.png" alt="输出结果"></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/06/16/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC2%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%8F%90%E5%8F%96%20%E5%8D%95%E5%85%83%E4%BA%94%20%E4%BF%A1%E6%81%AF%E7%BB%84%E7%BB%87%E4%B8%8E%E6%8F%90%E5%8F%96%E6%96%B9%E6%B3%95%EF%BC%89/">Python网络爬虫与信息提取（第2周 网络爬虫提取 单元五 信息组织与提取方法）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-06-16</time><div class="content"><h1 id="信息标记的三种形式"><a href="#信息标记的三种形式" class="headerlink" title="信息标记的三种形式"></a>信息标记的三种形式</h1><h2 id="XML（eXtensible-Markup-Language）"><a href="#XML（eXtensible-Markup-Language）" class="headerlink" title="XML（eXtensible Markup Language）"></a>XML（eXtensible Markup Language）</h2><p>标签中含有内容时：<code>&lt;name&gt; ... &lt;/name&gt;</code>  </p>
<p>标签中不含内容时：<code>&lt;name/&gt;</code>     </p>
<p>注释：<code>&lt;!-- --&gt;</code></p>
<h2 id="JSON-JavaScript-Object-Notation"><a href="#JSON-JavaScript-Object-Notation" class="headerlink" title="JSON(JavaScript Object Notation)"></a>JSON(JavaScript Object Notation)</h2><p>“key” : “value”<br>“key” : [“value1”,”value2”]<br>“key” : {“subkey”:”subvalue”}<br>“key” : {<br>&ensp;&ensp;”name1” : “value1”<br>&ensp;&ensp;”name2” : “value2”<br>}   </p>
<h2 id="YAML"><a href="#YAML" class="headerlink" title="YAML"></a>YAML</h2><p><strong>无类型键值对</strong> key:name     </p>
<p><strong>通过缩进表达所属关系</strong>：<br>name :<br>&ensp;&ensp;newname :<br>&ensp;&ensp;oldname :     </p>
<p><strong>用-号表达并列关系</strong>：<br>name :<br>-name1<br>-name2      </p>
<p><strong>用|表示整块数据 #表示注释</strong>      </p>
<h1 id="三种信息标记形式的比较"><a href="#三种信息标记形式的比较" class="headerlink" title="三种信息标记形式的比较"></a>三种信息标记形式的比较</h1><p>XML Internet.上的信息交互与传递。<br>JSON 移动应用云端和节点的信息通信，无注释。<br>YAML 各类系统的配置文件，有注释易读。       </p>
<h1 id="信息提取的一般方法"><a href="#信息提取的一般方法" class="headerlink" title="信息提取的一般方法"></a>信息提取的一般方法</h1><h2 id="方法一-完整解析信息的标记形式，再提取关键信息。"><a href="#方法一-完整解析信息的标记形式，再提取关键信息。" class="headerlink" title="方法一:完整解析信息的标记形式，再提取关键信息。"></a>方法一:完整解析信息的标记形式，再提取关键信息。</h2><p>XML JSON YAML<br><strong>需要标记解析器</strong><br>例如: bs4库的标签树遍历<br>优点:信息解析准确<br>缺点:提取过程繁琐，速度慢。      </p>
<h2 id="方法二-无视标记形式，直接搜索关键信息。"><a href="#方法二-无视标记形式，直接搜索关键信息。" class="headerlink" title="方法二:无视标记形式，直接搜索关键信息。"></a>方法二:无视标记形式，直接搜索关键信息。</h2><p><strong>搜索</strong><br>对信息的文本查找函数即可。<br>优点:提取过程简洁，速度较快。<br>缺点:提取结果准确性与信息内容相关。      </p>
<h1 id="基于bs4库的HTML内容查找方法"><a href="#基于bs4库的HTML内容查找方法" class="headerlink" title="基于bs4库的HTML内容查找方法"></a>基于bs4库的HTML内容查找方法</h1><p><strong>&lt; &gt;.find_ all(name, attrs, recursive, string, **kwargs)</strong><br>返回一个列表类型，存储查找的结果。<br><strong>name</strong>:对标签名称的检索字符串。<br><strong>attrs</strong>:对标签属性值的检索字符串，可标注属性检索。<br><strong>recursive</strong>:是否对子孙全部检索，默认True。<br><strong>string: &lt; &gt;… &lt; / &gt;</strong>:中字符串区域的检索字符串。      </p>
<p><strong>扩展方法</strong></p>
<table>
<thead>
<tr>
<th align="center">方法</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">&lt;&gt;.find()</td>
<td align="center">搜索且只返回一个结果，字符串类型，同.find_all()参数</td>
</tr>
<tr>
<td align="center">&lt;&gt;.find_parents()</td>
<td align="center">在先辈节点中搜索，返回列表类型，同.find_all()参数</td>
</tr>
<tr>
<td align="center">&lt;&gt;.find_parent()</td>
<td align="center">在先辈节点中返回一个结果，字符串类型，同.find()参数</td>
</tr>
<tr>
<td align="center">&lt;&gt;.find_next_siblings()</td>
<td align="center">在后续平行节点中搜索，返回列表类型，同.find_all()参数</td>
</tr>
<tr>
<td align="center">&lt;&gt;.find_ next sibling()</td>
<td align="center">在后续平行节点中返回一个结果，字符串类型,同.find()参数</td>
</tr>
<tr>
<td align="center">&lt;&gt;.find_previous_siblings()</td>
<td align="center">在前序平行节点中搜索，返回列表类型，同.find_all()参数</td>
</tr>
<tr>
<td align="center">&lt;&gt;.find_previous_sibling()</td>
<td align="center">在前序平行节点中返回一个结果，字符串类型，同.find()参数</td>
</tr>
</tbody></table>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/06/11/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%EF%BC%88%E7%AC%AC2%E5%91%A8%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%8F%90%E5%8F%96%20%E5%8D%95%E5%85%83%E5%9B%9B%20BeautifulSoup%E5%BA%93%E5%85%A5%E9%97%A8%EF%BC%89/">Python网络爬虫与信息提取（第2周 网络爬虫提取 单元四 BeautifulSoup库入门）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-06-11</time><div class="content"><h1 id="BeautifulSoup库的安装"><a href="#BeautifulSoup库的安装" class="headerlink" title="BeautifulSoup库的安装"></a>BeautifulSoup库的安装</h1><p>命令行内输入<code>pip install beautifulsoup4</code>      </p>
<h1 id="BeautifulSoup库的测试"><a href="#BeautifulSoup库的测试" class="headerlink" title="BeautifulSoup库的测试"></a>BeautifulSoup库的测试</h1><p>在任一浏览器中输入<code>http://python123.io/ws/demo.html</code><br>查看网站的源代码    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">r = requests.get(<span class="string">'http://python123.io/ws/demo.html'</span>)</span><br><span class="line">demo = r.text <span class="comment">#获取网站的源代码</span></span><br><span class="line">soup = BeautifulSoup(demo, <span class="string">'html.parser'</span>) <span class="comment">#对demo进行html的解析</span></span><br><span class="line">print(soup.prettify())</span><br></pre></td></tr></table></figure>
<p><strong>主要用法</strong>    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(<span class="string">'&lt;p&gt;data&lt;/p&gt;'</span>,<span class="string">'html.parser'</span>)</span><br></pre></td></tr></table></figure>
<h1 id="BeautifulSoup库的基本元素"><a href="#BeautifulSoup库的基本元素" class="headerlink" title="BeautifulSoup库的基本元素"></a>BeautifulSoup库的基本元素</h1><p><img src="https://i.loli.net/2020/06/11/cuA1DqJTfeilpkj.png" alt=""></p>
<p>属性是由键值对构成的<br><strong>BeautifulSoup库解析器</strong></p>
<table>
<thead>
<tr>
<th align="center">解析器</th>
<th align="center">使用方法</th>
<th align="center">条件</th>
</tr>
</thead>
<tbody><tr>
<td align="center">bs4的HTML解析器</td>
<td align="center">BeautifulSoup(mk,’html.parser)</td>
<td align="center">安装bs4库</td>
</tr>
<tr>
<td align="center">lxml的HTML解析器</td>
<td align="center">BeautifulSoup(mk,’lxml’)</td>
<td align="center">pip install lxml</td>
</tr>
<tr>
<td align="center">lxml的XML解析器</td>
<td align="center">BeautifulSoup(mk,’ xml’)</td>
<td align="center">pip install lxml</td>
</tr>
<tr>
<td align="center">html5lib的解析器</td>
<td align="center">BeautifulSoup( mk,’ html5lib’)</td>
<td align="center">pip install html5lib</td>
</tr>
</tbody></table>
<p><strong>BeautifulSoup类的基本元素</strong></p>
<table>
<thead>
<tr>
<th align="center">基本元素</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Tag</td>
<td align="center">标签，最基本的信息组织单元，分别用&lt;&gt;和&lt;/&gt;标明开头和结尾</td>
</tr>
<tr>
<td align="center">Name</td>
<td align="center">标签的名字，&lt; p &gt;…&lt; /p &gt;的名字是’p’， 格式: &lt; tag &gt;.name</td>
</tr>
<tr>
<td align="center">Attributes</td>
<td align="center">标签的属性，字典形式组织，格式: &lt; tag &gt; .attrs</td>
</tr>
<tr>
<td align="center">NavigableString</td>
<td align="center">标签内非属性字符串，&lt;&gt;…&lt;/&gt;中字符串， 格式: &lt; tag     &gt;.string</td>
</tr>
<tr>
<td align="center">Comment</td>
<td align="center">标签内字符串的注释部分，一种特殊的Comment类型</td>
</tr>
</tbody></table>
<p><strong>获取Tag的方法</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">r = requests.get(<span class="string">'http://python123.io/ws/demo.html'</span>)</span><br><span class="line">demo = r.text <span class="comment">#获取网站的源代码</span></span><br><span class="line">soup = BeautifulSoup(demo, <span class="string">'html.parser'</span>)</span><br><span class="line">print(soup.title)</span><br><span class="line">tag = soup.a</span><br><span class="line">print(tag)</span><br></pre></td></tr></table></figure>
<p><strong>获取Name的方法</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">r = requests.get(<span class="string">'http://python123.io/ws/demo.html'</span>)</span><br><span class="line">demo = r.text <span class="comment">#获取网站的源代码</span></span><br><span class="line">soup = BeautifulSoup(demo, <span class="string">'html.parser'</span>)</span><br><span class="line">tag = soup.a</span><br><span class="line">print(soup.a.name)</span><br><span class="line">print(soup.a.parent.name)</span><br><span class="line">print(soup.a.parent.parent.name)</span><br></pre></td></tr></table></figure>
<p><strong>获取Attributes的方法</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">r = requests.get(<span class="string">'http://python123.io/ws/demo.html'</span>)</span><br><span class="line">demo = r.text <span class="comment">#获取网站的源代码</span></span><br><span class="line">soup = BeautifulSoup(demo, <span class="string">'html.parser'</span>)</span><br><span class="line">tag = soup.a</span><br><span class="line">print(tag.attrs)</span><br><span class="line">print(tag.attrs[<span class="string">'class'</span>]) <span class="comment">#获取class对应的值</span></span><br><span class="line">print(tag.attrs[<span class="string">'href'</span>]) <span class="comment">#获取标签链接</span></span><br><span class="line">print(type(tag.attrs)) <span class="comment">#获取标签属性的类型</span></span><br></pre></td></tr></table></figure>
<p><strong>获取NavigableString的方法</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">r = requests.get(<span class="string">'http://python123.io/ws/demo.html'</span>)</span><br><span class="line">demo = r.text <span class="comment">#获取网站的源代码</span></span><br><span class="line">soup = BeautifulSoup(demo, <span class="string">'html.parser'</span>)</span><br><span class="line">print(soup.a.string)</span><br><span class="line">print((soup.p.string))</span><br><span class="line">print(type(soup.p.string))</span><br></pre></td></tr></table></figure>
<p><strong>获取Comment的方法</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">r = requests.get(<span class="string">'http://python123.io/ws/demo.html'</span>)</span><br><span class="line">demo = r.text <span class="comment">#获取网站的源代码</span></span><br><span class="line">soup = BeautifulSoup(demo, <span class="string">'html.parser'</span>)</span><br><span class="line">print(soup.p)</span><br><span class="line">newsoup = BeautifulSoup(<span class="string">'&lt;p class="title"&gt;&lt;b&gt;The demo python introduces several python courses.&lt;/b&gt;&lt;/p&gt;'</span>, <span class="string">'html.parser'</span>)</span><br><span class="line">print(newsoup.b.string)</span><br><span class="line">print(type(newsoup.b.string))</span><br></pre></td></tr></table></figure>
<h1 id="基于bs4库的HTML内容遍历方法"><a href="#基于bs4库的HTML内容遍历方法" class="headerlink" title="基于bs4库的HTML内容遍历方法"></a>基于bs4库的HTML内容遍历方法</h1><p><img src="https://i.loli.net/2020/06/11/VOEiaJBo3fGAF1t.png" alt="HTML基本格式及三种遍历方法"></p>
<p><strong>标签树的下行遍历</strong></p>
<table>
<thead>
<tr>
<th align="center">属性</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">.contents</td>
<td align="center">子节点的列表，将&lt; tag &gt;所有儿子节点存入列表</td>
</tr>
<tr>
<td align="center">.children</td>
<td align="center">子节点的迭代类型，与.contents类似，用于循环遍历儿子节点</td>
</tr>
<tr>
<td align="center">.descendants</td>
<td align="center">子孙节点的迭代类型，包含所有子孙节点，用于循环遍历</td>
</tr>
</tbody></table>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">r = requests.get(<span class="string">'http://python123.io/ws/demo.html'</span>)</span><br><span class="line">demo = r.text <span class="comment">#获取网站的源代码</span></span><br><span class="line">soup = BeautifulSoup(demo, <span class="string">'html.parser'</span>)</span><br><span class="line">print(soup.head)</span><br><span class="line">print(soup.head.contents)</span><br><span class="line">print(soup.body.contents)</span><br><span class="line">print(soup.body.contents[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>遍历儿子节点：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> soup.body.children:</span><br><span class="line">    pirnt(child)</span><br></pre></td></tr></table></figure>
<p><strong>标签树的上行遍历</strong></p>
<table>
<thead>
<tr>
<th align="center">属性</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">.parent</td>
<td align="center">节点的父亲标签</td>
</tr>
<tr>
<td align="center">.parents</td>
<td align="center">节点先辈标签的迭代类型，用于循环遍历先辈节点</td>
</tr>
</tbody></table>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(demo, <span class="string">'html.parser'</span>)</span><br><span class="line"><span class="keyword">for</span> parent <span class="keyword">in</span> soup.a.parents:</span><br><span class="line">    <span class="keyword">if</span> parent <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        print(parent)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(parent.name) <span class="comment">#打印soup.a标签所有的先辈</span></span><br></pre></td></tr></table></figure>
<p><strong>标签树的平行遍历</strong></p>
<table>
<thead>
<tr>
<th align="center">属性</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">.next_sibling</td>
<td align="center">返回按照HTML文本顺序的下一个平行节点标签</td>
</tr>
<tr>
<td align="center">.previous_sibling</td>
<td align="center">返回按照HTML文本顺序的上一 个平行节点标签</td>
</tr>
<tr>
<td align="center">.next_siblings</td>
<td align="center">迭代类型，返回按照HTML文本顺序的后续所有平行节点标签</td>
</tr>
<tr>
<td align="center">.previous_siblings</td>
<td align="center">迭代类型，返回按照HTML文本顺序的前续所有平行节点标签</td>
</tr>
</tbody></table>
<p><img src="https://i.loli.net/2020/06/11/PKTVZSgwBEQtL6a.png" alt="平行遍历发生在同一个父节点下的各节点间"></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(demo, <span class="string">'html.parser'</span>)</span><br><span class="line">print(soup.a.next_sibling) <span class="comment">#NavigableString也构成了节点</span></span><br><span class="line">print(soup.a.next_sibling.next_sibling)</span><br><span class="line">print(soup.a.previous_sibling)</span><br><span class="line">print(soup.a.parent)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#对标签树进行循环遍历</span></span><br><span class="line"><span class="keyword">for</span> sibling <span class="keyword">in</span> soup.a.next_siblings:</span><br><span class="line">    print(sibling)  <span class="comment">#遍历后续节点</span></span><br><span class="line"><span class="keyword">for</span> sibling <span class="keyword">in</span> soup.a.previous_siblings:</span><br><span class="line">    print(sibling)  <span class="comment">#遍历前续节点</span></span><br></pre></td></tr></table></figure>


<p><strong>遍历类型总结</strong></p>
<p><img src="https://i.loli.net/2020/06/11/Jh4sljIbeLHQoMy.png" alt=""></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/06/11/%E5%88%B0%E6%AD%A4%E4%B8%BA%E6%AD%A2/">到此为止</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-06-11</time><div class="content"><h1 id="到此为止"><a href="#到此为止" class="headerlink" title="到此为止"></a>到此为止</h1><p>走到现在这一步其实在我的意料之中。<br>最开始接触你我以为你是一个开朗的女孩儿，但是越接触我越能感觉到你身上无止境的负能量和像无底洞一样的抱怨和埋怨。<br>每天都在说好烦啊烦死了。学习工作压力一大，烦死不爽和叹气就挂在你的嘴边。我就像你身边的垃圾桶，负面情绪的处理站。<br>我最难熬的是大一下的期末，11门考试排的很密，而且还要做物理实验。那一个月，你每天都在抱怨，抱怨完寝室抱怨实验，抱怨完实验抱怨考试。<br>我每天都在如履薄冰，一边承受着和你同样的压力，一边接受你每天高强度的抱怨和负面情绪，不敢疏导，生怕俩人又开始生气吵架。<br>可能是我在家太舒坦了，我现在真不知道我当时是怎么忍受过来而没有崩溃的。<br>哪怕就是心理医生，也很难承受这样的高强度折磨吧？<br>聊天搜素里输入一个烦字，一拉就是一串，密密麻麻。<br>记得就在几星期前，你又开始因为一些琐碎的小事各种怨气缠身，聊天记录全都是烦死了，我好烦，真烦。<br>我在电脑这边实在是忍受不了了，情绪突然变得暴躁，用拳头疯狂锤桌子，然后把手边的草稿纸撕得粉碎，无力地坐在椅子上，心跳很快。<br>然后我平静了一下，聊天框里打出：没事啦宝贝 不用烦。<br>换到一年前，我根本想不到我这种正能量充电器会变得这样狂躁。<br>我知道，这个情况不解决，疙瘩早晚在那里，迟早得要解决。<br>昨天你对我说，既然你接受不了我的负面情绪，就别再委屈自己了，对自己好一点？<br>嗯，好。        </p>
</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/"><i class="fa fa-chevron-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://i.loli.net/2020/05/12/jFsUgX9TBbnQu2o.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2020 By LyC</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.7.0"></script><script src="/js/fancybox.js?version=1.7.0"></script><script src="/js/sidebar.js?version=1.7.0"></script><script src="/js/copy.js?version=1.7.0"></script><script src="/js/fireworks.js?version=1.7.0"></script><script src="/js/transition.js?version=1.7.0"></script><script src="/js/scroll.js?version=1.7.0"></script><script src="/js/head.js?version=1.7.0"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" data-click="false"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>